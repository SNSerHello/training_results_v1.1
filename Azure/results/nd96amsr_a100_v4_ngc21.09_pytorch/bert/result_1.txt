+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : DGXA100_1x8x56x1
+ : 1
+ : ./api_logs
+ : 0
+ : 1
++ basename /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : /lustre/fsw/containers/6148_bertv11.sqsh.squashfs
+ : language_model
+ : 0
++ date +%y%m%d%H%M%S%N
+ : 211018201445942768272
+ : /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ : ''
+ : 0
+ : 0
+ : /workspace/bert
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ LOG_BASE=bert_1x8x56_211018201445942768272
+ readonly LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445942768272
+ LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445942768272
+ srun --ntasks=1 --ntasks-per-node=1 mkdir -p /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results
+ [[ 0 -gt 0 ]]
+ CONT_FILE=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ echo 'CI directory structure\n'
CI directory structure\n
++ ls
+ echo Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
+++ func_update_file_path_for_ci mounts.txt /shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/language_model/pytorch
+++ declare new_path
+++ '[' -f mounts.txt ']'
+++ new_path=mounts.txt
+++ '[' '!' -f mounts.txt ']'
+++ echo mounts.txt
++ func_get_container_mounts mounts.txt
++ declare -a mount_array
++ readarray -t mount_array
+++ envsubst
++++ printf %s, '${DATADIR}:/workspace/data' '${DATADIR_PHASE2}:/workspace/data_phase2' '${CHECKPOINTDIR}:/results' '${CHECKPOINTDIR_PHASE1}:/workspace/phase1' '${EVALDIR}:/workspace/evaldata' '${UNITTESTDIR}:/workspace/unit_test_data' '${PWD}/bind.sh:/bm_utils/bind.sh' '${PWD}/azure.sh:/bm_utils/azure.sh' '${PWD}/run_and_time.sh:/bm_utils/run_and_time.sh' /opt/microsoft:/opt/microsoft
++++ sed 's/[,]*$//'
++ local cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
++ echo /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ CONT_MOUNTS=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ '[' 0 -eq 1 ']'
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ srun --ntasks=1 mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ srun --ntasks=1 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh --container-name=language_model true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445942768272_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0418
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1634588089979, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --ntasks=8 --ntasks-per-node=8 --container-name=language_model --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft --container-workdir=/workspace/bert /bm_utils/run_and_time.sh
4: Run vars: id 6148 gpus 8 mparams ''
2: Run vars: id 6148 gpus 8 mparams ''
5: Run vars: id 6148 gpus 8 mparams ''
0: Run vars: id 6148 gpus 8 mparams ''
3: Run vars: id 6148 gpus 8 mparams ''
6: Run vars: id 6148 gpus 8 mparams ''
1: Run vars: id 6148 gpus 8 mparams ''
7: Run vars: id 6148 gpus 8 mparams ''
2: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
4: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
5: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
3: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
6: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
1: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
7: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: num_sockets = 2 num_nodes=4 cores_per_socket=48
6: num_sockets = 2 num_nodes=4 cores_per_socket=48
4: num_sockets = 2 num_nodes=4 cores_per_socket=48
6: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=6 --allreduce
6: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=29660
0: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=0 --allreduce
0: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=17776
4: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=4 --allreduce
4: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=22232
1: num_sockets = 2 num_nodes=4 cores_per_socket=48
1: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=1 --allreduce
1: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=26856
3: num_sockets = 2 num_nodes=4 cores_per_socket=48
3: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=3 --allreduce_
3: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=9316
2: num_sockets = 2 num_nodes=4 cores_per_socket=48
5: num_sockets = 2 num_nodes=4 cores_per_socket=48
2: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=2 --allreduce_
2: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=29959
5: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=5 --allreduce
5: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=28548
7: num_sockets = 2 num_nodes=4 cores_per_socket=48
7: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=7 --allreduce
7: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=1815
6: :::MLLOG {"namespace": "", "time_ms": 1634588093981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
2: :::MLLOG {"namespace": "", "time_ms": 1634588094030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
4: :::MLLOG {"namespace": "", "time_ms": 1634588094148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588094199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
7: :::MLLOG {"namespace": "", "time_ms": 1634588094239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
5: :::MLLOG {"namespace": "", "time_ms": 1634588094254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
1: :::MLLOG {"namespace": "", "time_ms": 1634588094278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
3: :::MLLOG {"namespace": "", "time_ms": 1634588094287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: device: cuda:0 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588095313, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 66}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 71}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 75}}
7: device: cuda:7 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 79}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xND96amsr_A100_v4", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 83}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "seed", "value": 17776, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1095}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 448, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1097}}
3: device: cuda:3 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 56, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1099}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1101}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095314, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1103}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095315, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 7100.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1105}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095315, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1107}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, seed=17776, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
4: device: cuda:4 n_gpu: 8, distributed training: True, 16-bits training: True
6: device: cuda:6 n_gpu: 8, distributed training: True, 16-bits training: True
2: device: cuda:2 n_gpu: 8, distributed training: True, 16-bits training: True
5: device: cuda:5 n_gpu: 8, distributed training: True, 16-bits training: True
1: device: cuda:1 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588101851, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 699}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112216, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 731}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112216, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.9, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 734}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112216, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.999, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 735}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112216, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 736}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112231, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112232, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112232, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
6: Torch distributed is available.
6: Torch distributed is initialized.
3: Torch distributed is available.
3: Torch distributed is initialized.
5: Torch distributed is available.
5: Torch distributed is initialized.
0: Torch distributed is available.
0: Torch distributed is initialized.
7: Torch distributed is available.
7: Torch distributed is initialized.
4: Torch distributed is available.
4: Torch distributed is initialized.
1: Torch distributed is available.
1: Torch distributed is initialized.
2: Torch distributed is available.
2: Torch distributed is initialized.
0: :::MLLOG {"namespace": "", "time_ms": 1634588128063, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1370}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128063, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1371}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128086, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1382, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128089, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1384, "first_epoch_num": 1, "epoch_count": 1}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=17776, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
0: epoch: 1
0: :::MLLOG {"namespace": "", "time_ms": 1634588190563, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.37389516830444336, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 335, 'eval_loss': 4.110085964202881, 'eval_mlm_accuracy': 0.37389516830444336}
0: :::MLLOG {"namespace": "", "time_ms": 1634588252114, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.40270644426345825, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 670, 'eval_loss': 3.8605260848999023, 'eval_mlm_accuracy': 0.40270644426345825}
0: :::MLLOG {"namespace": "", "time_ms": 1634588319864, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.42977336049079895, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1005, 'eval_loss': 3.615898370742798, 'eval_mlm_accuracy': 0.42977336049079895}
0: :::MLLOG {"namespace": "", "time_ms": 1634588387199, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.47120389342308044, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1340, 'eval_loss': 3.2528436183929443, 'eval_mlm_accuracy': 0.47120389342308044}
0: :::MLLOG {"namespace": "", "time_ms": 1634588449224, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.555078387260437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1675, 'eval_loss': 2.555615186691284, 'eval_mlm_accuracy': 0.555078387260437}
0: :::MLLOG {"namespace": "", "time_ms": 1634588511184, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6414258480072021, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2009, 'eval_loss': 1.8596599102020264, 'eval_mlm_accuracy': 0.6414258480072021}
0: :::MLLOG {"namespace": "", "time_ms": 1634588570938, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6981120109558105, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2344, 'eval_loss': 1.4455747604370117, 'eval_mlm_accuracy': 0.6981120109558105}
0: :::MLLOG {"namespace": "", "time_ms": 1634588631861, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7071724534034729, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2679, 'eval_loss': 1.388224720954895, 'eval_mlm_accuracy': 0.7071724534034729}
0: :::MLLOG {"namespace": "", "time_ms": 1634588692281, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7101311087608337, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3014, 'eval_loss': 1.3684930801391602, 'eval_mlm_accuracy': 0.7101311087608337}
0: :::MLLOG {"namespace": "", "time_ms": 1634588757382, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7127254605293274, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3349, 'eval_loss': 1.3561429977416992, 'eval_mlm_accuracy': 0.7127254605293274}
0: :::MLLOG {"namespace": "", "time_ms": 1634588828751, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.714026153087616, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3684, 'eval_loss': 1.348233938217163, 'eval_mlm_accuracy': 0.714026153087616}
0: :::MLLOG {"namespace": "", "time_ms": 1634588901823, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.715240478515625, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4018, 'eval_loss': 1.335318684577942, 'eval_mlm_accuracy': 0.715240478515625}
0: :::MLLOG {"namespace": "", "time_ms": 1634588975944, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7161325216293335, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4353, 'eval_loss': 1.331695318222046, 'eval_mlm_accuracy': 0.7161325216293335}
0: :::MLLOG {"namespace": "", "time_ms": 1634589048690, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7174401879310608, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4688, 'eval_loss': 1.325047254562378, 'eval_mlm_accuracy': 0.7174401879310608}
0: :::MLLOG {"namespace": "", "time_ms": 1634589118442, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7179422378540039, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5023, 'eval_loss': 1.3214704990386963, 'eval_mlm_accuracy': 0.7179422378540039}
0: :::MLLOG {"namespace": "", "time_ms": 1634589188096, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7185564041137695, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5358, 'eval_loss': 1.3167622089385986, 'eval_mlm_accuracy': 0.7185564041137695}
0: :::MLLOG {"namespace": "", "time_ms": 1634589255883, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7190210819244385, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5692, 'eval_loss': 1.3148165941238403, 'eval_mlm_accuracy': 0.7190210819244385}
0: :::MLLOG {"namespace": "", "time_ms": 1634589326972, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7198243737220764, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6027, 'eval_loss': 1.3098809719085693, 'eval_mlm_accuracy': 0.7198243737220764}
0: :::MLLOG {"namespace": "", "time_ms": 1634589398500, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7199224829673767, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6362, 'eval_loss': 1.3078081607818604, 'eval_mlm_accuracy': 0.7199224829673767}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461896, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7204362154006958, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6697, 'eval_loss': 1.3059285879135132, 'eval_mlm_accuracy': 0.7204362154006958}
0: 0.720436 > 0.720000, Target MLM Accuracy reached at 6697
0: (1, 6704.0) {'final_loss': 0.0}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461981, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1710, "first_epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461982, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1713, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461982, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3000256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1726}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461982, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1729}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589461982, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1732, "status": "success"}}
0: {'e2e_time': 1367.8746445178986, 'training_sequences_per_second': 2359.2714181059446, 'final_loss': 0.0, 'raw_train_time': 1348.2128319740295}
7: ENDING TIMING RUN AT 2021-10-18 08:37:44 PM
7: RESULT,bert,1815,1374,root,2021-10-18 08:14:50 PM
0: ENDING TIMING RUN AT 2021-10-18 08:37:44 PM
0: RESULT,bert,17776,1374,root,2021-10-18 08:14:50 PM
5: ENDING TIMING RUN AT 2021-10-18 08:37:44 PM
5: RESULT,bert,28548,1374,root,2021-10-18 08:14:50 PM
1: ENDING TIMING RUN AT 2021-10-18 08:37:45 PM
1: RESULT,bert,26856,1375,root,2021-10-18 08:14:50 PM
2: ENDING TIMING RUN AT 2021-10-18 08:37:45 PM
2: RESULT,bert,29959,1375,root,2021-10-18 08:14:50 PM
4: ENDING TIMING RUN AT 2021-10-18 08:37:45 PM
4: RESULT,bert,22232,1375,root,2021-10-18 08:14:50 PM
6: ENDING TIMING RUN AT 2021-10-18 08:37:45 PM
6: RESULT,bert,29660,1375,root,2021-10-18 08:14:50 PM
3: ENDING TIMING RUN AT 2021-10-18 08:37:46 PM
3: RESULT,bert,9316,1376,root,2021-10-18 08:14:50 PM
+ [[ 0 -gt 0 ]]
