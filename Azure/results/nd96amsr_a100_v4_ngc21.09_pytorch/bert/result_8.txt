+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : DGXA100_1x8x56x1
+ : 1
+ : ./api_logs
+ : 0
+ : 1
++ basename /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : /lustre/fsw/containers/6158_bertv11.sqsh.squashfs
+ : language_model
+ : 0
++ date +%y%m%d%H%M%S%N
+ : 211018201445995452660
+ : /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445
+ : ''
+ : 0
+ : 0
+ : /workspace/bert
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ LOG_BASE=bert_1x8x56_211018201445995452660
+ readonly LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445/bert_1x8x56_211018201445995452660
+ LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445/bert_1x8x56_211018201445995452660
+ srun --ntasks=1 --ntasks-per-node=1 mkdir -p /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results
+ [[ 0 -gt 0 ]]
+ CONT_FILE=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ echo 'CI directory structure\n'
CI directory structure\n
++ ls
+ echo Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
+++ func_update_file_path_for_ci mounts.txt /shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/language_model/pytorch
+++ declare new_path
+++ '[' -f mounts.txt ']'
+++ new_path=mounts.txt
+++ '[' '!' -f mounts.txt ']'
+++ echo mounts.txt
++ func_get_container_mounts mounts.txt
++ declare -a mount_array
++ readarray -t mount_array
+++ envsubst
++++ sed 's/[,]*$//'
++++ printf %s, '${DATADIR}:/workspace/data' '${DATADIR_PHASE2}:/workspace/data_phase2' '${CHECKPOINTDIR}:/results' '${CHECKPOINTDIR_PHASE1}:/workspace/phase1' '${EVALDIR}:/workspace/evaldata' '${UNITTESTDIR}:/workspace/unit_test_data' '${PWD}/bind.sh:/bm_utils/bind.sh' '${PWD}/azure.sh:/bm_utils/azure.sh' '${PWD}/run_and_time.sh:/bm_utils/run_and_time.sh' /opt/microsoft:/opt/microsoft
++ local cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
++ echo /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ CONT_MOUNTS=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ '[' 0 -eq 1 ']'
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445
+ srun --ntasks=1 mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445
+ srun --ntasks=1 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh --container-name=language_model true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201445/bert_1x8x56_211018201445995452660_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0420
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1634588089844, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --ntasks=8 --ntasks-per-node=8 --container-name=language_model --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft --container-workdir=/workspace/bert /bm_utils/run_and_time.sh
7: Run vars: id 6158 gpus 8 mparams ''
1: Run vars: id 6158 gpus 8 mparams ''
4: Run vars: id 6158 gpus 8 mparams ''
2: Run vars: id 6158 gpus 8 mparams ''
0: Run vars: id 6158 gpus 8 mparams ''
3: Run vars: id 6158 gpus 8 mparams ''
5: Run vars: id 6158 gpus 8 mparams ''
6: Run vars: id 6158 gpus 8 mparams ''
4: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
2: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
7: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
1: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
6: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
3: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
5: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: num_sockets = 2 num_nodes=4 cores_per_socket=48
0: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=0 --allreduce
0: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=8937
3: num_sockets = 2 num_nodes=4 cores_per_socket=48
3: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=3 --allreduce_
3: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=7700
4: num_sockets = 2 num_nodes=4 cores_per_socket=48
4: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=4 --allreduce
4: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=31275
2: num_sockets = 2 num_nodes=4 cores_per_socket=48
5: num_sockets = 2 num_nodes=4 cores_per_socket=48
2: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=2 --allreduce_
2: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=23661
1: num_sockets = 2 num_nodes=4 cores_per_socket=48
7: num_sockets = 2 num_nodes=4 cores_per_socket=48
6: num_sockets = 2 num_nodes=4 cores_per_socket=48
5: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=5 --allreduce
5: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=8659
1: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=1 --allreduce
1: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=2737
6: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=6 --allreduce
6: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=27891
7: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=7 --allreduce
7: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=14468
2: :::MLLOG {"namespace": "", "time_ms": 1634588093973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
4: :::MLLOG {"namespace": "", "time_ms": 1634588094019, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
5: :::MLLOG {"namespace": "", "time_ms": 1634588094101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
7: :::MLLOG {"namespace": "", "time_ms": 1634588094190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
1: :::MLLOG {"namespace": "", "time_ms": 1634588094230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
6: :::MLLOG {"namespace": "", "time_ms": 1634588094245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588094257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
3: :::MLLOG {"namespace": "", "time_ms": 1634588094266, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: device: cuda:0 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 66}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 71}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 75}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 79}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xND96amsr_A100_v4", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 83}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "seed", "value": 8937, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1095}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095358, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 448, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1097}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095359, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 56, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1099}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095359, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1101}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095359, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1103}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095359, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 7100.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1105}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095359, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1107}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, seed=8937, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
1: device: cuda:1 n_gpu: 8, distributed training: True, 16-bits training: True
2: device: cuda:2 n_gpu: 8, distributed training: True, 16-bits training: True
7: device: cuda:7 n_gpu: 8, distributed training: True, 16-bits training: True
4: device: cuda:4 n_gpu: 8, distributed training: True, 16-bits training: True
6: device: cuda:6 n_gpu: 8, distributed training: True, 16-bits training: True
5: device: cuda:5 n_gpu: 8, distributed training: True, 16-bits training: True
3: device: cuda:3 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588102144, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 699}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112436, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 731}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112436, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.9, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 734}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112436, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.999, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 735}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112436, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 736}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112454, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112454, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112455, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
3: Torch distributed is available.
3: Torch distributed is initialized.
5: Torch distributed is available.
5: Torch distributed is initialized.
2: Torch distributed is available.
2: Torch distributed is initialized.
7: Torch distributed is available.
7: Torch distributed is initialized.
6: Torch distributed is available.
6: Torch distributed is initialized.
0: Torch distributed is available.
0: Torch distributed is initialized.
1: Torch distributed is available.
1: Torch distributed is initialized.
4: Torch distributed is available.
4: Torch distributed is initialized.
0: :::MLLOG {"namespace": "", "time_ms": 1634588128630, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1370}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128630, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1371}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128651, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1382, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128654, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1384, "first_epoch_num": 1, "epoch_count": 1}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=8937, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
0: epoch: 1
0: :::MLLOG {"namespace": "", "time_ms": 1634588190291, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.37255945801734924, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 335, 'eval_loss': 4.120389461517334, 'eval_mlm_accuracy': 0.37255945801734924}
0: :::MLLOG {"namespace": "", "time_ms": 1634588250344, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3990005552768707, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 670, 'eval_loss': 3.8909871578216553, 'eval_mlm_accuracy': 0.3990005552768707}
0: :::MLLOG {"namespace": "", "time_ms": 1634588310542, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4231695234775543, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1005, 'eval_loss': 3.6759355068206787, 'eval_mlm_accuracy': 0.4231695234775543}
0: :::MLLOG {"namespace": "", "time_ms": 1634588372182, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4806239604949951, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1340, 'eval_loss': 3.1648647785186768, 'eval_mlm_accuracy': 0.4806239604949951}
0: :::MLLOG {"namespace": "", "time_ms": 1634588436688, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5693626403808594, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1675, 'eval_loss': 2.4365272521972656, 'eval_mlm_accuracy': 0.5693626403808594}
0: :::MLLOG {"namespace": "", "time_ms": 1634588502023, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6549044251441956, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2009, 'eval_loss': 1.7604382038116455, 'eval_mlm_accuracy': 0.6549044251441956}
0: :::MLLOG {"namespace": "", "time_ms": 1634588567765, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7006433606147766, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2344, 'eval_loss': 1.4289005994796753, 'eval_mlm_accuracy': 0.7006433606147766}
0: :::MLLOG {"namespace": "", "time_ms": 1634588634873, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7082186341285706, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2679, 'eval_loss': 1.3822201490402222, 'eval_mlm_accuracy': 0.7082186341285706}
0: :::MLLOG {"namespace": "", "time_ms": 1634588703417, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7107779383659363, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3014, 'eval_loss': 1.3610527515411377, 'eval_mlm_accuracy': 0.7107779383659363}
0: :::MLLOG {"namespace": "", "time_ms": 1634588770882, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7130057215690613, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3349, 'eval_loss': 1.352805733680725, 'eval_mlm_accuracy': 0.7130057215690613}
0: :::MLLOG {"namespace": "", "time_ms": 1634588834007, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7140331864356995, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3684, 'eval_loss': 1.3465254306793213, 'eval_mlm_accuracy': 0.7140331864356995}
0: :::MLLOG {"namespace": "", "time_ms": 1634588900245, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7151774168014526, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4018, 'eval_loss': 1.3360929489135742, 'eval_mlm_accuracy': 0.7151774168014526}
0: :::MLLOG {"namespace": "", "time_ms": 1634588970038, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7159129977226257, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4353, 'eval_loss': 1.331477165222168, 'eval_mlm_accuracy': 0.7159129977226257}
0: :::MLLOG {"namespace": "", "time_ms": 1634589040760, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.716884434223175, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4688, 'eval_loss': 1.3268952369689941, 'eval_mlm_accuracy': 0.716884434223175}
0: :::MLLOG {"namespace": "", "time_ms": 1634589110863, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7173864841461182, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5023, 'eval_loss': 1.3227574825286865, 'eval_mlm_accuracy': 0.7173864841461182}
0: :::MLLOG {"namespace": "", "time_ms": 1634589180692, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7186498045921326, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5358, 'eval_loss': 1.3180865049362183, 'eval_mlm_accuracy': 0.7186498045921326}
0: :::MLLOG {"namespace": "", "time_ms": 1634589248652, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7187291979789734, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5692, 'eval_loss': 1.3152192831039429, 'eval_mlm_accuracy': 0.7187291979789734}
0: :::MLLOG {"namespace": "", "time_ms": 1634589314064, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7192709445953369, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6027, 'eval_loss': 1.3122282028198242, 'eval_mlm_accuracy': 0.7192709445953369}
0: :::MLLOG {"namespace": "", "time_ms": 1634589376057, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7196772694587708, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6362, 'eval_loss': 1.3094981908798218, 'eval_mlm_accuracy': 0.7196772694587708}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448076, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7201863527297974, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 6697, 'eval_loss': 1.3071366548538208, 'eval_mlm_accuracy': 0.7201863527297974}
0: 0.720186 > 0.720000, Target MLM Accuracy reached at 6697
0: (1, 6705.0) {'final_loss': 0.0}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448154, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1710, "first_epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448155, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1713, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448155, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3000256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1726}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448155, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1729}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589448155, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1732, "status": "success"}}
0: {'e2e_time': 1353.9915070533752, 'training_sequences_per_second': 2384.2377845519213, 'final_loss': 0.0, 'raw_train_time': 1334.0951228141785}
7: ENDING TIMING RUN AT 2021-10-18 08:37:30 PM
7: RESULT,bert,14468,1360,root,2021-10-18 08:14:50 PM
0: ENDING TIMING RUN AT 2021-10-18 08:37:30 PM
0: RESULT,bert,8937,1360,root,2021-10-18 08:14:50 PM
6: ENDING TIMING RUN AT 2021-10-18 08:37:31 PM
6: RESULT,bert,27891,1361,root,2021-10-18 08:14:50 PM
1: ENDING TIMING RUN AT 2021-10-18 08:37:31 PM
1: RESULT,bert,2737,1361,root,2021-10-18 08:14:50 PM
2: ENDING TIMING RUN AT 2021-10-18 08:37:31 PM
2: RESULT,bert,23661,1361,root,2021-10-18 08:14:50 PM
4: ENDING TIMING RUN AT 2021-10-18 08:37:31 PM
4: RESULT,bert,31275,1361,root,2021-10-18 08:14:50 PM
5: ENDING TIMING RUN AT 2021-10-18 08:37:31 PM
5: RESULT,bert,8659,1361,root,2021-10-18 08:14:50 PM
3: ENDING TIMING RUN AT 2021-10-18 08:37:32 PM
3: RESULT,bert,7700,1362,root,2021-10-18 08:14:50 PM
+ [[ 0 -gt 0 ]]
