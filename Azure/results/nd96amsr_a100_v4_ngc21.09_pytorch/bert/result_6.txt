+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : DGXA100_1x8x56x1
+ : 1
+ : ./api_logs
+ : 0
+ : 1
++ basename /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ : /lustre/fsw/containers/6155_bertv11.sqsh.squashfs
+ : language_model
+ : 0
++ date +%y%m%d%H%M%S%N
+ : 211018201445976025993
+ : /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ : ''
+ : 0
+ : 0
+ : /workspace/bert
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ LOG_BASE=bert_1x8x56_211018201445976025993
+ readonly LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445976025993
+ LOG_FILE_BASE=/mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445976025993
+ srun --ntasks=1 --ntasks-per-node=1 mkdir -p /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results
+ [[ 0 -gt 0 ]]
+ CONT_FILE=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh
+ echo 'CI directory structure\n'
CI directory structure\n
++ ls
+ echo Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
Dockerfile LICENSE NOTICE README.md README_dgxa100_n128_ngc21.09_pytorch.md README_dgxa100_n256_ngc21.09_pytorch.md README_dgxa100_n8_ngc21.09_pytorch.md README_dgxa100_ngc21.09_pytorch.md azure.sh bind.sh bind_pyt.py bmm1.py bmm2.py cleanup_scripts config_DGXA100_128x8x3x1.sh config_DGXA100_1x4x56x2.sh config_DGXA100_1x8x56x1.sh config_DGXA100_256x8x4x1.sh config_DGXA100_4gpu_common.sh config_DGXA100_8x8x48x1.sh config_DGXA100_common.sh config_DGXA100_unittest.sh convert_tf_checkpoint.py dgxa100_nic_affinity.xml extract_features.py file_utils.py fmha.py function.py fwd_loss_bwd_trainer.py inference.py input_preprocessing mha.py mhalib mhalib.cpython-38-x86_64-linux-gnu.so mlperf_logger.py model modeling.py mounts.txt optim optimization.py padding.py requirements.txt run.sub run_and_time.sh run_pretraining.py run_squad.py run_test.sh run_with_docker.sh scaleoutbridge.py schedulers.py scripts softmax.py tokenization.py unit_test utils.py
+++ func_update_file_path_for_ci mounts.txt /shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/language_model/pytorch
+++ declare new_path
+++ '[' -f mounts.txt ']'
+++ new_path=mounts.txt
+++ '[' '!' -f mounts.txt ']'
+++ echo mounts.txt
++ func_get_container_mounts mounts.txt
++ declare -a mount_array
++ readarray -t mount_array
+++ envsubst
++++ sed 's/[,]*$//'
++++ printf %s, '${DATADIR}:/workspace/data' '${DATADIR_PHASE2}:/workspace/data_phase2' '${CHECKPOINTDIR}:/results' '${CHECKPOINTDIR_PHASE1}:/workspace/phase1' '${EVALDIR}:/workspace/evaldata' '${UNITTESTDIR}:/workspace/unit_test_data' '${PWD}/bind.sh:/bm_utils/bind.sh' '${PWD}/azure.sh:/bm_utils/azure.sh' '${PWD}/run_and_time.sh:/bm_utils/run_and_time.sh' /opt/microsoft:/opt/microsoft
++ local cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
++ echo /mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ CONT_MOUNTS=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft
+ '[' 0 -eq 1 ']'
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ srun --ntasks=1 mkdir -p /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444
+ srun --ntasks=1 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/bertv11.sqsh --container-name=language_model true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/bert/1N-m18.201444/bert_1x8x56_211018201445976025993_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C041C
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1634588089990, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --ntasks=8 --ntasks-per-node=8 --container-name=language_model --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/training-4320/hdf5_4320_shards_varlength:/workspace/data_phase2,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/results:/results,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/phase1:/workspace/phase1,/mnt/resource_nvme/mlcommons/v1.1/bm_data/bert_data/hdf5/eval_varlength:/workspace/evaldata,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/unit_test:/workspace/unit_test_data,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/bert/implementations/pytorch/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft --container-workdir=/workspace/bert /bm_utils/run_and_time.sh
6: Run vars: id 6155 gpus 8 mparams ''
1: Run vars: id 6155 gpus 8 mparams ''
3: Run vars: id 6155 gpus 8 mparams ''
4: Run vars: id 6155 gpus 8 mparams ''
5: Run vars: id 6155 gpus 8 mparams ''
7: Run vars: id 6155 gpus 8 mparams ''
0: Run vars: id 6155 gpus 8 mparams ''
7: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
4: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
1: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
3: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
5: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
6: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
2: Run vars: id 6155 gpus 8 mparams ''
2: STARTING TIMING RUN AT 2021-10-18 08:14:50 PM
0: num_sockets = 2 num_nodes=4 cores_per_socket=48
0: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=0 --allreduce
0: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=22672
6: num_sockets = 2 num_nodes=4 cores_per_socket=48
6: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=6 --allreduce
6: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=32430
1: num_sockets = 2 num_nodes=4 cores_per_socket=48
5: num_sockets = 2 num_nodes=4 cores_per_socket=48
1: + exec numactl --physcpubind=24-47 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=1 --allreduce
1: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=56
7: num_sockets = 2 num_nodes=4 cores_per_socket=48
2: num_sockets = 2 num_nodes=4 cores_per_socket=48
3: num_sockets = 2 num_nodes=4 cores_per_socket=48
5: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=5 --allreduce
5: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=18329
7: + exec numactl --physcpubind=48-71 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=7 --allreduce
7: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=25549
4: num_sockets = 2 num_nodes=4 cores_per_socket=48
2: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=2 --allreduce_
2: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=2084
3: + exec numactl --physcpubind=0-23 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=3 --allreduce_
3: post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=3214
4: + exec numactl --physcpubind=72-95 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=56 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=7100 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples 10000 --cache_eval_data --output_dir=/results --fp16 --fused_bias_fc --fused_bias_mha --fused_dropout_add --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --bert_config_path=/workspace/phase1/bert_config.json --local_rank=4 --allreduce
4: _post_accumulation --allreduce_post_accumulation_fp16 --dense_seq_output --unpad --unpad_fmha --exchange_padding --seed=31639
5: :::MLLOG {"namespace": "", "time_ms": 1634588093941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
7: :::MLLOG {"namespace": "", "time_ms": 1634588093992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
2: :::MLLOG {"namespace": "", "time_ms": 1634588094093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
4: :::MLLOG {"namespace": "", "time_ms": 1634588094159, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
3: :::MLLOG {"namespace": "", "time_ms": 1634588094179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588094211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
1: :::MLLOG {"namespace": "", "time_ms": 1634588094223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
6: :::MLLOG {"namespace": "", "time_ms": 1634588094231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1072}}
0: device: cuda:0 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588095298, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 66}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095298, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 71}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095298, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 75}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 79}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xND96amsr_A100_v4", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 83}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "seed", "value": 22672, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1095}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 448, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1097}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 56, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1099}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1101}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1103}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 7100.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1105}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588095299, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1107}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, seed=22672, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
7: device: cuda:7 n_gpu: 8, distributed training: True, 16-bits training: True
1: device: cuda:1 n_gpu: 8, distributed training: True, 16-bits training: True
3: device: cuda:3 n_gpu: 8, distributed training: True, 16-bits training: True
2: device: cuda:2 n_gpu: 8, distributed training: True, 16-bits training: True
5: device: cuda:5 n_gpu: 8, distributed training: True, 16-bits training: True
6: device: cuda:6 n_gpu: 8, distributed training: True, 16-bits training: True
4: device: cuda:4 n_gpu: 8, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1634588102134, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 699}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112539, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 731}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112540, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.9, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 734}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112540, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.999, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 735}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112540, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 736}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112554, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112554, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588112554, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
7: Torch distributed is available.
7: Torch distributed is initialized.
2: Torch distributed is available.
2: Torch distributed is initialized.
0: Torch distributed is available.
0: Torch distributed is initialized.
5: Torch distributed is available.
5: Torch distributed is initialized.
3: Torch distributed is available.
3: Torch distributed is initialized.
6: Torch distributed is available.
6: Torch distributed is initialized.
4: Torch distributed is available.
4: Torch distributed is initialized.
1: Torch distributed is available.
1: Torch distributed is initialized.
0: :::MLLOG {"namespace": "", "time_ms": 1634588128333, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1370}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128334, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1371}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128355, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1382, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634588128358, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1384, "first_epoch_num": 1, "epoch_count": 1}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_bias_fc=True, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_mha=False, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-2
0: 8252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=7100.0, min_samples_to_start_checkpoints=3000000, n_gpu=8, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, pad_fmha=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=22672, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=56, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
0: epoch: 1
0: :::MLLOG {"namespace": "", "time_ms": 1634588196547, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.37388816475868225, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 335, 'eval_loss': 4.103542804718018, 'eval_mlm_accuracy': 0.37388816475868225}
0: :::MLLOG {"namespace": "", "time_ms": 1634588262664, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3942181169986725, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 670, 'eval_loss': 3.8753628730773926, 'eval_mlm_accuracy': 0.3942181169986725}
0: :::MLLOG {"namespace": "", "time_ms": 1634588326762, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5014839768409729, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1005, 'eval_loss': 2.9771621227264404, 'eval_mlm_accuracy': 0.5014839768409729}
0: :::MLLOG {"namespace": "", "time_ms": 1634588391084, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6268053650856018, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1340, 'eval_loss': 1.9851059913635254, 'eval_mlm_accuracy': 0.6268053650856018}
0: :::MLLOG {"namespace": "", "time_ms": 1634588450980, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6928508877754211, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 1675, 'eval_loss': 1.4818785190582275, 'eval_mlm_accuracy': 0.6928508877754211}
0: :::MLLOG {"namespace": "", "time_ms": 1634588511901, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.70725417137146, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2009, 'eval_loss': 1.3852285146713257, 'eval_mlm_accuracy': 0.70725417137146}
0: :::MLLOG {"namespace": "", "time_ms": 1634588575185, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7113734483718872, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2344, 'eval_loss': 1.3616544008255005, 'eval_mlm_accuracy': 0.7113734483718872}
0: :::MLLOG {"namespace": "", "time_ms": 1634588638243, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7135007381439209, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 2679, 'eval_loss': 1.349124789237976, 'eval_mlm_accuracy': 0.7135007381439209}
0: :::MLLOG {"namespace": "", "time_ms": 1634588697730, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7140378355979919, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3014, 'eval_loss': 1.3426728248596191, 'eval_mlm_accuracy': 0.7140378355979919}
0: :::MLLOG {"namespace": "", "time_ms": 1634588759871, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7154576182365417, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3349, 'eval_loss': 1.3338897228240967, 'eval_mlm_accuracy': 0.7154576182365417}
0: :::MLLOG {"namespace": "", "time_ms": 1634588825216, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7167443037033081, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 3684, 'eval_loss': 1.3305559158325195, 'eval_mlm_accuracy': 0.7167443037033081}
0: :::MLLOG {"namespace": "", "time_ms": 1634588892785, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7172673940658569, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4018, 'eval_loss': 1.3238518238067627, 'eval_mlm_accuracy': 0.7172673940658569}
0: :::MLLOG {"namespace": "", "time_ms": 1634588962179, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.718131422996521, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4353, 'eval_loss': 1.318616271018982, 'eval_mlm_accuracy': 0.718131422996521}
0: :::MLLOG {"namespace": "", "time_ms": 1634589034631, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7191355228424072, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 4688, 'eval_loss': 1.3132712841033936, 'eval_mlm_accuracy': 0.7191355228424072}
0: :::MLLOG {"namespace": "", "time_ms": 1634589108758, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.719691276550293, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5023, 'eval_loss': 1.3087129592895508, 'eval_mlm_accuracy': 0.719691276550293}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176581, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7205296158790588, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1579, "epoch_num": 1}}
0: {'global_steps': 5358, 'eval_loss': 1.304473638534546, 'eval_mlm_accuracy': 0.7205296158790588}
0: 0.720530 > 0.720000, Target MLM Accuracy reached at 5358
0: (1, 5364.0) {'final_loss': 0.0}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176657, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1710, "first_epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176657, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1713, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176657, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 2400384, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1726}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176657, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1729}}
0: :::MLLOG {"namespace": "", "time_ms": 1634589176657, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1732, "status": "success"}}
0: {'e2e_time': 1082.538533449173, 'training_sequences_per_second': 2993.331725157417, 'final_loss': 0.0, 'raw_train_time': 1062.628633260727}
7: ENDING TIMING RUN AT 2021-10-18 08:32:58 PM
7: RESULT,bert,25549,1088,root,2021-10-18 08:14:50 PM
0: ENDING TIMING RUN AT 2021-10-18 08:32:59 PM
0: RESULT,bert,22672,1089,root,2021-10-18 08:14:50 PM
6: ENDING TIMING RUN AT 2021-10-18 08:32:59 PM
6: RESULT,bert,32430,1089,root,2021-10-18 08:14:50 PM
1: ENDING TIMING RUN AT 2021-10-18 08:33:00 PM
1: RESULT,bert,56,1090,root,2021-10-18 08:14:50 PM
3: ENDING TIMING RUN AT 2021-10-18 08:33:00 PM
5: ENDING TIMING RUN AT 2021-10-18 08:33:00 PM
5: RESULT,bert,18329,1090,root,2021-10-18 08:14:50 PM
3: RESULT,bert,3214,1090,root,2021-10-18 08:14:50 PM
4: ENDING TIMING RUN AT 2021-10-18 08:33:00 PM
4: RESULT,bert,31639,1090,root,2021-10-18 08:14:50 PM
2: ENDING TIMING RUN AT 2021-10-18 08:33:01 PM
2: RESULT,bert,2084,1091,root,2021-10-18 08:14:50 PM
+ [[ 0 -gt 0 ]]
