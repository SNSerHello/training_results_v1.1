+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019060624612513781
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019060624612513781
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019060624612513781
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019060624612513781
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07391/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019060624612513781_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C044D
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C04C4
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C04BE
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C04CC
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C04DA
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C04BD
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:06:27 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634623592.326571] [ip-0A0C04C8:67306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.328210] [ip-0A0C0437:48349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.333192] [ip-0A0C0437:48347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.348586] [ip-0A0C040F:21951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.361071] [ip-0A0C0491:33988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.365885] [ip-0A0C048B:34677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.365991] [ip-0A0C040F:21950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.369812] [ip-0A0C04C8:67297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.370399] [ip-0A0C04A7:13842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.369130] [ip-0A0C0497:84364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.371080] [ip-0A0C0436:52153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.376699] [ip-0A0C04C8:67299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.392734] [ip-0A0C04A7:13841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.394784] [ip-0A0C046E:53894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.396621] [ip-0A0C048B:34676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.400659] [ip-0A0C049D:36412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.401992] [ip-0A0C046E:53899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.402147] [ip-0A0C044D:49204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.406368] [ip-0A0C049C:23652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.408866] [ip-0A0C04D4:63312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.411905] [ip-0A0C0437:48348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.414334] [ip-0A0C0483:51097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.414333] [ip-0A0C0483:51093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.414625] [ip-0A0C044D:49205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.421460] [ip-0A0C04D3:65355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.421554] [ip-0A0C0460:51556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.427093] [ip-0A0C0492:31988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.429797] [ip-0A0C0497:84365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.438563] [ip-0A0C04B6:6115 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.440835] [ip-0A0C0464:51894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.445477] [ip-0A0C04C2:91879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.445466] [ip-0A0C046C:47443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.445742] [ip-0A0C04CD:63715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.448503] [ip-0A0C0488:50585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.449891] [ip-0A0C0435:85907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.449323] [ip-0A0C04CF:92760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.451086] [ip-0A0C0492:31993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.454811] [ip-0A0C04B6:6139 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.457450] [ip-0A0C044D:49208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.464813] [ip-0A0C048F:41491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.464660] [ip-0A0C0488:50583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.465206] [ip-0A0C0494:33630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.466439] [ip-0A0C0436:52148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.468421] [ip-0A0C046C:47442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.468910] [ip-0A0C047E:48689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.470466] [ip-0A0C0436:52149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.472339] [ip-0A0C04C9:66174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.472803] [ip-0A0C0435:85932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.472755] [ip-0A0C04BB:11941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.472558] [ip-0A0C04AD:13911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.473071] [ip-0A0C04D4:63308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.473182] [ip-0A0C0437:48352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.473461] [ip-0A0C04BB:11942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.473471] [ip-0A0C04C2:91877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.477540] [ip-0A0C04A0:26361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.476774] [ip-0A0C04C6:65090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.476731] [ip-0A0C0497:84360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.480644] [ip-0A0C04C2:91881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.479997] [ip-0A0C04CF:92745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.481909] [ip-0A0C0491:33982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.482831] [ip-0A0C049C:23657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.484222] [ip-0A0C049F:24860:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.484718] [ip-0A0C0437:48353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.485727] [ip-0A0C0498:46768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.487485] [ip-0A0C0495:36202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.490244] [ip-0A0C04DA:60594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.490585] [ip-0A0C0436:52151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.493739] [ip-0A0C04CC:66973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.494600] [ip-0A0C040F:21953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.494661] [ip-0A0C040F:21954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.495813] [ip-0A0C0460:51550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.496546] [ip-0A0C0491:33984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.496734] [ip-0A0C04C8:67300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.497992] [ip-0A0C0483:51092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.501770] [ip-0A0C04C4:69358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.502684] [ip-0A0C04D3:65350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.503065] [ip-0A0C04C8:67301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.503506] [ip-0A0C04B3:44610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.504955] [ip-0A0C04A0:26359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.506827] [ip-0A0C049D:36409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.507863] [ip-0A0C0491:33983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.508810] [ip-0A0C0437:48351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.509190] [ip-0A0C0435:85903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.511701] [ip-0A0C04D8:66956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.512825] [ip-0A0C0492:31994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.512563] [ip-0A0C04C8:67302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.513465] [ip-0A0C046C:47438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.514315] [ip-0A0C04D4:63314:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.514295] [ip-0A0C04D9:90503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.514264] [ip-0A0C04D9:90499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.517288] [ip-0A0C0438:52755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.517932] [ip-0A0C0437:48354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.519645] [ip-0A0C04CC:66972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.519297] [ip-0A0C04A9:36009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.521263] [ip-0A0C0436:52154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.523125] [ip-0A0C04CD:63713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.523384] [ip-0A0C040F:21952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.523033] [ip-0A0C04B5:37536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.522421] [ip-0A0C04B2:13013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.525619] [ip-0A0C04C9:66144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.526652] [ip-0A0C0472:53829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.527796] [ip-0A0C0496:18519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.526455] [ip-0A0C0497:84361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.527359] [ip-0A0C047E:48688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.526857] [ip-0A0C0488:50587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.527851] [ip-0A0C046B:42527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.528844] [ip-0A0C04D9:90497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.529458] [ip-0A0C046E:53895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.530344] [ip-0A0C04C8:67298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.529876] [ip-0A0C0466:54482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.530837] [ip-0A0C04C8:67326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.530583] [ip-0A0C04B6:6111 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.531396] [ip-0A0C04B4:46351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.532441] [ip-0A0C04D8:66955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.532409] [ip-0A0C0441:45110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.532249] [ip-0A0C0464:51902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.533432] [ip-0A0C0437:48350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.533132] [ip-0A0C04C6:65088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.533666] [ip-0A0C0489:52171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.534357] [ip-0A0C0464:51898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.535499] [ip-0A0C04AB:27284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.535609] [ip-0A0C0460:51551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.537636] [ip-0A0C04AD:13915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.538242] [ip-0A0C046B:42525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.538381] [ip-0A0C040F:21955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.538542] [ip-0A0C0487:51804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.538689] [ip-0A0C049D:36408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.538882] [ip-0A0C049D:36411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.539943] [ip-0A0C04AD:13913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.540257] [ip-0A0C040F:21957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.540484] [ip-0A0C048B:34670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.542600] [ip-0A0C04A0:26362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.541490] [ip-0A0C04AF:28565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.541417] [ip-0A0C0497:84362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.542864] [ip-0A0C048B:34675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.545095] [ip-0A0C04AA:11663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.545095] [ip-0A0C04AA:11670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.545526] [ip-0A0C0493:33460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.545769] [ip-0A0C045F:49763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.546348] [ip-0A0C0491:33986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.546467] [ip-0A0C04C5:71281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.546615] [ip-0A0C04AA:11667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.546497] [ip-0A0C04C5:71283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.547602] [ip-0A0C04B3:44606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.547435] [ip-0A0C0429:42388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.549574] [ip-0A0C04BE:69947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.549794] [ip-0A0C04CF:92747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.550365] [ip-0A0C0493:33459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.550855] [ip-0A0C0483:51095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.550961] [ip-0A0C0487:51805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.550976] [ip-0A0C04CD:63720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.551369] [ip-0A0C0472:53831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.551877] [ip-0A0C04A6:36518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.552725] [ip-0A0C04D3:65351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.553261] [ip-0A0C040F:21956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.560423] [ip-0A0C04A2:28363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.554197] [ip-0A0C046E:53896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.554621] [ip-0A0C048A:48612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.554694] [ip-0A0C04C4:69363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.556253] [ip-0A0C04A7:13844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.555246] [ip-0A0C0498:46767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.555786] [ip-0A0C04B0:23373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.555319] [ip-0A0C0498:46771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.555796] [ip-0A0C044D:49222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.554830] [ip-0A0C049C:23655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.559198] [ip-0A0C0492:31992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.559690] [ip-0A0C04A7:13839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.558583] [ip-0A0C049C:23651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.559784] [ip-0A0C0494:33629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.560227] [ip-0A0C04B5:37537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.560821] [ip-0A0C04A4:37053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.562020] [ip-0A0C04AE:42086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.561876] [ip-0A0C04A5:39550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.563088] [ip-0A0C04AF:28566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.562517] [ip-0A0C04B2:13011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564059] [ip-0A0C04C4:69359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564941] [ip-0A0C04A7:13837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564602] [ip-0A0C0491:33987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564847] [ip-0A0C0491:33981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564894] [ip-0A0C04BA:4318 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.565343] [ip-0A0C0441:45109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.564988] [ip-0A0C04B2:13012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.566342] [ip-0A0C04BE:69923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.566213] [ip-0A0C0482:39479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.567041] [ip-0A0C0487:51803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.567199] [ip-0A0C0441:45114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.568204] [ip-0A0C0435:85931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.569259] [ip-0A0C04C0:68712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.569303] [ip-0A0C0443:15492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.571498] [ip-0A0C04B6:6113 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.572213] [ip-0A0C048B:34673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.572284] [ip-0A0C0489:52170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.572761] [ip-0A0C048B:34671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.573514] [ip-0A0C0436:52155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.573662] [ip-0A0C047E:48687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.574185] [ip-0A0C0491:33985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.574155] [ip-0A0C0438:52758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.574964] [ip-0A0C048A:48614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.574950] [ip-0A0C0488:50584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.576103] [ip-0A0C048C:32933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.577376] [ip-0A0C049D:36414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578164] [ip-0A0C04D3:65349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578672] [ip-0A0C0495:36203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578354] [ip-0A0C0466:54478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578736] [ip-0A0C048B:34674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578709] [ip-0A0C04CD:63718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.578743] [ip-0A0C0497:84358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.579798] [ip-0A0C04C4:69361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.579445] [ip-0A0C044D:49207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.580420] [ip-0A0C0460:51553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.580653] [ip-0A0C04A9:36026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.581636] [ip-0A0C04D4:63311:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.581044] [ip-0A0C0497:84363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.581880] [ip-0A0C04C2:91878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.582002] [ip-0A0C04D3:65352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.581481] [ip-0A0C0497:84359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.582428] [ip-0A0C0486:51800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.583131] [ip-0A0C04C0:68718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.582758] [ip-0A0C048F:41489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.582911] [ip-0A0C0466:54479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.583303] [ip-0A0C04DA:60596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.584263] [ip-0A0C0436:52152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.585024] [ip-0A0C04A7:13840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.584976] [ip-0A0C046E:53897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.586315] [ip-0A0C04AC:30361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.586418] [ip-0A0C04B3:44612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.585347] [ip-0A0C049B:25627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.586818] [ip-0A0C0436:52150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.586914] [ip-0A0C04C9:66148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.586849] [ip-0A0C048F:41484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.587885] [ip-0A0C0483:51100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.588996] [ip-0A0C04BA:4314 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.588582] [ip-0A0C049F:24867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.589895] [ip-0A0C04A7:13843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.587516] [ip-0A0C0484:53880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.590004] [ip-0A0C04BD:67200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.591392] [ip-0A0C04A7:13838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.590400] [ip-0A0C0449:49520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.591186] [ip-0A0C046E:53893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.590831] [ip-0A0C044D:49211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.591202] [ip-0A0C0429:42391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.590880] [ip-0A0C044D:49206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.592749] [ip-0A0C0485:16876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.593214] [ip-0A0C0495:36201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.593703] [ip-0A0C04D9:90502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.593198] [ip-0A0C044D:49212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.594706] [ip-0A0C049F:24863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.595723] [ip-0A0C0494:33633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.597149] [ip-0A0C046B:42523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.597218] [ip-0A0C0483:51096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.596925] [ip-0A0C042D:23334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.597625] [ip-0A0C0483:51101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.597903] [ip-0A0C04B6:6109 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.598017] [ip-0A0C04B9:69969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.598666] [ip-0A0C0483:51094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.598935] [ip-0A0C04C2:91883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.599518] [ip-0A0C04B3:44611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.599308] [ip-0A0C04AA:11665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.599677] [ip-0A0C04BA:4317 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600730] [ip-0A0C0496:18521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600053] [ip-0A0C048B:34672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600009] [ip-0A0C046E:53898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600010] [ip-0A0C0460:51552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600251] [ip-0A0C04BD:67204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.601087] [ip-0A0C04CC:66979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.600768] [ip-0A0C0495:36200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.601156] [ip-0A0C046C:47440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.601791] [ip-0A0C049D:36416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.602232] [ip-0A0C046E:53892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.603087] [ip-0A0C0492:31991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.604090] [ip-0A0C0460:51555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.605520] [ip-0A0C04D4:63313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.605469] [ip-0A0C04DA:60597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.605821] [ip-0A0C04C6:65091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.607185] [ip-0A0C04D9:90500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.607662] [ip-0A0C04AC:30363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.607734] [ip-0A0C04AF:28564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.607789] [ip-0A0C04C7:68219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.608049] [ip-0A0C0472:53832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.609424] [ip-0A0C04A0:26357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.608181] [ip-0A0C04A5:39545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.608852] [ip-0A0C04D4:63309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.609601] [ip-0A0C04C2:91885:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.609683] [ip-0A0C04AB:27291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.608953] [ip-0A0C049C:23654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.610767] [ip-0A0C04BB:11945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.610511] [ip-0A0C04B6:6114 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.610499] [ip-0A0C0464:51896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.610898] [ip-0A0C04D3:65353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.611242] [ip-0A0C04AB:27287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.613022] [ip-0A0C04B9:69975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.612853] [ip-0A0C04A9:36008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.613523] [ip-0A0C049D:36413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.614394] [ip-0A0C047E:48683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.616533] [ip-0A0C0485:16875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.623190] [ip-0A0C04A2:28360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.618869] [ip-0A0C049D:36410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.619274] [ip-0A0C047E:48686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.619661] [ip-0A0C048A:48609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.620583] [ip-0A0C0492:31995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.618231] [ip-0A0C04BC:65131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.620764] [ip-0A0C0492:31989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.620357] [ip-0A0C04D8:66959:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.622771] [ip-0A0C048D:37839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.629132] [ip-0A0C04A2:28364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.622758] [ip-0A0C04C2:91882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.621566] [ip-0A0C049C:23658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.623733] [ip-0A0C0492:31990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.623312] [ip-0A0C04B4:46349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.623710] [ip-0A0C04B4:46352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.623626] [ip-0A0C042D:23333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.622209] [ip-0A0C04BC:65133:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.624436] [ip-0A0C0499:38358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.625016] [ip-0A0C04D4:63331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.624652] [ip-0A0C048F:41490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.624433] [ip-0A0C0488:50589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.624557] [ip-0A0C0488:50586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.626091] [ip-0A0C045F:49761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.626080] [ip-0A0C04B7:7718 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.626196] [ip-0A0C0473:50308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.626218] [ip-0A0C045F:49757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.626363] [ip-0A0C04D3:65354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.628932] [ip-0A0C0435:85908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.628528] [ip-0A0C04B6:6108 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.629714] [ip-0A0C0486:51799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.627991] [ip-0A0C049C:23656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.630522] [ip-0A0C0435:85910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.630569] [ip-0A0C04C6:65089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.630944] [ip-0A0C04D3:65348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.631663] [ip-0A0C04D4:63310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.632619] [ip-0A0C04A0:26358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.631507] [ip-0A0C04CF:92742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.631878] [ip-0A0C046C:47439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.633300] [ip-0A0C0438:52754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.633313] [ip-0A0C0460:51554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.633922] [ip-0A0C04B1:40215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.633860] [ip-0A0C0494:33635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.634767] [ip-0A0C04C9:66151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.635658] [ip-0A0C0460:51557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.635676] [ip-0A0C0494:33634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.636804] [ip-0A0C04AE:42089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.636774] [ip-0A0C0429:42393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.637197] [ip-0A0C04C7:68218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.636773] [ip-0A0C0498:46774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.637395] [ip-0A0C04CD:63714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.637908] [ip-0A0C0435:85904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.637093] [ip-0A0C0490:38040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.638778] [ip-0A0C04C2:91880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.638809] [ip-0A0C0466:54483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.639416] [ip-0A0C04B6:6110 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.639869] [ip-0A0C0464:51909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.640084] [ip-0A0C04CF:92744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.641074] [ip-0A0C0488:50582:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.640662] [ip-0A0C049B:25621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.642732] [ip-0A0C04BB:11939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.644618] [ip-0A0C0435:85906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.645517] [ip-0A0C0488:50613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.647967] [ip-0A0C0429:42387:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.646732] [ip-0A0C049C:23653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.649255] [ip-0A0C04CC:66978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.649978] [ip-0A0C04A0:26356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.648728] [ip-0A0C048F:41488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.649033] [ip-0A0C04C3:68182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.649131] [ip-0A0C04B7:7717 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.649445] [ip-0A0C048F:41486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.650533] [ip-0A0C04AE:42090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.650538] [ip-0A0C0495:36199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.650730] [ip-0A0C0472:53826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.650448] [ip-0A0C04A9:36012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.651233] [ip-0A0C04C9:66146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.650658] [ip-0A0C04A9:36006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.651505] [ip-0A0C04D8:66960:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.652279] [ip-0A0C04C9:66149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.652001] [ip-0A0C0464:51904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.652423] [ip-0A0C049F:24861:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.652902] [ip-0A0C0449:49496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.653637] [ip-0A0C04B0:23374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.654799] [ip-0A0C04BB:11958:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.654663] [ip-0A0C04AD:13912:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.654715] [ip-0A0C04AD:13914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656110] [ip-0A0C047E:48685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656302] [ip-0A0C047E:48682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656388] [ip-0A0C047E:48684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656668] [ip-0A0C04C7:68220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656641] [ip-0A0C04CD:63719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.656930] [ip-0A0C0489:52177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.657806] [ip-0A0C0495:36204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.657139] [ip-0A0C04C6:65092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.657391] [ip-0A0C04CF:92743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.655463] [ip-0A0C0484:53886:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.657508] [ip-0A0C04CF:92748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.657789] [ip-0A0C0464:51897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.658030] [ip-0A0C04A8:42164:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.659046] [ip-0A0C04B5:37531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.659316] [ip-0A0C04CF:92741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.659937] [ip-0A0C046C:47445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.660421] [ip-0A0C04D9:90496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.659968] [ip-0A0C049F:24866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.661777] [ip-0A0C0496:18525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.661339] [ip-0A0C04A1:54594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.661347] [ip-0A0C04AA:11669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.660850] [ip-0A0C04DB:59173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.662690] [ip-0A0C04CC:66974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.663014] [ip-0A0C04D9:90498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.663533] [ip-0A0C04CD:63721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.663921] [ip-0A0C048C:32935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.664410] [ip-0A0C04C9:66145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.664643] [ip-0A0C0438:52757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.664607] [ip-0A0C046C:47441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.665287] [ip-0A0C04CD:63716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.665737] [ip-0A0C04AA:11664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.665780] [ip-0A0C042D:23339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.665724] [ip-0A0C046C:47451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.667354] [ip-0A0C04C9:66147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.667246] [ip-0A0C0464:51895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.668615] [ip-0A0C048F:41485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.668675] [ip-0A0C0461:15815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.670487] [ip-0A0C04A6:36520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.670322] [ip-0A0C0494:33632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.671360] [ip-0A0C0473:50307:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672034] [ip-0A0C04B0:23376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.671905] [ip-0A0C049F:24865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672685] [ip-0A0C04BB:11944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672312] [ip-0A0C04DA:60591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672805] [ip-0A0C04AD:13910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672548] [ip-0A0C04C6:65094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.673152] [ip-0A0C0487:51801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672683] [ip-0A0C04C6:65093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.673094] [ip-0A0C0443:15487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.672875] [ip-0A0C04DB:59178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.674436] [ip-0A0C04BD:67202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.674291] [ip-0A0C045F:49762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.674342] [ip-0A0C048F:41487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.675173] [ip-0A0C04B4:46344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.675109] [ip-0A0C04C6:65087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.676152] [ip-0A0C04BB:11938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.676411] [ip-0A0C04DA:60592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.677865] [ip-0A0C04A0:26360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.677881] [ip-0A0C0496:18523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.677438] [ip-0A0C04B9:69974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.678076] [ip-0A0C04D8:66962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.677725] [ip-0A0C0482:39481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.678303] [ip-0A0C046B:42529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.678704] [ip-0A0C04D9:90501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.679722] [ip-0A0C04CC:66976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.679274] [ip-0A0C04D8:66961:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.679469] [ip-0A0C04BB:11940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.679412] [ip-0A0C04AD:13916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.681500] [ip-0A0C04A0:26363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.681787] [ip-0A0C048C:32934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.682117] [ip-0A0C046B:42524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.682068] [ip-0A0C0449:49491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.682423] [ip-0A0C04C5:71280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.682377] [ip-0A0C0482:39474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.682260] [ip-0A0C0494:33628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.683623] [ip-0A0C049F:24862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.684010] [ip-0A0C04C3:68181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.684239] [ip-0A0C049F:24864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.683957] [ip-0A0C0494:33631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.685706] [ip-0A0C04B0:23377:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.686091] [ip-0A0C0493:33462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.686735] [ip-0A0C04C4:69360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.686715] [ip-0A0C0441:45115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.687220] [ip-0A0C048D:37838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.688235] [ip-0A0C04A6:36519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.693985] [ip-0A0C04A2:28359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.686915] [ip-0A0C0498:46770:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.687016] [ip-0A0C0498:46773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.688175] [ip-0A0C0495:36197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.688649] [ip-0A0C04AA:11666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.688594] [ip-0A0C0493:33461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.689259] [ip-0A0C0489:52176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.690174] [ip-0A0C04AD:13909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.690578] [ip-0A0C04A5:39551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.691257] [ip-0A0C0495:36198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.691243] [ip-0A0C04AA:11668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.692018] [ip-0A0C04CC:66975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.691945] [ip-0A0C04AE:42085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.692896] [ip-0A0C04CC:66977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.692565] [ip-0A0C0472:53834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.692690] [ip-0A0C0487:51802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.691730] [ip-0A0C0498:46772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.693130] [ip-0A0C04C4:69357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.694540] [ip-0A0C048C:32936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.694813] [ip-0A0C046B:42522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.702119] [ip-0A0C04A2:28362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.695931] [ip-0A0C0441:45112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.697014] [ip-0A0C049B:25619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.698695] [ip-0A0C04B3:44608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.699638] [ip-0A0C0496:18522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.699191] [ip-0A0C04A9:36005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.700598] [ip-0A0C04DB:59187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.702129] [ip-0A0C04C0:68719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.702772] [ip-0A0C048A:48613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.703336] [ip-0A0C04A6:36524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.704048] [ip-0A0C04C4:69364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.703867] [ip-0A0C0443:15491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.704868] [ip-0A0C0499:38355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.706020] [ip-0A0C04AF:28562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.706937] [ip-0A0C04DA:60593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.707192] [ip-0A0C04A9:36007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708014] [ip-0A0C0441:45111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708016] [ip-0A0C04C5:71282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708822] [ip-0A0C04D8:66954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.709286] [ip-0A0C04D8:66957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708769] [ip-0A0C0490:38036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.709514] [ip-0A0C0438:52756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.709021] [ip-0A0C0490:38038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.709900] [ip-0A0C04B3:44607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708927] [ip-0A0C0498:46769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.709593] [ip-0A0C0438:52751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708588] [ip-0A0C04B2:13008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.708692] [ip-0A0C04B2:13036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.710121] [ip-0A0C04C4:69362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.710646] [ip-0A0C04BE:69925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.710838] [ip-0A0C0443:15489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.711425] [ip-0A0C04B4:46347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.711708] [ip-0A0C04BE:69926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.712551] [ip-0A0C04A9:36015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.713476] [ip-0A0C045F:49758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.713866] [ip-0A0C045F:49760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715032] [ip-0A0C0472:53830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.714923] [ip-0A0C0441:45113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715284] [ip-0A0C0487:51809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715700] [ip-0A0C0441:45116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715885] [ip-0A0C04C5:71277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.716112] [ip-0A0C0472:53827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.716407] [ip-0A0C04A4:37051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.716357] [ip-0A0C04C3:68180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715816] [ip-0A0C04B2:13009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.718251] [ip-0A0C04A4:37052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.718011] [ip-0A0C0429:42389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.717913] [ip-0A0C0466:54481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.715948] [ip-0A0C0484:53881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.719645] [ip-0A0C0487:51808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.721023] [ip-0A0C04B4:46348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.720716] [ip-0A0C048C:32932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.721313] [ip-0A0C04C0:68713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.720826] [ip-0A0C0493:33458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.721220] [ip-0A0C04B0:23379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.721328] [ip-0A0C04DA:60595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.722057] [ip-0A0C04BA:4321 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.722432] [ip-0A0C04C0:68716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.723143] [ip-0A0C045F:49764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.723024] [ip-0A0C0493:33464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.723242] [ip-0A0C045F:49759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.724307] [ip-0A0C0487:51807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.724500] [ip-0A0C04AF:28567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.724523] [ip-0A0C0482:39478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727063] [ip-0A0C0496:18526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.726185] [ip-0A0C0473:50309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.726342] [ip-0A0C0466:54484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.726880] [ip-0A0C046B:42528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727110] [ip-0A0C04B4:46345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727439] [ip-0A0C0486:51803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727155] [ip-0A0C04DA:60598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727454] [ip-0A0C046B:42521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727612] [ip-0A0C0438:52753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727862] [ip-0A0C0438:52752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727501] [ip-0A0C0461:15814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.728160] [ip-0A0C04B1:40213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727881] [ip-0A0C0499:38357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727953] [ip-0A0C04B5:37535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.728738] [ip-0A0C04B3:44609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.727903] [ip-0A0C04A8:42170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.728701] [ip-0A0C04B5:37534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.728916] [ip-0A0C04A5:39547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.730513] [ip-0A0C0496:18524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.730013] [ip-0A0C04A1:54580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.729562] [ip-0A0C04A5:39548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.729836] [ip-0A0C04B5:37533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.730301] [ip-0A0C0429:42392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731366] [ip-0A0C04B3:44613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731524] [ip-0A0C0486:51807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731295] [ip-0A0C04BE:69929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731242] [ip-0A0C0429:42390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731413] [ip-0A0C04BA:4315 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731492] [ip-0A0C0472:53828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.732926] [ip-0A0C0496:18520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.729870] [ip-0A0C0484:53883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.732499] [ip-0A0C0429:42394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.732570] [ip-0A0C0466:54477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.731015] [ip-0A0C0484:53882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.734648] [ip-0A0C04A4:37048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.735084] [ip-0A0C0466:54480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.736751] [ip-0A0C04BA:4316 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.737281] [ip-0A0C0485:16872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.736063] [ip-0A0C04B2:13010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.744392] [ip-0A0C04A2:28357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.738350] [ip-0A0C04B1:40219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.738592] [ip-0A0C0489:52173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.739181] [ip-0A0C04AF:28561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.738926] [ip-0A0C0489:52174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.739926] [ip-0A0C04B4:46346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.740075] [ip-0A0C04C7:68216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.740438] [ip-0A0C04AF:28568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.740517] [ip-0A0C04BD:67207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.740900] [ip-0A0C04AC:30359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.740349] [ip-0A0C04B5:37530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.739927] [ip-0A0C04B2:13007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.741071] [ip-0A0C04C5:71286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.741938] [ip-0A0C04A6:36522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.742011] [ip-0A0C04C5:71303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.742455] [ip-0A0C0443:15490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749127] [ip-0A0C04A2:28358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749247] [ip-0A0C04A2:28361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.742862] [ip-0A0C0482:39475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.743331] [ip-0A0C04C5:71285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.743797] [ip-0A0C0499:38356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.744216] [ip-0A0C04B9:69970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.745119] [ip-0A0C04C0:68714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.745046] [ip-0A0C04BE:69928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.743880] [ip-0A0C049B:25623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.745328] [ip-0A0C0449:49494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.746153] [ip-0A0C04BA:4320 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.745857] [ip-0A0C042D:23340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.746772] [ip-0A0C048A:48607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.746560] [ip-0A0C048C:32930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.746866] [ip-0A0C048C:32931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.747457] [ip-0A0C04A4:37054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.748126] [ip-0A0C04A6:36526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.747653] [ip-0A0C0482:39480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.748070] [ip-0A0C04AB:27285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749079] [ip-0A0C04A6:36523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.748638] [ip-0A0C04B0:23378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.747223] [ip-0A0C04BC:65134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.748866] [ip-0A0C04B0:23380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.748762] [ip-0A0C04B5:37532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749340] [ip-0A0C04A5:39546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749809] [ip-0A0C04BA:4319 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.747299] [ip-0A0C0484:53884:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.750059] [ip-0A0C048C:32937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.749989] [ip-0A0C0489:52172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.751415] [ip-0A0C04A6:36521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.750750] [ip-0A0C04B0:23375:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.751383] [ip-0A0C04C0:68715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.752826] [ip-0A0C04AB:27286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.752727] [ip-0A0C0489:52175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.753191] [ip-0A0C0473:50310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.753719] [ip-0A0C04A4:37047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.754429] [ip-0A0C04C0:68717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.754529] [ip-0A0C04BE:69927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.755106] [ip-0A0C0482:39477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.756263] [ip-0A0C0493:33463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.756813] [ip-0A0C04AF:28563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.756379] [ip-0A0C0493:33465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.755993] [ip-0A0C04BC:65129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.757892] [ip-0A0C04BE:69924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.758965] [ip-0A0C0485:16868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.759081] [ip-0A0C04AB:27288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.759547] [ip-0A0C04BD:67206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.759912] [ip-0A0C04A4:37049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.760275] [ip-0A0C04AB:27289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.760940] [ip-0A0C048A:48611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.760824] [ip-0A0C04AE:42087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.761292] [ip-0A0C048D:37832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.763988] [ip-0A0C0486:51805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.763665] [ip-0A0C042D:23341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.765516] [ip-0A0C048D:37833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.766355] [ip-0A0C04A4:37050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.767936] [ip-0A0C04B1:40218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.765227] [ip-0A0C0484:53887:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.769804] [ip-0A0C048A:48610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.769418] [ip-0A0C0482:39476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.770088] [ip-0A0C0443:15486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.770206] [ip-0A0C0443:15493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.770156] [ip-0A0C04A5:39544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.771005] [ip-0A0C04A1:54571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.770339] [ip-0A0C04A8:42167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.770429] [ip-0A0C0484:53885:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.773116] [ip-0A0C04B9:69972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.774038] [ip-0A0C048A:48608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.774098] [ip-0A0C04AC:30358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.773483] [ip-0A0C049B:25618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.775641] [ip-0A0C04AB:27293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.775071] [ip-0A0C049B:25624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.776783] [ip-0A0C04AE:42082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.777338] [ip-0A0C0443:15488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.778976] [ip-0A0C0485:16874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.778944] [ip-0A0C0473:50312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.778581] [ip-0A0C04DB:59177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.780368] [ip-0A0C04BD:67201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.781646] [ip-0A0C04B7:7719 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.781289] [ip-0A0C049B:25620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.782089] [ip-0A0C04A5:39549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.782646] [ip-0A0C0473:50306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.784563] [ip-0A0C0486:51802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.784285] [ip-0A0C04B7:7723 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.784332] [ip-0A0C0461:15812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.785825] [ip-0A0C04AE:42084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.785936] [ip-0A0C04AE:42083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.787928] [ip-0A0C0499:38359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.789700] [ip-0A0C04AC:30356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.789723] [ip-0A0C0461:15813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.790751] [ip-0A0C04C7:68215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.792623] [ip-0A0C04B9:69973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.793516] [ip-0A0C049B:25622:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.795044] [ip-0A0C04A8:42166:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.797103] [ip-0A0C04BD:67203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.797667] [ip-0A0C04AC:30357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.797337] [ip-0A0C04BD:67205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.799101] [ip-0A0C04B1:40242:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.798927] [ip-0A0C0490:38043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.800984] [ip-0A0C0486:51801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.801323] [ip-0A0C0486:51806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.800263] [ip-0A0C04BC:65135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.802915] [ip-0A0C0485:16871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.802789] [ip-0A0C048D:37834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.802071] [ip-0A0C04BC:65137:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.804112] [ip-0A0C042D:23338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805201] [ip-0A0C04AC:30360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805493] [ip-0A0C0485:16869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805136] [ip-0A0C04C7:68214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805021] [ip-0A0C042D:23336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805392] [ip-0A0C0449:49493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.805939] [ip-0A0C0473:50311:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.807425] [ip-0A0C04AC:30362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.808193] [ip-0A0C0449:49490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.809905] [ip-0A0C04A1:54574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.809687] [ip-0A0C0473:50305:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.811157] [ip-0A0C04C7:68213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.810909] [ip-0A0C04B9:69976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.812110] [ip-0A0C04C3:68185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.811072] [ip-0A0C04BC:65130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.812797] [ip-0A0C04B9:69971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.817556] [ip-0A0C048D:37837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.817436] [ip-0A0C04B7:7724 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.817296] [ip-0A0C0490:38041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.817933] [ip-0A0C0449:49495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.818984] [ip-0A0C0485:16873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.819126] [ip-0A0C042D:23337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.819242] [ip-0A0C0449:49492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.818506] [ip-0A0C04DB:59174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.818420] [ip-0A0C04BC:65128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.820261] [ip-0A0C04C7:68217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.823635] [ip-0A0C04DB:59175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.825566] [ip-0A0C048D:37831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.825287] [ip-0A0C0499:38353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.825563] [ip-0A0C0499:38352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.827176] [ip-0A0C048D:37848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.826615] [ip-0A0C04A8:42165:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.829052] [ip-0A0C04A1:54570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.830360] [ip-0A0C04A8:42163:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.831303] [ip-0A0C04B7:7722 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.831402] [ip-0A0C04B7:7721 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.833865] [ip-0A0C04B1:40214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.836366] [ip-0A0C04C3:68184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.837786] [ip-0A0C04B7:7720 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.838753] [ip-0A0C04DB:59179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.840598] [ip-0A0C04B1:40212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.840843] [ip-0A0C04A1:54572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.843928] [ip-0A0C04B1:40216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.845907] [ip-0A0C04A1:54573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.846050] [ip-0A0C04A1:54592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.845102] [ip-0A0C04A8:42169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.846277] [ip-0A0C0499:38354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.846798] [ip-0A0C0490:38037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.847566] [ip-0A0C0490:38039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.848107] [ip-0A0C04C3:68187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.848195] [ip-0A0C04C3:68183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.850075] [ip-0A0C0490:38042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.852862] [ip-0A0C04C3:68186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.852652] [ip-0A0C0461:15808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.852733] [ip-0A0C04A8:42168:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.853034] [ip-0A0C04DB:59176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.870399] [ip-0A0C0461:15809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.871164] [ip-0A0C0461:15810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623592.874826] [ip-0A0C0461:15811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634623593783, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634623593824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634623593824, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634623593825, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623593825, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634623593825, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634623593825, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:45] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:06:46] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:85910 - context.c:584] INFO job (ID: 867565247494025871) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85910 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85910 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85906 - context.c:584] INFO job (ID: 867564274741699515) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85906 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85906 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85907 - context.c:584] INFO job (ID: 867564411445547090) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85907 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85907 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85932 - context.c:584] INFO job (ID: 867564892735615894) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85932 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85932 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85903 - context.c:584] INFO job (ID: 867565214052820253) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85903 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85903 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85908 - context.c:584] INFO job (ID: 867564459669734662) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85908 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85908 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85931 - context.c:584] INFO job (ID: 867564762307999923) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85931 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85931 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:85904 - context.c:584] INFO job (ID: 867564935386046034) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:85904 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:85904 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686530, "event_type": "POINT_IN_TIME", "key": "seed", "value": 240163675, "metadata": {"file": "main.py", "lineno": 72}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623686531, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:08:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623710688, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634623710708, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623710712, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634623710713, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623713298, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634623713298, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634623713298, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623713299, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623714948, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2037.6993912000537, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623714948, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623714948, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2037.6993912000537, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634623714948, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623714948, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623715623, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4983.114751373968, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623715623, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623715623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4983.114751373968, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623715623, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623715624, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623716274, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5168.225293271156, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623716274, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623716274, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5168.225293271156, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623716274, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623716275, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623716908, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5306.372883576816, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623716908, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623716908, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5306.372883576816, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634623716909, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623716909, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623717534, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.14802588772, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623717534, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623717534, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.14802588772, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634623717534, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623717534, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623718149, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5468.657428103114, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623718149, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623718149, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5468.657428103114, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634623718149, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623718149, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623718763, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5478.868821924711, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623718763, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623718763, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5478.868821924711, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623718763, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623718763, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623719375, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5492.905277731006, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623719375, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623719375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5492.905277731006, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634623719376, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623719376, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623719997, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.677778084639, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623719998, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623719998, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.677778084639, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634623719998, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623719998, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623720611, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5484.292944210498, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623720612, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623720612, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5484.292944210498, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634623720612, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623720612, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623721230, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5437.824385831961, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623721231, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623721231, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5437.824385831961, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634623721231, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623721231, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623721845, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5475.718212799389, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623721846, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623721846, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5475.718212799389, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634623721846, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623721846, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623722467, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.559870811466, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623722468, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623722468, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.559870811466, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634623722468, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623722468, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623723073, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5554.773944137179, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623723074, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623723074, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5554.773944137179, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634623723074, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623723075, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623723687, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5486.880856854191, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623723687, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623723688, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5486.880856854191, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634623723688, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623723688, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623724306, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5439.434197560741, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623724306, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623724306, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5439.434197560741, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634623724306, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623724307, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623724933, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.599128926555, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623724934, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623724934, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.599128926555, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634623724934, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623724934, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623725547, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5479.316161556074, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623725548, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623725548, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5479.316161556074, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634623725548, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623725548, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623726167, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.43861504469, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623726168, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623726168, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.43861504469, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634623726168, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623726168, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623726784, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.203654181533, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623726784, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623726785, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.203654181533, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634623726785, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623726785, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623727401, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5457.078449807395, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623727401, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623727401, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5457.078449807395, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634623727401, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623727402, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623728025, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5386.520547945205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623728026, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623728026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5386.520547945205, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634623728026, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623728026, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623728651, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5379.186751184307, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623728652, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623728652, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5379.186751184307, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634623728652, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623728652, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623729268, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5457.359507334186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623729268, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623729268, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5457.359507334186, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634623729268, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623729268, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623729884, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5460.019363829621, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623729884, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623729885, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5460.019363829621, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634623729885, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623729885, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623730499, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5472.658356438873, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623730499, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623730500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5472.658356438873, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634623730500, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623730500, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623731127, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5357.263963311903, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623731128, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623731128, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5357.263963311903, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634623731128, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623731128, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623731743, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5467.933893724792, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623731743, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623731743, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5467.933893724792, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634623731743, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623731743, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623732355, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5496.369365268984, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623732355, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623732355, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5496.369365268984, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634623732356, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623732356, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623732975, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5427.090137640646, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623732975, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623732976, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5427.090137640646, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634623732976, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623732976, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623733595, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5425.892863261766, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623733596, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623733596, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5425.892863261766, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634623733596, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623733596, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623734206, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5510.894394145639, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623734206, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623734206, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5510.894394145639, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634623734207, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623734207, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623734811, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5564.3912729839885, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623734811, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623734811, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5564.3912729839885, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634623734811, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623734811, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623735423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5499.578128688974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623735423, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623735423, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5499.578128688974, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634623735423, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623735423, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623736049, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5372.020331007837, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623736049, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623736050, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5372.020331007837, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634623736050, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623736050, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623736655, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5553.012001763679, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623736656, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623736656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5553.012001763679, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634623736656, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623736656, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623737285, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5339.572345039866, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623737286, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623737286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5339.572345039866, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634623737286, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623737286, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623737903, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.419736336786, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623737903, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623737903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.419736336786, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634623737903, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623737903, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623738513, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5508.245032348995, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623738514, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623738514, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5508.245032348995, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634623738514, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623738514, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623739128, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5480.136473966277, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623739128, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623739128, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5480.136473966277, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634623739128, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623739128, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623739738, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5513.630432503235, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623739738, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623739738, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5513.630432503235, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634623739738, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623739739, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623740350, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5500.093252338236, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623740350, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623740350, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5500.093252338236, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634623740350, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623740350, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623740966, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5462.588648262501, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623740966, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623740966, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5462.588648262501, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634623740966, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623740966, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623741573, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5536.478116298219, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623741574, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623741574, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5536.478116298219, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634623741574, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623741574, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623742191, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.189979255583, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623742191, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623742191, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.189979255583, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634623742192, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623742192, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623742799, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5536.045316431331, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623742799, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623742799, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5536.045316431331, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634623742799, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623742799, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623743410, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.482594913847, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623743410, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623743410, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.482594913847, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634623743411, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623743411, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623744018, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5529.138386753164, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623744019, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623744019, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5529.138386753164, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634623744019, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623744019, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623744648, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5341.17915657582, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623744648, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623744648, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5341.17915657582, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634623744649, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623744649, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623745258, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5511.685388512523, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623745259, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623745259, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5511.685388512523, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634623745333, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623745333, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623745349, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623745777, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8773295283317566, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623745777, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623745935, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5591.504479833805, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623745935, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623745935, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5591.504479833805, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623746119, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623746119, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623746121, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623746535, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8464022278785706, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623746535, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623746779, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5093.237719892662, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623746780, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623746780, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5093.237719892662, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623746815, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623746815, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623746831, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623747231, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8589180111885071, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623747231, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623747411, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5646.033691443809, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623747411, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623747411, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5646.033691443809, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623747523, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623747524, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623747537, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623747937, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8772308826446533, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623747938, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623748137, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5483.2921709506, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623748137, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623748137, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5483.2921709506, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623748173, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623748174, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623748190, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623748627, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8762366771697998, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623748627, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623748819, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5209.204277259694, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623748819, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623748819, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5209.204277259694, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623748846, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623748846, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623748861, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623749298, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8885817527770996, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623749298, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623749479, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5314.232667863038, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623749479, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623749479, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5314.232667863038, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623749505, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623749505, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623749520, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623749966, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8861652612686157, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623749966, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623750151, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5208.166639319181, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623750151, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623750151, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5208.166639319181, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623750198, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623750198, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623750214, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623750634, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8893808722496033, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623750634, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623750829, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5328.990378379809, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623750830, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623750830, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5328.990378379809, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623750857, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623750857, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623750874, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623751301, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8713504076004028, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623751302, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623751482, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.325033740019, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623751483, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623751483, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.325033740019, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623751512, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623751512, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623751527, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623751955, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8730577230453491, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623751955, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623752145, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5313.096684003927, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623752145, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623752145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5313.096684003927, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623752174, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623752174, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623752189, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623752627, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895035982131958, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623752628, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623752829, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5136.425292760532, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623752829, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623752829, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5136.425292760532, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623752855, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623752856, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623752871, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623753315, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8908653259277344, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623753315, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623753503, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5191.913105979361, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623753504, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623753504, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5191.913105979361, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623753530, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623753531, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623753544, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623753990, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8886458873748779, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623753991, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623754182, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5160.866761684481, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623754182, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623754182, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5160.866761684481, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623754209, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623754209, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623754223, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623754669, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8933815956115723, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623754670, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623754854, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5214.286775642496, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623754854, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623754855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5214.286775642496, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623754887, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623754887, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623754902, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623755326, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8657346367835999, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623755326, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623755525, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5265.703927253135, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623755526, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623755526, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5265.703927253135, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623755553, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623755554, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623755569, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623756005, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8816705942153931, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623756005, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623756196, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5229.174815262825, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623756197, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623756197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5229.174815262825, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623756226, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623756226, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623756241, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623756668, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8712172508239746, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623756668, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623756857, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5331.738338575712, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623756857, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623756857, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5331.738338575712, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623756888, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623756889, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623756904, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623757326, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8674033284187317, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623757327, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623757517, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5347.963097819013, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623757517, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623757518, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5347.963097819013, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623757552, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623757552, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623757568, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623757984, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910820484161377, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623757984, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623758173, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5408.5490070861015, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623758174, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623758174, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5408.5490070861015, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623758202, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623758202, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623758216, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623758653, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.884726345539093, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623758653, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623758846, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5218.986825937505, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623758846, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623758846, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5218.986825937505, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623758876, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623758876, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623758891, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623759316, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8709579110145569, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623759316, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623759506, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.880290740003, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623759506, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623759506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.880290740003, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623759537, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623759537, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623759553, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623759975, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8868404626846313, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623759975, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623760157, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5423.6480924873285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623760157, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623760157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5423.6480924873285, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623760195, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623760196, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623760211, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623760611, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8736010193824768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623760611, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623760799, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5565.419673416873, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623760800, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623760800, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5565.419673416873, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623760828, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623760828, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623760842, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623761278, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8893829584121704, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623761278, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623761500, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5000.254198349506, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623761501, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623761501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5000.254198349506, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623761526, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623761526, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623761542, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623761987, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8660463690757751, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623761987, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623762170, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5221.918504218557, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623762170, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623762171, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5221.918504218557, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623762199, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623762200, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623762214, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623762641, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8871582746505737, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623762641, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623762827, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5353.3445114099195, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623762828, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623762828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5353.3445114099195, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623762855, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623762855, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623762869, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623763298, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8979365825653076, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623763298, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623763486, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5327.463391049033, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623763486, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623763486, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5327.463391049033, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623763516, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623763516, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623763532, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623763955, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8931018114089966, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623763955, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623764201, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4904.978443707745, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623764201, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623764201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4904.978443707745, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623764231, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623764231, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623764247, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623764670, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8921445608139038, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623764670, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623764883, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5152.999647151526, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623764883, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623764884, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5152.999647151526, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623764919, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623764919, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623764933, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623765333, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8842111229896545, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623765334, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623765549, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5328.355706118992, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623765550, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623765550, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5328.355706118992, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623765585, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623765585, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623765601, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623766015, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8818973302841187, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623766015, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623766203, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5436.859375578682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623766204, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623766204, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5436.859375578682, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623766240, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623766241, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623766255, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623766655, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8928733468055725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623766655, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623766847, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5539.067628935025, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623766848, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623766848, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5539.067628935025, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623766877, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623766877, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623766893, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623767316, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8944913744926453, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623767316, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623767499, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.910018504482, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623767500, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623767500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.910018504482, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623767536, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623767536, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623767550, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623767952, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8854895830154419, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623767952, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623768139, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5572.358111650704, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623768140, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623768140, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5572.358111650704, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623768169, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623768169, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623768185, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623768607, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8914010524749756, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623768607, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623768792, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.609410438, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623768793, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623768793, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.609410438, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623768831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623768832, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623768846, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623769266, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8862611055374146, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623769266, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623769448, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.679124042664, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623769449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623769449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.679124042664, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623769479, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623769480, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623769495, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623769911, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.895257830619812, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623769911, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623770101, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5411.780709555825, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623770101, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623770101, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5411.780709555825, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623770130, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623770131, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623770146, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623770570, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910458087921143, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623770570, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623770750, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5423.944504569968, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623770751, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623770751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5423.944504569968, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623770780, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623770780, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623770796, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623771218, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8816466331481934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623771219, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623771399, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.036732019847, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623771399, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623771400, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.036732019847, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623771428, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623771429, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623771444, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623771867, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8987449407577515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623771867, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623772048, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5425.37065774556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623772049, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623772049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5425.37065774556, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623772079, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623772080, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623772094, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623772514, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8932265043258667, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623772514, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623772694, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5472.486221570886, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623772694, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623772695, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5472.486221570886, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623772722, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623772722, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623772737, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623773174, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8974188566207886, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623773174, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623773354, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.035635769857, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623773355, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623773355, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.035635769857, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623773383, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623773383, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623773398, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623773826, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9038227796554565, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623773826, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623774009, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5367.416850495423, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623774010, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623774010, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5367.416850495423, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623774039, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623774040, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623774055, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623774478, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9057927131652832, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623774478, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623774666, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.091264603059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623774667, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623774667, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.091264603059, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623774703, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623774703, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623774718, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623775118, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9117006659507751, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623775118, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623775118, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634623775298, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5654.28610977038, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623775298, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623775298, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5654.28610977038, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:41 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:42 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:42 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:44 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:45 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:47 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:48 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
ENDING TIMING RUN AT 2021-10-19 06:09:49 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:50 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
ENDING TIMING RUN AT 2021-10-19 06:09:51 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:52 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
ENDING TIMING RUN AT 2021-10-19 06:09:54 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:06:27 AM
