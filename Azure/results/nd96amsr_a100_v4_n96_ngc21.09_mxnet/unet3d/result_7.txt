+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019054810801182498
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019054810801182498
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019054810801182498
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019054810801182498
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07363/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019054810801182498_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C0475
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C047A
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:48:14 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634622499.287206] [ip-0A0C040C:70730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.303422] [ip-0A0C040C:70728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.359081] [ip-0A0C040E:46253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.369156] [ip-0A0C040B:4270 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.374771] [ip-0A0C040B:4273 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.425680] [ip-0A0C040C:70727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.432163] [ip-0A0C040E:46258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.434637] [ip-0A0C040C:70733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.456883] [ip-0A0C044E:35696:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.475158] [ip-0A0C040C:70731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.485446] [ip-0A0C040C:70729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.486958] [ip-0A0C0410:74976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.487551] [ip-0A0C040C:70732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.487801] [ip-0A0C040C:70734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.489331] [ip-0A0C040A:59787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.500405] [ip-0A0C040B:4271 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.502815] [ip-0A0C040E:46256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.503822] [ip-0A0C047F:27683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.506120] [ip-0A0C040B:4272 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.512519] [ip-0A0C040E:46260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.513238] [ip-0A0C0409:21259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.517749] [ip-0A0C0409:21264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.520190] [ip-0A0C040B:4269 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.520495] [ip-0A0C044E:35698:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.521371] [ip-0A0C0416:45405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.522394] [ip-0A0C0410:74980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.527700] [ip-0A0C042E:57372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.529224] [ip-0A0C044C:40535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.529977] [ip-0A0C040E:46254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.530720] [ip-0A0C0476:18652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.532976] [ip-0A0C0408:82494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.534321] [ip-0A0C0410:74978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.535471] [ip-0A0C046D:27184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.540489] [ip-0A0C047F:27686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.546711] [ip-0A0C0412:37772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.547821] [ip-0A0C040E:46259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.548988] [ip-0A0C040B:4268 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.552248] [ip-0A0C0407:63019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.556075] [ip-0A0C040A:59785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.556416] [ip-0A0C040A:59784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.557823] [ip-0A0C040E:46257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.560060] [ip-0A0C0414:74996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.560050] [ip-0A0C044A:37584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.561352] [ip-0A0C040E:46255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.561893] [ip-0A0C0457:38633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.563846] [ip-0A0C0423:52124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.563576] [ip-0A0C044E:35700:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.564300] [ip-0A0C047B:31066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.564449] [ip-0A0C0416:45404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.565567] [ip-0A0C040B:4266 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.568004] [ip-0A0C044C:40531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.571152] [ip-0A0C047F:27688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.572037] [ip-0A0C040B:4267 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.573012] [ip-0A0C040A:59781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.572507] [ip-0A0C046A:33494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.574242] [ip-0A0C044C:40537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.574628] [ip-0A0C0428:47101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.574911] [ip-0A0C0420:59472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.576282] [ip-0A0C0475:24628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.577252] [ip-0A0C0414:74994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.577513] [ip-0A0C0475:24630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.583009] [ip-0A0C0448:50059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.585389] [ip-0A0C0407:63017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.585898] [ip-0A0C0408:82502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.585971] [ip-0A0C044E:35697:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.586736] [ip-0A0C0409:21260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.587933] [ip-0A0C0420:59478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.588839] [ip-0A0C0445:48849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.596487] [ip-0A0C042E:57348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.598494] [ip-0A0C045B:46276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.598566] [ip-0A0C043F:39421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.600980] [ip-0A0C0459:38644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.601648] [ip-0A0C0411:57894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.606210] [ip-0A0C0448:50061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.606355] [ip-0A0C047C:34376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.607786] [ip-0A0C044A:37578:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.608381] [ip-0A0C0407:63020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.608405] [ip-0A0C0442:38129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.608833] [ip-0A0C0412:37774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.610752] [ip-0A0C047C:34373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.612497] [ip-0A0C0456:46014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.613965] [ip-0A0C0428:47120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.613401] [ip-0A0C0412:37791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.613968] [ip-0A0C046A:33493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.614087] [ip-0A0C046A:33491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.616309] [ip-0A0C0428:47098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.616487] [ip-0A0C047B:31065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.618221] [ip-0A0C0457:38636:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.618416] [ip-0A0C043E:32082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.620038] [ip-0A0C0476:18653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.623203] [ip-0A0C0477:29177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.623184] [ip-0A0C0477:29181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.623445] [ip-0A0C0476:18654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.624762] [ip-0A0C045B:46274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.624512] [ip-0A0C041B:76291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.625829] [ip-0A0C047F:27689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.629277] [ip-0A0C0408:82495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.629804] [ip-0A0C042A:48888:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.630781] [ip-0A0C0474:24454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.632547] [ip-0A0C0432:78351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.632540] [ip-0A0C044E:35702:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.634888] [ip-0A0C0453:43968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.635305] [ip-0A0C0442:38131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.635909] [ip-0A0C041B:76286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.637338] [ip-0A0C045B:46271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.637620] [ip-0A0C0518:56950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.639513] [ip-0A0C042A:48889:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.644586] [ip-0A0C0457:38637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.647480] [ip-0A0C0459:38640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.648467] [ip-0A0C0423:52130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.647976] [ip-0A0C040A:59783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.647808] [ip-0A0C0410:74979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.648461] [ip-0A0C0457:38635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.650323] [ip-0A0C0481:22508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.651638] [ip-0A0C0414:75001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.653197] [ip-0A0C044B:41763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.653870] [ip-0A0C0431:48794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.654493] [ip-0A0C0409:21285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.654485] [ip-0A0C041A:81725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.655399] [ip-0A0C0432:78350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.655954] [ip-0A0C043F:39416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.655996] [ip-0A0C046D:27177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.656832] [ip-0A0C0445:48879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.657930] [ip-0A0C0420:59477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.659410] [ip-0A0C0456:46011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.659192] [ip-0A0C042E:57349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.660510] [ip-0A0C0463:32936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.661083] [ip-0A0C044C:40544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.660623] [ip-0A0C043A:43660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.661836] [ip-0A0C046D:27179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.662599] [ip-0A0C0411:57890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.663690] [ip-0A0C0433:52315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.663719] [ip-0A0C0433:52313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.665333] [ip-0A0C041D:54997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.666169] [ip-0A0C041A:81731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.667719] [ip-0A0C044E:35701:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.667835] [ip-0A0C044E:35695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.670355] [ip-0A0C044E:35699:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672060] [ip-0A0C043B:43735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672037] [ip-0A0C0410:74977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672596] [ip-0A0C0409:21266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672131] [ip-0A0C0410:74982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672070] [ip-0A0C043B:43711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.672501] [ip-0A0C0410:74975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.673019] [ip-0A0C0419:62029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.673764] [ip-0A0C040A:59780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.674017] [ip-0A0C040A:59786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.674795] [ip-0A0C047C:34374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.674922] [ip-0A0C043E:32083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.674983] [ip-0A0C040A:59782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.676367] [ip-0A0C0409:21263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.676449] [ip-0A0C043E:32077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.676524] [ip-0A0C0477:29183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.676007] [ip-0A0C042E:57345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678091] [ip-0A0C0431:48793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678180] [ip-0A0C047F:27684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678127] [ip-0A0C041B:76293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.677894] [ip-0A0C0459:38643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678189] [ip-0A0C047F:27687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678819] [ip-0A0C0455:37392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678923] [ip-0A0C0410:74981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.678801] [ip-0A0C0455:37389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.679756] [ip-0A0C0518:56947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.679933] [ip-0A0C0477:29176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.680836] [ip-0A0C0469:28025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.680924] [ip-0A0C047B:31063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.682495] [ip-0A0C0416:45399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.683101] [ip-0A0C047A:29376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.685733] [ip-0A0C041E:58097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.686746] [ip-0A0C044A:37579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.686076] [ip-0A0C0442:38125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.688461] [ip-0A0C0448:50056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.688778] [ip-0A0C0476:18647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.689450] [ip-0A0C044B:41770:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.690418] [ip-0A0C041C:64109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.690776] [ip-0A0C0416:45401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.691455] [ip-0A0C0458:46410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.692135] [ip-0A0C0440:37691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.692917] [ip-0A0C047F:27685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.692952] [ip-0A0C0421:60255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.693305] [ip-0A0C0407:63018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.693349] [ip-0A0C047F:27682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.694125] [ip-0A0C0409:21261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.694787] [ip-0A0C0408:82498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.696722] [ip-0A0C0409:21262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.697339] [ip-0A0C0423:52126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.697152] [ip-0A0C0477:29175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.699613] [ip-0A0C0447:45392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.700554] [ip-0A0C042E:57342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.700908] [ip-0A0C046A:33489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.702102] [ip-0A0C0458:46404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.702683] [ip-0A0C0416:45400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.701514] [ip-0A0C042E:57343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.703289] [ip-0A0C0450:14110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.703310] [ip-0A0C0456:46008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.703956] [ip-0A0C0454:42875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.704205] [ip-0A0C0426:56786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.705803] [ip-0A0C0452:42093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.705953] [ip-0A0C0427:55127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.707599] [ip-0A0C0518:56942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.707544] [ip-0A0C0470:23430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.707828] [ip-0A0C0475:24627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.708233] [ip-0A0C044C:40538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.709834] [ip-0A0C043A:43659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.710555] [ip-0A0C0451:40891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.710376] [ip-0A0C042F:53437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.710871] [ip-0A0C0445:48853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.711320] [ip-0A0C041A:81724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.711678] [ip-0A0C043D:34679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.711665] [ip-0A0C043D:34678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.712703] [ip-0A0C044C:40533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.712284] [ip-0A0C0465:30412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.714043] [ip-0A0C0411:57892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.714212] [ip-0A0C0408:82497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.714714] [ip-0A0C043F:39423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.715084] [ip-0A0C0445:48855:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.715547] [ip-0A0C041D:55003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.715843] [ip-0A0C046D:27178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.716413] [ip-0A0C0418:54838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.717773] [ip-0A0C046D:27183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.718572] [ip-0A0C044C:40532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.719718] [ip-0A0C044C:40534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.719665] [ip-0A0C0454:42876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.719956] [ip-0A0C047B:31068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.719798] [ip-0A0C0476:18651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.720572] [ip-0A0C0407:63024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.720856] [ip-0A0C0417:88905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.721349] [ip-0A0C0453:43972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.720909] [ip-0A0C0417:88904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.721764] [ip-0A0C0475:24626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.722497] [ip-0A0C0423:52127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.724147] [ip-0A0C0416:45398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.724802] [ip-0A0C0412:37775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.726318] [ip-0A0C0416:45403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.726152] [ip-0A0C044F:36289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.726166] [ip-0A0C044F:36293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.726198] [ip-0A0C044F:36286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.726719] [ip-0A0C0427:55129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.727196] [ip-0A0C045B:46272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.727309] [ip-0A0C0448:50058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.727326] [ip-0A0C0459:38642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.728487] [ip-0A0C045E:39748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.728940] [ip-0A0C042F:53436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.729479] [ip-0A0C047B:31067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.730431] [ip-0A0C0474:24455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.730231] [ip-0A0C0412:37770:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.732225] [ip-0A0C0476:18650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.732469] [ip-0A0C0465:30420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.732428] [ip-0A0C0476:18649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.733936] [ip-0A0C0470:23426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.735233] [ip-0A0C0428:47100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.734447] [ip-0A0C0442:38124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.735331] [ip-0A0C044A:37583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.735004] [ip-0A0C0408:82500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.736356] [ip-0A0C043A:43662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.736575] [ip-0A0C047D:24965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.737679] [ip-0A0C0416:45402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.737692] [ip-0A0C0420:59474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.739763] [ip-0A0C0408:82499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.740361] [ip-0A0C0450:14107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.740203] [ip-0A0C0408:82501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.740452] [ip-0A0C046D:27181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.739947] [ip-0A0C042E:57344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.741872] [ip-0A0C0428:47103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.741902] [ip-0A0C042C:51512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.741689] [ip-0A0C0434:47020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.741689] [ip-0A0C0434:47021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.742015] [ip-0A0C0476:18648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.744576] [ip-0A0C0428:47096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.743554] [ip-0A0C042E:57346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.744619] [ip-0A0C0462:69792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.745444] [ip-0A0C0453:43966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.745272] [ip-0A0C0462:69799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.745603] [ip-0A0C046D:27182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.745780] [ip-0A0C045A:42402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.746613] [ip-0A0C0414:74999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.745973] [ip-0A0C046D:27180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.746082] [ip-0A0C0480:23607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.746143] [ip-0A0C0457:38634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.746265] [ip-0A0C0474:24453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.746870] [ip-0A0C0420:59473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.747103] [ip-0A0C0421:60258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.747543] [ip-0A0C0419:62027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.748740] [ip-0A0C0423:52125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749392] [ip-0A0C0414:74997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749002] [ip-0A0C0475:24625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749225] [ip-0A0C0463:32937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749005] [ip-0A0C0481:22507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749294] [ip-0A0C047C:34375:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.748902] [ip-0A0C0412:37773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.750140] [ip-0A0C0431:48799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749992] [ip-0A0C045D:29871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.749985] [ip-0A0C0481:22510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.750508] [ip-0A0C0421:60256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.750952] [ip-0A0C0407:63022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.750763] [ip-0A0C043F:39429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.751207] [ip-0A0C044A:37585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.751184] [ip-0A0C047B:31064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.751832] [ip-0A0C0457:38632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.752805] [ip-0A0C047B:31069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.753852] [ip-0A0C0463:32940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.754064] [ip-0A0C0456:46015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.753780] [ip-0A0C0412:37771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.756098] [ip-0A0C0474:24449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757153] [ip-0A0C044A:37581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757354] [ip-0A0C045B:46269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757361] [ip-0A0C045D:29870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757293] [ip-0A0C0467:28518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757670] [ip-0A0C042C:51511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757637] [ip-0A0C043C:44456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.757606] [ip-0A0C0412:37777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.759021] [ip-0A0C0414:75000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.758997] [ip-0A0C0407:63021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.759876] [ip-0A0C0424:47149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.760299] [ip-0A0C0457:38631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.760564] [ip-0A0C0465:30419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.760659] [ip-0A0C0457:38638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.761179] [ip-0A0C0479:24971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.761946] [ip-0A0C0423:52129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.761843] [ip-0A0C046A:33490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.763063] [ip-0A0C0428:47097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.762113] [ip-0A0C046A:33492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.763742] [ip-0A0C0446:46253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.763957] [ip-0A0C0411:57895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.763830] [ip-0A0C0451:40895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.764112] [ip-0A0C0470:23425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.764297] [ip-0A0C0407:63023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.764615] [ip-0A0C0448:50060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.764480] [ip-0A0C043E:32076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.764932] [ip-0A0C043F:39419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.765189] [ip-0A0C0419:62032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.766154] [ip-0A0C0414:74998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.765803] [ip-0A0C0469:28027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.766533] [ip-0A0C0414:74995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.766543] [ip-0A0C047C:34377:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.767092] [ip-0A0C0418:54842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.767532] [ip-0A0C0452:42087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.768641] [ip-0A0C045E:39749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.768954] [ip-0A0C0475:24623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.769132] [ip-0A0C0475:24650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.770314] [ip-0A0C0420:59471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.770496] [ip-0A0C0420:59476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.770344] [ip-0A0C041F:62032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.772132] [ip-0A0C044A:37582:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.771977] [ip-0A0C047D:24968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.772158] [ip-0A0C047D:24964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.772905] [ip-0A0C042B:56117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.773436] [ip-0A0C047A:29377:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.773613] [ip-0A0C047A:29380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.774054] [ip-0A0C044A:37580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.774038] [ip-0A0C0453:43965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.774514] [ip-0A0C047B:31070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.774586] [ip-0A0C0420:59475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.774881] [ip-0A0C042A:48890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.775104] [ip-0A0C044B:41768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.775537] [ip-0A0C0428:47099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.775696] [ip-0A0C0448:50064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.776143] [ip-0A0C0419:62028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.777267] [ip-0A0C0413:90217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.776593] [ip-0A0C046A:33495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.777778] [ip-0A0C045E:39751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.777138] [ip-0A0C043A:43688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.777321] [ip-0A0C0422:82751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.778828] [ip-0A0C0426:56789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.778991] [ip-0A0C041B:76289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.779539] [ip-0A0C045A:42399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.779003] [ip-0A0C046A:33488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.779892] [ip-0A0C0474:24448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.779980] [ip-0A0C041F:62035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.780600] [ip-0A0C0426:56785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.780642] [ip-0A0C0451:40894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.782289] [ip-0A0C0423:52128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.781603] [ip-0A0C0445:48852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.782649] [ip-0A0C0477:29206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.782828] [ip-0A0C0475:24624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.783877] [ip-0A0C0423:52123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.783610] [ip-0A0C0424:47153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.784307] [ip-0A0C047C:34372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.784398] [ip-0A0C041D:55002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.785310] [ip-0A0C0442:38128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.786020] [ip-0A0C042A:48886:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.787553] [ip-0A0C0433:52314:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.787690] [ip-0A0C0411:57891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788092] [ip-0A0C041E:58101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788284] [ip-0A0C0477:29178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788423] [ip-0A0C0477:29174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788569] [ip-0A0C0439:35641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788232] [ip-0A0C0459:38639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.788557] [ip-0A0C0439:35642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.789090] [ip-0A0C0448:50063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.789224] [ip-0A0C0448:50062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.790782] [ip-0A0C0430:86183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.791604] [ip-0A0C045B:46273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.791269] [ip-0A0C0455:37390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.792254] [ip-0A0C045B:46270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.791560] [ip-0A0C0442:38126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.792679] [ip-0A0C047C:34370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.792639] [ip-0A0C0445:48854:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.793187] [ip-0A0C0518:56946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.793349] [ip-0A0C0445:48850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.793848] [ip-0A0C0469:28023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.793636] [ip-0A0C0445:48856:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.794108] [ip-0A0C0463:32935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.794068] [ip-0A0C047C:34371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.794874] [ip-0A0C045C:39175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.795437] [ip-0A0C0425:61430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.796234] [ip-0A0C043F:39417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.797950] [ip-0A0C041B:76290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.797855] [ip-0A0C041A:81727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.799050] [ip-0A0C043F:39420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.799766] [ip-0A0C0433:52320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.799344] [ip-0A0C0459:38638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.799885] [ip-0A0C043B:43710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.800302] [ip-0A0C042B:56115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.800085] [ip-0A0C0459:38645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.801048] [ip-0A0C0411:57914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.800732] [ip-0A0C0442:38130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.801800] [ip-0A0C045B:46275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.801273] [ip-0A0C041D:54998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.802257] [ip-0A0C045A:42405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.802502] [ip-0A0C0447:45397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.803703] [ip-0A0C0450:14111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.803744] [ip-0A0C0455:37391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.805015] [ip-0A0C0467:28513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.806892] [ip-0A0C041C:64113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.806919] [ip-0A0C0440:37692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.807349] [ip-0A0C0418:54840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.807321] [ip-0A0C0459:38641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.807401] [ip-0A0C0442:38127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.808236] [ip-0A0C041E:58104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.808534] [ip-0A0C0456:46012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.808476] [ip-0A0C043C:44455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.809141] [ip-0A0C0456:46009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.808910] [ip-0A0C0481:22514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.810671] [ip-0A0C0432:78352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.810981] [ip-0A0C043F:39418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.811416] [ip-0A0C0427:55130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.812750] [ip-0A0C043A:43663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.813340] [ip-0A0C0471:27798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.813326] [ip-0A0C0471:27794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.814217] [ip-0A0C0474:24450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.814229] [ip-0A0C0480:23603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.814926] [ip-0A0C043B:43714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.817104] [ip-0A0C0518:56949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.817112] [ip-0A0C0411:57897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.817238] [ip-0A0C0411:57893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.817304] [ip-0A0C042A:48884:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.819009] [ip-0A0C0456:46010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.819745] [ip-0A0C0433:52344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.820903] [ip-0A0C042A:48887:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.821244] [ip-0A0C0430:86185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.821438] [ip-0A0C0431:48822:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.821026] [ip-0A0C043A:43665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.822252] [ip-0A0C043E:32080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.822564] [ip-0A0C0456:46013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.823633] [ip-0A0C043E:32079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.824108] [ip-0A0C0481:22520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.824841] [ip-0A0C0432:78348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.824912] [ip-0A0C0432:78346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.825507] [ip-0A0C0431:48797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.826963] [ip-0A0C0458:46406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.827245] [ip-0A0C041A:81729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.827963] [ip-0A0C044B:41765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.828177] [ip-0A0C044B:41767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.829231] [ip-0A0C041B:76292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.829421] [ip-0A0C042A:48885:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.829574] [ip-0A0C041B:76288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.830011] [ip-0A0C0452:42101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.830497] [ip-0A0C0474:24452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.830564] [ip-0A0C0465:30415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.831022] [ip-0A0C0455:37386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.830966] [ip-0A0C0447:45396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.831644] [ip-0A0C041E:58100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.832154] [ip-0A0C0474:24451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.832644] [ip-0A0C041C:64114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.832635] [ip-0A0C041A:81730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.833059] [ip-0A0C0470:23427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.834951] [ip-0A0C047A:29378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.834851] [ip-0A0C0453:43973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835030] [ip-0A0C043E:32078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835295] [ip-0A0C043E:32075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835732] [ip-0A0C0454:42879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835859] [ip-0A0C0433:52316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835559] [ip-0A0C044B:41766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835571] [ip-0A0C0440:37685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.835881] [ip-0A0C043B:43712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.836254] [ip-0A0C0433:52317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.836399] [ip-0A0C041B:76287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.836621] [ip-0A0C0454:42874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.836716] [ip-0A0C0440:37690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.837877] [ip-0A0C042A:48883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.838150] [ip-0A0C043D:34676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.838274] [ip-0A0C0421:60251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.838705] [ip-0A0C0518:56945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.839630] [ip-0A0C0432:78347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.839802] [ip-0A0C0432:78349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.840209] [ip-0A0C0453:43969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.840626] [ip-0A0C0426:56788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.840302] [ip-0A0C0453:43971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.841291] [ip-0A0C0425:61424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.842274] [ip-0A0C0518:56943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.842854] [ip-0A0C044B:41764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.843316] [ip-0A0C0453:43967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.843074] [ip-0A0C0462:69794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.844696] [ip-0A0C0450:14105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.845409] [ip-0A0C041A:81728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.846763] [ip-0A0C0458:46405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.846955] [ip-0A0C0419:62030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.848896] [ip-0A0C0518:56944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.849420] [ip-0A0C0432:78353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.849473] [ip-0A0C0452:42086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.849756] [ip-0A0C0469:28024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.849795] [ip-0A0C043B:43709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.849868] [ip-0A0C043B:43713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.850977] [ip-0A0C0446:46254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.851087] [ip-0A0C0430:86208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.850995] [ip-0A0C0451:40899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.851421] [ip-0A0C043D:34685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.851840] [ip-0A0C0431:48798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.853388] [ip-0A0C0439:35644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.853525] [ip-0A0C0417:88908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.855081] [ip-0A0C044F:36292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.856559] [ip-0A0C041C:64112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.856955] [ip-0A0C0469:28022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.856819] [ip-0A0C041C:64111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.857324] [ip-0A0C0447:45394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.857682] [ip-0A0C0433:52319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.857616] [ip-0A0C0413:90219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.858078] [ip-0A0C0413:90222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.858464] [ip-0A0C0480:23632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859125] [ip-0A0C0431:48792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.858563] [ip-0A0C0417:88911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859097] [ip-0A0C043B:43708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859491] [ip-0A0C0463:32938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859312] [ip-0A0C041A:81726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859525] [ip-0A0C0481:22513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.860005] [ip-0A0C0479:24973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.859668] [ip-0A0C0481:22509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.860543] [ip-0A0C0431:48796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.860575] [ip-0A0C0463:32933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.860999] [ip-0A0C043C:44454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861158] [ip-0A0C0422:82749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861652] [ip-0A0C0455:37387:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861851] [ip-0A0C041E:58103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.862287] [ip-0A0C0479:24974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861544] [ip-0A0C043A:43661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861727] [ip-0A0C0417:88910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.861634] [ip-0A0C043A:43658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.862775] [ip-0A0C043D:34683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.862941] [ip-0A0C047A:29382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.863420] [ip-0A0C0481:22511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.864136] [ip-0A0C044F:36288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.865949] [ip-0A0C0418:54837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.867003] [ip-0A0C0446:46250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.866639] [ip-0A0C044B:41771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.868518] [ip-0A0C0465:30416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.868941] [ip-0A0C0455:37388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.869386] [ip-0A0C045E:39753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.869743] [ip-0A0C0479:24996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.870446] [ip-0A0C0450:14104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.870294] [ip-0A0C0421:60253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.870645] [ip-0A0C045A:42400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.872697] [ip-0A0C0467:28520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.873251] [ip-0A0C0427:55131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.873665] [ip-0A0C0434:47019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.874666] [ip-0A0C0419:62031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.876872] [ip-0A0C0471:27799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.877009] [ip-0A0C0463:32934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.876890] [ip-0A0C0462:69796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.877757] [ip-0A0C0455:37393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.877991] [ip-0A0C047A:29406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.878063] [ip-0A0C041E:58102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.877953] [ip-0A0C0419:62026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.878068] [ip-0A0C0419:62025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.878344] [ip-0A0C041D:54999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.879040] [ip-0A0C0463:32939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.879020] [ip-0A0C0427:55128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.879344] [ip-0A0C041E:58098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.879714] [ip-0A0C0452:42089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.879964] [ip-0A0C0469:28026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.880583] [ip-0A0C0446:46255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.880683] [ip-0A0C0469:28035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.882006] [ip-0A0C0458:46407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.883055] [ip-0A0C045E:39752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.883168] [ip-0A0C0469:28021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.883350] [ip-0A0C0458:46408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.883788] [ip-0A0C0426:56790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.883841] [ip-0A0C042C:51506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.885077] [ip-0A0C041C:64115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.885406] [ip-0A0C041D:55004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.885506] [ip-0A0C042F:53431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.886609] [ip-0A0C0454:42880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.886905] [ip-0A0C041E:58099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.886769] [ip-0A0C0447:45395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.887599] [ip-0A0C0450:14108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.888214] [ip-0A0C0434:47022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.888441] [ip-0A0C044F:36294:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889412] [ip-0A0C0454:42877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.888925] [ip-0A0C042F:53432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889399] [ip-0A0C041C:64110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889588] [ip-0A0C047A:29379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889652] [ip-0A0C047A:29381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889739] [ip-0A0C0458:46411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.889997] [ip-0A0C0413:90225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.890979] [ip-0A0C041C:64108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.891209] [ip-0A0C0427:55140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.890981] [ip-0A0C041D:55000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.890950] [ip-0A0C0417:88903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.891425] [ip-0A0C0421:60257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.891994] [ip-0A0C0480:23604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.892325] [ip-0A0C0418:54844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.892607] [ip-0A0C0418:54841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.892654] [ip-0A0C0458:46429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.893732] [ip-0A0C041D:55001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.895158] [ip-0A0C0440:37687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.895316] [ip-0A0C0421:60252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.897273] [ip-0A0C0450:14109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.897044] [ip-0A0C0452:42092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.897489] [ip-0A0C0418:54839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.896850] [ip-0A0C045C:39177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.897322] [ip-0A0C047D:24969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.898172] [ip-0A0C0454:42881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.898321] [ip-0A0C0427:55126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.898394] [ip-0A0C0465:30417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.899424] [ip-0A0C0426:56791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.899719] [ip-0A0C045E:39747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.899550] [ip-0A0C0470:23428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.899553] [ip-0A0C041F:62039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.900159] [ip-0A0C0450:14106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.900831] [ip-0A0C044F:36290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.900877] [ip-0A0C0465:30414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.900974] [ip-0A0C044F:36287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.901316] [ip-0A0C0451:40896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.901815] [ip-0A0C047D:24963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.902702] [ip-0A0C043D:34677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.903649] [ip-0A0C043D:34681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.903548] [ip-0A0C0421:60254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.903874] [ip-0A0C0418:54843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.903989] [ip-0A0C043D:34680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.904134] [ip-0A0C0425:61431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.904643] [ip-0A0C0417:88902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.905556] [ip-0A0C0440:37688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.905758] [ip-0A0C0447:45390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.905602] [ip-0A0C043C:44449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.905560] [ip-0A0C045C:39173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.907268] [ip-0A0C0446:46257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.906710] [ip-0A0C0465:30413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.907179] [ip-0A0C0479:24970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.907369] [ip-0A0C0454:42878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.907626] [ip-0A0C0470:23423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.908263] [ip-0A0C0434:47018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.908460] [ip-0A0C045D:29868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.910078] [ip-0A0C0417:88907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.910675] [ip-0A0C045D:29873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.910847] [ip-0A0C0447:45393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.911180] [ip-0A0C042C:51508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.911394] [ip-0A0C042F:53434:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.912168] [ip-0A0C0451:40892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.912253] [ip-0A0C0451:40898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.913516] [ip-0A0C0427:55125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.913482] [ip-0A0C042F:53435:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.913521] [ip-0A0C042F:53430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.915826] [ip-0A0C0440:37689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.915950] [ip-0A0C0440:37686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.916295] [ip-0A0C0451:40893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.916444] [ip-0A0C0470:23424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.916714] [ip-0A0C0467:28514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.917049] [ip-0A0C0470:23429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.918591] [ip-0A0C0413:90220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.920269] [ip-0A0C0452:42090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.920582] [ip-0A0C0424:47152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.920888] [ip-0A0C0452:42088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.920915] [ip-0A0C0422:82750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.921497] [ip-0A0C0424:47150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.922030] [ip-0A0C0439:35646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.922653] [ip-0A0C042B:56116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.922981] [ip-0A0C042C:51510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.922943] [ip-0A0C0434:47016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.923034] [ip-0A0C0434:47015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.923241] [ip-0A0C047D:24966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.923678] [ip-0A0C045A:42398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.923999] [ip-0A0C0480:23605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.924525] [ip-0A0C0426:56787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.924640] [ip-0A0C0426:56792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.924894] [ip-0A0C0439:35640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.924318] [ip-0A0C042F:53433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.924558] [ip-0A0C043C:44452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.925245] [ip-0A0C045D:29867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.925378] [ip-0A0C0447:45391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.925727] [ip-0A0C045E:39750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.925448] [ip-0A0C047D:24967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.926293] [ip-0A0C047D:24962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.926975] [ip-0A0C0430:86179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.926806] [ip-0A0C0462:69797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.927353] [ip-0A0C0480:23610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.927374] [ip-0A0C0434:47017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.927420] [ip-0A0C0480:23608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.931173] [ip-0A0C042C:51509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.931285] [ip-0A0C041F:62033:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.932448] [ip-0A0C0422:82753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.933572] [ip-0A0C045E:39754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.935338] [ip-0A0C0467:28515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.937013] [ip-0A0C042C:51505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.938076] [ip-0A0C042C:51507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.938746] [ip-0A0C0425:61428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.938815] [ip-0A0C0462:69793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.940769] [ip-0A0C042B:56114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.941148] [ip-0A0C045D:29874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.941735] [ip-0A0C0479:24972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.941769] [ip-0A0C0462:69795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.943629] [ip-0A0C045A:42401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.946344] [ip-0A0C043C:44453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.946628] [ip-0A0C0462:69798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947056] [ip-0A0C0424:47148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947029] [ip-0A0C0425:61429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947707] [ip-0A0C045A:42404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947894] [ip-0A0C045A:42397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947835] [ip-0A0C045C:39178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.947970] [ip-0A0C045C:39174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.948777] [ip-0A0C041F:62036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.952456] [ip-0A0C0479:24975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.954247] [ip-0A0C043C:44451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.954418] [ip-0A0C043C:44450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.955914] [ip-0A0C0479:24976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.955730] [ip-0A0C045D:29872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.956605] [ip-0A0C0480:23606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.956563] [ip-0A0C0467:28516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.958041] [ip-0A0C0439:35643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.959083] [ip-0A0C045D:29869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.963562] [ip-0A0C041F:62037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.964955] [ip-0A0C042B:56121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.965541] [ip-0A0C041F:62038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.966762] [ip-0A0C042B:56134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.966539] [ip-0A0C041F:62034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.968191] [ip-0A0C0467:28519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.971515] [ip-0A0C0467:28517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.972316] [ip-0A0C0422:82754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.972978] [ip-0A0C0439:35645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.973062] [ip-0A0C0439:35639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.973592] [ip-0A0C0446:46251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.973570] [ip-0A0C0422:82752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.975665] [ip-0A0C0446:46258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.978721] [ip-0A0C0424:47146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.978692] [ip-0A0C042B:56118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.978443] [ip-0A0C045C:39172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.979575] [ip-0A0C0471:27797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.980161] [ip-0A0C0424:47151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.980201] [ip-0A0C0424:47147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.982688] [ip-0A0C0446:46252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.984796] [ip-0A0C0430:86184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.985026] [ip-0A0C0430:86181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.985592] [ip-0A0C0430:86180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.986073] [ip-0A0C042B:56120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.986761] [ip-0A0C0425:61425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.986942] [ip-0A0C0425:61423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.987031] [ip-0A0C0425:61427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.986983] [ip-0A0C0422:82764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.988379] [ip-0A0C0430:86178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.989733] [ip-0A0C0413:90221:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.990043] [ip-0A0C0413:90223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.990147] [ip-0A0C0413:90218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622499.997479] [ip-0A0C0471:27792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.002075] [ip-0A0C0422:82755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.007021] [ip-0A0C045C:39176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.008579] [ip-0A0C0471:27793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.009202] [ip-0A0C0471:27796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.011121] [ip-0A0C0471:27795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622500.011634] [ip-0A0C045C:39179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634622500917, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634622500957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634622500957, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634622500958, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634622500958, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634622500958, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634622500958, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:48:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:21262 - context.c:584] INFO job (ID: 867538095130806164) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21262 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21262 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21264 - context.c:584] INFO job (ID: 867538099259216922) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21264 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21264 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21261 - context.c:584] INFO job (ID: 867538405205404888) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21261 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21261 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21285 - context.c:584] INFO job (ID: 867538824244822009) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21285 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21285 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21260 - context.c:584] INFO job (ID: 867538376516920972) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21260 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21260 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21259 - context.c:584] INFO job (ID: 867538714577752646) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21259 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21259 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21263 - context.c:584] INFO job (ID: 867538653033808214) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21263 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21263 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:21266 - context.c:584] INFO job (ID: 867538848296362691) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:21266 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:21266 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592655, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2614979502, "metadata": {"file": "main.py", "lineno": 72}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592656, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622592657, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:49:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634622616610, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634622616629, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622616634, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634622616635, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634622619230, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634622619230, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634622619230, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634622619231, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634622620714, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2265.6382810946357, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622620714, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622620715, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2265.6382810946357, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634622620715, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622620715, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622621403, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4882.379484450579, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622621404, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622621404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4882.379484450579, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622621404, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622621404, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622622056, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5152.632259385466, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622622057, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622622057, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5152.632259385466, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634622622057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622622057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622622702, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5209.298628484235, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622622703, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622622703, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5209.298628484235, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634622622703, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622622703, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622623337, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5304.120091909378, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622623337, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622623337, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5304.120091909378, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634622623338, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622623338, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622623968, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5334.969755084133, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622623968, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622623968, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5334.969755084133, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634622623969, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622623969, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622624595, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.728840895228, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622624595, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622624595, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.728840895228, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634622624596, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622624596, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622625223, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5362.994721433962, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622625223, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622625223, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5362.994721433962, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634622625223, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622625223, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622625846, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.027090372358, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622625846, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622625846, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.027090372358, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634622625846, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622625846, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622626472, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.225923003922, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622626472, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622626472, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.225923003922, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634622626473, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622626473, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622627098, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.971185550387, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622627099, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622627099, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.971185550387, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634622627099, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622627099, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622627738, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5255.908317380471, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622627739, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622627739, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5255.908317380471, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634622627739, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622627739, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622628368, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5342.62287338356, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622628368, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622628368, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5342.62287338356, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634622628368, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622628368, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622628998, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5339.916291357003, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622628998, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622628998, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5339.916291357003, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634622628998, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622628998, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622629631, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.575638409061, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622629631, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622629631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.575638409061, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634622629631, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622629631, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622630264, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5314.108427279182, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622630264, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622630264, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5314.108427279182, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634622630264, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622630264, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622630896, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.711977839226, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622630897, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622630897, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.711977839226, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634622630897, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622630897, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622631523, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.563722435351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622631523, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622631523, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.563722435351, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634622631523, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622631523, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622632152, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5346.331920453325, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622632152, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622632152, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5346.331920453325, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634622632152, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622632153, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622632776, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.736745022644, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622632777, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622632777, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.736745022644, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634622632777, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622632777, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622633399, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5400.439013679228, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622633400, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622633400, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5400.439013679228, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634622633400, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622633400, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622634026, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5367.678525723061, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622634026, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622634026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5367.678525723061, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634622634026, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622634026, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622634648, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.273478536523, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622634648, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622634649, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.273478536523, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634622634649, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622634649, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622635272, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.237303612696, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622635272, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622635272, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.237303612696, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634622635273, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622635273, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622635894, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5408.764887394648, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622635894, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622635894, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5408.764887394648, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634622635894, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622635895, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622636522, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5354.811090301629, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622636523, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622636523, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5354.811090301629, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634622636523, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622636523, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622637151, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5353.893621612422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622637151, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622637151, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5353.893621612422, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634622637151, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622637151, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622637774, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5395.162888203454, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622637774, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622637774, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5395.162888203454, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634622637775, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622637775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622638401, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5368.551642485154, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622638401, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622638401, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5368.551642485154, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634622638401, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622638401, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622639026, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5380.0985323979185, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622639026, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622639026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5380.0985323979185, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634622639026, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622639026, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622639647, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5414.186221715277, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622639647, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622639648, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5414.186221715277, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634622639648, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622639648, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622640265, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.532269012684, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622640265, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622640265, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.532269012684, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634622640265, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622640266, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622640889, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5393.822758764809, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622640889, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622640889, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5393.822758764809, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634622640889, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622640889, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622641514, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.355776469143, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622641515, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622641515, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.355776469143, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634622641515, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622641515, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622642133, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5440.311914797848, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622642133, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622642133, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5440.311914797848, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634622642133, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622642133, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622642757, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5390.849595997887, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622642757, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622642757, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5390.849595997887, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634622642757, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622642757, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622643376, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.3723854065765, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622643376, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622643376, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.3723854065765, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634622643376, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622643377, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622644000, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.98401758387, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622644000, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622644001, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.98401758387, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634622644001, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622644001, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622644620, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.216681169576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622644620, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622644621, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.216681169576, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634622644621, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622644621, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622645252, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5325.444196973464, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622645252, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622645252, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5325.444196973464, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634622645252, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622645252, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622645870, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.946670447863, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622645870, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622645870, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.946670447863, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634622645870, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622645870, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622646488, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5439.858322200422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622646488, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622646488, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5439.858322200422, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634622646488, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622646489, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622647108, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.915550910607, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622647109, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622647109, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.915550910607, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634622647109, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622647109, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622647732, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5392.103660414652, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622647732, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622647733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5392.103660414652, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634622647733, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622647733, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622648349, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.368538671564, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622648349, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622648349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.368538671564, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634622648349, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622648349, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622648970, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.62259538372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622648970, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622648971, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.62259538372, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634622648971, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622648971, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622649590, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5425.786325174618, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622649590, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622649590, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5425.786325174618, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634622649591, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622649591, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622650208, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5446.139741443973, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622650208, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622650208, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5446.139741443973, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634622650208, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622650208, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622650834, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5368.028147286458, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622650835, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622650835, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5368.028147286458, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634622650835, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622650835, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622651454, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5424.276441057707, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622651455, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622651455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5424.276441057707, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634622651526, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622651526, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622651542, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634622651971, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8806735873222351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634622651971, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634622652136, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5514.881851309216, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622652136, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622652136, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5514.881851309216, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634622652212, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622652212, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622652227, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634622652675, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8974505066871643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634622652675, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634622652899, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4891.6018426730425, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622652900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622652900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4891.6018426730425, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634622652968, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622652968, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622652982, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634622653380, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8934006094932556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634622653380, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634622653578, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5509.65124616377, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622653578, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622653578, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5509.65124616377, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634622653685, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622653685, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622653701, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634622654107, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8926833868026733, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634622654107, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634622654319, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5301.663774981228, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622654320, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622654320, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5301.663774981228, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634622654355, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622654355, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622654371, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634622654799, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8731456995010376, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634622654799, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634622655002, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5190.0201850424455, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622655003, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622655003, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5190.0201850424455, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634622655054, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622655054, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622655070, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634622655478, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910803198814392, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634622655478, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634622655673, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.155727901557, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622655673, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622655673, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.155727901557, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634622655707, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622655707, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622655723, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634622656150, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8977696895599365, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634622656150, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634622656354, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5195.628388546721, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622656354, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622656354, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5195.628388546721, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634622656390, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622656390, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622656408, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634622656832, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8862940073013306, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634622656832, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634622657024, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5306.442814637537, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622657024, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622657024, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5306.442814637537, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634622657059, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622657059, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622657075, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634622657493, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8853870630264282, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634622657493, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634622657690, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5322.646430868555, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622657690, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622657691, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5322.646430868555, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634622657730, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622657730, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622657744, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634622658163, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8948084115982056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634622658163, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634622658360, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.325394233858, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622658360, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622658360, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.325394233858, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634622658399, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622658399, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622658415, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634622658842, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8912466764450073, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634622658843, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634622659040, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5241.438994264598, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622659040, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622659040, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5241.438994264598, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634622659076, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622659076, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622659093, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634622659516, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8919678330421448, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634622659516, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634622659719, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5228.563694645613, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622659719, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622659719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5228.563694645613, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634622659768, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622659768, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622659784, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634622660185, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8863796591758728, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634622660186, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634622660378, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5509.7567949969525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622660379, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622660379, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5509.7567949969525, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634622660414, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622660414, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622660430, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634622660851, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8916575908660889, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634622660851, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634622661052, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5267.211460081784, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622661052, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622661052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5267.211460081784, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634622661086, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622661087, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622661102, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634622661519, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8903975486755371, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634622661519, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634622661716, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5337.212450260198, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622661716, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622661717, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5337.212450260198, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634622661757, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622661757, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622661772, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634622662193, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8475264310836792, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634622662193, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634622662389, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5318.1151053237145, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622662389, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622662389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5318.1151053237145, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634622662426, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622662426, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622662441, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634622662867, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8797754049301147, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634622662867, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634622663062, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5284.929399204159, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622663062, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622663062, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5284.929399204159, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634622663100, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622663100, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622663116, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634622663546, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8911433815956116, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634622663546, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634622663740, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5255.367362852134, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622663740, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622663740, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5255.367362852134, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634622663779, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622663779, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622663796, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634622664220, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8862000703811646, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634622664220, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634622664421, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5234.436355640603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622664421, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622664421, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5234.436355640603, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634622664456, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622664456, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622664471, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634622664901, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8852795362472534, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634622664901, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634622665096, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5250.648537661563, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622665097, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622665097, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5250.648537661563, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634622665143, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622665143, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622665158, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634622665565, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8785871863365173, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634622665565, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634622665760, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.944666933292, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622665760, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622665760, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.944666933292, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634622665797, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622665798, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622665815, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634622666234, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8944754004478455, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634622666234, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634622666434, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5283.427554697116, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622666434, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622666434, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5283.427554697116, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634622666480, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622666481, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622666496, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634622666921, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9016004800796509, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634622666921, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634622667114, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5304.804913909468, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622667114, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622667114, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5304.804913909468, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634622667166, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622667166, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622667182, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634622667592, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8899221420288086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634622667593, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634622667783, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5447.120681598307, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622667783, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622667784, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5447.120681598307, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634622667813, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622667814, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622667828, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634622668297, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8968285322189331, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634622668297, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634622668497, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4913.849514014703, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622668498, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622668498, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4913.849514014703, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634622668533, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622668533, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622668550, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634622668976, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8894366025924683, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634622668976, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634622669175, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5240.817208107821, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622669175, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622669175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5240.817208107821, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634622669210, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622669210, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622669226, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634622669651, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8914319276809692, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634622669651, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634622669845, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5296.037399192791, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622669845, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622669845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5296.037399192791, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634622669881, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622669881, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622669897, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634622670326, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8848027586936951, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634622670326, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634622670554, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4993.420737657346, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622670555, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622670555, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4993.420737657346, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634622670589, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622670590, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622670605, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634622671028, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8928119540214539, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634622671028, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634622671240, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5170.191493038167, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622671240, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622671240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5170.191493038167, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634622671276, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622671276, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622671292, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634622671719, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8948971033096313, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634622671719, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634622671938, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5077.4419067994, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622671939, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622671939, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5077.4419067994, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634622671975, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622671975, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622671990, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634622672446, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9027394652366638, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634622672446, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634622672659, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4914.375566877906, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622672659, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622672660, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4914.375566877906, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634622672697, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622672697, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622672713, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634622673141, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8885078430175781, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634622673141, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634622673340, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5230.2984858595355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622673340, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622673340, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5230.2984858595355, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634622673376, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622673376, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622673391, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634622673816, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8938982486724854, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634622673816, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634622674007, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.813697061569, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622674007, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622674007, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.813697061569, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634622674043, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622674043, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622674057, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634622674485, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8862023949623108, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634622674485, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634622674680, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5277.997928928155, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622674680, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622674680, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5277.997928928155, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634622674717, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622674717, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622674732, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634622675171, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8478363752365112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634622675171, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634622675368, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5161.749510758908, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622675368, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622675368, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5161.749510758908, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634622675405, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622675405, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622675421, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634622675864, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8784579038619995, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634622675864, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634622676064, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5102.507882117983, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622676064, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622676064, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5102.507882117983, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634622676111, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622676111, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622676126, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634622676541, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8536802530288696, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634622676541, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634622676727, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.88827692596, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622676727, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622676727, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.88827692596, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634622676775, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622676775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622676790, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634622677200, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.866199791431427, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634622677200, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634622677388, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5480.792900761399, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622677389, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622677389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5480.792900761399, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634622677424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622677424, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622677439, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634622677880, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001233577728271, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634622677880, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634622678078, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5141.582410237588, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622678079, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622678079, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5141.582410237588, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634622678115, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622678115, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622678131, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634622678559, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8847905397415161, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634622678559, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634622678770, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5133.502778595345, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622678770, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622678770, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5133.502778595345, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634622678805, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622678805, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622678820, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634622679248, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8953386545181274, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634622679248, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634622679438, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5305.9932899778505, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622679439, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622679439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5305.9932899778505, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634622679475, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622679475, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622679490, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634622679912, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8610749244689941, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634622679913, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634622680110, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5291.841344583254, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622680110, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622680111, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5291.841344583254, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634622680154, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622680154, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622680170, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634622680600, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8969460725784302, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634622680600, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634622680795, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5242.35147423753, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622680796, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622680796, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5242.35147423753, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634622680831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622680831, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622680848, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634622681277, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9013359546661377, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634622681277, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634622681473, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5234.829113261078, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622681474, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622681474, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5234.829113261078, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634622681511, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622681512, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622681527, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634622681953, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9016171097755432, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634622681954, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634622682146, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5293.182960838774, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622682147, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622682147, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5293.182960838774, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634622682184, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622682185, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622682201, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634622682627, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8992581367492676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634622682627, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634622682826, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5240.593089399604, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622682826, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622682826, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5240.593089399604, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634622682876, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622682876, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622682892, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634622683305, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8933020830154419, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634622683305, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634622683500, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5384.084012861107, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622683500, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622683501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5384.084012861107, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634622683536, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622683536, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622683551, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634622683980, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9025789499282837, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634622683980, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634622684178, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5236.424081322358, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622684178, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622684178, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5236.424081322358, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634622684213, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622684213, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634622684228, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634622684657, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9096784591674805, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634622684657, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634622684657, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634622684851, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5263.711608665545, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634622684852, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634622684852, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5263.711608665545, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:31 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:32 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:33 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:33 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:34 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:35 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:36 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:37 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:38 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:39 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:40 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:41 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:42 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:43 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
ENDING TIMING RUN AT 2021-10-19 05:51:44 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:48:14 AM
