+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019051438190553138
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019051438190553138
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437/unet3d_96x8x1_211019051438190553138
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437/unet3d_96x8x1_211019051438190553138
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07379/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437/unet3d_96x8x1_211019051438190553138_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C044D
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C04BD
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C04CC
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C04C4
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04DA
Clearing cache on ip-0A0C04BE
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051437:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:14:42 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634620487.595889] [ip-0A0C040F:91946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.620883] [ip-0A0C040F:91921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.632706] [ip-0A0C040F:91924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.642785] [ip-0A0C04CC:32655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.643540] [ip-0A0C044D:22023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.646060] [ip-0A0C04CC:32658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.664990] [ip-0A0C0464:24402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.672572] [ip-0A0C049C:93905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.674707] [ip-0A0C049F:95089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.679648] [ip-0A0C044D:22015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.684934] [ip-0A0C042D:93119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.692449] [ip-0A0C04BE:42272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.711877] [ip-0A0C04BA:74559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.716724] [ip-0A0C044D:22009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.729845] [ip-0A0C0492:4166 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.730312] [ip-0A0C049F:95086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.731453] [ip-0A0C04A4:9871 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.732345] [ip-0A0C0492:4165 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.736582] [ip-0A0C04BE:42266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.736655] [ip-0A0C0437:21248:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.737588] [ip-0A0C044D:22013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.738759] [ip-0A0C04A4:9875 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.739201] [ip-0A0C04B0:93347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.739303] [ip-0A0C04A9:8477 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.746703] [ip-0A0C049B:95580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.746771] [ip-0A0C0497:56830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.749360] [ip-0A0C049C:93902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.751711] [ip-0A0C0435:51469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.753904] [ip-0A0C048A:21222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.754606] [ip-0A0C04C5:43951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.754629] [ip-0A0C04C5:43945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.756911] [ip-0A0C049C:93926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.756875] [ip-0A0C048A:21218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.758241] [ip-0A0C04B9:42398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.760957] [ip-0A0C04B0:93352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.762131] [ip-0A0C0492:4160 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.763576] [ip-0A0C04DB:25056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.764373] [ip-0A0C04B3:17091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.766954] [ip-0A0C04A5:11512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.766954] [ip-0A0C04A5:11507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.768924] [ip-0A0C049B:95579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.769198] [ip-0A0C0488:22522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.769883] [ip-0A0C040F:91927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.769971] [ip-0A0C0461:86051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.771971] [ip-0A0C049F:95088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.772859] [ip-0A0C0449:21941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.774587] [ip-0A0C0437:21250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.778967] [ip-0A0C040F:91925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.780989] [ip-0A0C0464:24399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.781672] [ip-0A0C0464:24406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.782878] [ip-0A0C04B1:13010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.782891] [ip-0A0C04B1:13018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.782966] [ip-0A0C04A9:8470 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.783151] [ip-0A0C04A9:8474 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.784162] [ip-0A0C04CC:32652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.784408] [ip-0A0C042D:93120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.784518] [ip-0A0C042D:93117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.785648] [ip-0A0C0499:11124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.786662] [ip-0A0C04C8:33321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.787043] [ip-0A0C04CC:32654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.787661] [ip-0A0C04CC:32653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.787748] [ip-0A0C0494:6462 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.788930] [ip-0A0C0473:23041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.789229] [ip-0A0C0482:12203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.789278] [ip-0A0C04AD:84117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.789729] [ip-0A0C0473:23046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.790385] [ip-0A0C040F:91919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.791312] [ip-0A0C048D:10424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.792807] [ip-0A0C04C0:41100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.793360] [ip-0A0C0487:24288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.794332] [ip-0A0C040F:91920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.794970] [ip-0A0C0499:11120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.795282] [ip-0A0C046B:8486 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.796392] [ip-0A0C049D:2168 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.799992] [ip-0A0C04DB:25057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.802141] [ip-0A0C04BE:42271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.802355] [ip-0A0C04B3:17090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.803294] [ip-0A0C0443:85703:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.803212] [ip-0A0C04B1:13013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.803149] [ip-0A0C0495:9039 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.803295] [ip-0A0C0443:85708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.804282] [ip-0A0C048B:7177 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.804892] [ip-0A0C046E:26355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.805603] [ip-0A0C0482:12205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.807650] [ip-0A0C0484:26382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.808382] [ip-0A0C040F:91923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.811939] [ip-0A0C0496:88737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.812030] [ip-0A0C04BD:39944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.812330] [ip-0A0C046E:26351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.813508] [ip-0A0C0466:27043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.813096] [ip-0A0C04BB:81828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.814048] [ip-0A0C0435:51471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.814183] [ip-0A0C0497:56826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.815835] [ip-0A0C049C:93908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.815724] [ip-0A0C04CC:32656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.815911] [ip-0A0C0464:24400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.816761] [ip-0A0C0461:86048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.818473] [ip-0A0C0464:24414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.818763] [ip-0A0C044D:22012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.819866] [ip-0A0C042D:93118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.820762] [ip-0A0C0461:86050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.820893] [ip-0A0C0483:23411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.822044] [ip-0A0C04A7:83681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.823126] [ip-0A0C0495:9041 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.823444] [ip-0A0C0488:22524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.823701] [ip-0A0C0437:21246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.824147] [ip-0A0C0472:26411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.826351] [ip-0A0C0491:6806 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.826094] [ip-0A0C04A4:9874 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.826761] [ip-0A0C0466:27047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.826771] [ip-0A0C04BA:74561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.827348] [ip-0A0C04C8:33323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.827283] [ip-0A0C045F:22533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.827396] [ip-0A0C049F:95090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.828223] [ip-0A0C046B:8488 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.829266] [ip-0A0C0460:24058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.830031] [ip-0A0C049D:2175 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.832263] [ip-0A0C0487:24287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.832354] [ip-0A0C0487:24286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.832722] [ip-0A0C04C8:33327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.832452] [ip-0A0C044D:22010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.832862] [ip-0A0C046B:8482 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.834170] [ip-0A0C048C:5767 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.835716] [ip-0A0C04C2:64216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.836017] [ip-0A0C04CC:32657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.836397] [ip-0A0C047E:21515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.836503] [ip-0A0C04BA:74560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.837114] [ip-0A0C0488:22519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.837817] [ip-0A0C04CC:32659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.838688] [ip-0A0C044D:22014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.838808] [ip-0A0C044D:22011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.839621] [ip-0A0C0498:19266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.840517] [ip-0A0C0488:22518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.840282] [ip-0A0C0489:24674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.844621] [ip-0A0C04AA:81912:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.844443] [ip-0A0C04DB:25055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.845858] [ip-0A0C0483:23415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.846937] [ip-0A0C049C:93903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.847163] [ip-0A0C04BE:42264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.847616] [ip-0A0C0484:26384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.847684] [ip-0A0C04AE:14920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.848445] [ip-0A0C048A:21215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.848715] [ip-0A0C04BD:39947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.849430] [ip-0A0C0496:88734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.850369] [ip-0A0C0496:88739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.851574] [ip-0A0C04C0:41096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.851846] [ip-0A0C046C:20246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.853077] [ip-0A0C04A6:9071 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.853920] [ip-0A0C048D:10429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.854299] [ip-0A0C04B0:93351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.855399] [ip-0A0C048C:5769 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.856858] [ip-0A0C0460:24064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.857306] [ip-0A0C042D:93121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.856837] [ip-0A0C04A6:9068 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.856773] [ip-0A0C0449:21943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.857468] [ip-0A0C0449:21942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.857995] [ip-0A0C0495:9040 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.858435] [ip-0A0C049C:93906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.858274] [ip-0A0C042D:93124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.858569] [ip-0A0C04A4:9877 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.862270] [ip-0A0C04BE:42268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.863765] [ip-0A0C04C2:64223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.864375] [ip-0A0C042D:93122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.864645] [ip-0A0C04B7:77592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.865350] [ip-0A0C04B1:13012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.866161] [ip-0A0C0464:24401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.866497] [ip-0A0C042D:93123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.866446] [ip-0A0C04C9:32185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.866560] [ip-0A0C04B9:42402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.867345] [ip-0A0C0435:51472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.866819] [ip-0A0C04C6:37464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.867321] [ip-0A0C0461:86047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.868246] [ip-0A0C04C0:41101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.868603] [ip-0A0C0498:19270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.869526] [ip-0A0C049D:2172 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.869570] [ip-0A0C04AE:14924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.869702] [ip-0A0C04AD:84113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.873124] [ip-0A0C0473:23043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.873050] [ip-0A0C0472:26414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.873673] [ip-0A0C0438:25302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.873811] [ip-0A0C0492:4163 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.876250] [ip-0A0C04AF:980  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.876708] [ip-0A0C0489:24668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.877375] [ip-0A0C0464:24405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.877274] [ip-0A0C04C6:37461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.878341] [ip-0A0C04A5:11508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.878963] [ip-0A0C04BB:81826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.879421] [ip-0A0C0464:24403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.879981] [ip-0A0C049C:93904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.881049] [ip-0A0C046E:26352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.880807] [ip-0A0C04BB:81824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.881932] [ip-0A0C04B0:93350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.882178] [ip-0A0C04CF:65099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.883335] [ip-0A0C04BA:74553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.883936] [ip-0A0C049C:93907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.883581] [ip-0A0C045F:22534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.884461] [ip-0A0C04B3:17088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.884408] [ip-0A0C0437:21244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.884705] [ip-0A0C04D8:32569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.884721] [ip-0A0C049F:95092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.885530] [ip-0A0C04BC:37837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.886061] [ip-0A0C04BA:74554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.886059] [ip-0A0C04AA:81917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.886169] [ip-0A0C04C3:40552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.886314] [ip-0A0C0460:24062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.886823] [ip-0A0C04C3:40551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.887554] [ip-0A0C04B2:82877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.887311] [ip-0A0C0449:21945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.888170] [ip-0A0C04B2:82878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.888066] [ip-0A0C04B9:42399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.889705] [ip-0A0C048D:10423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.889904] [ip-0A0C04AD:84120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.890462] [ip-0A0C0493:6278 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.891287] [ip-0A0C0494:6461 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.891989] [ip-0A0C04BE:42269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.892274] [ip-0A0C04BC:37844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.892089] [ip-0A0C04C5:43952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.892290] [ip-0A0C049B:95581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.892925] [ip-0A0C049F:95087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.892795] [ip-0A0C04C5:43947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.894165] [ip-0A0C049F:95091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.894476] [ip-0A0C04BE:42270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.894255] [ip-0A0C0486:24366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.896483] [ip-0A0C04B7:77594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.896897] [ip-0A0C04A5:11514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.896777] [ip-0A0C0494:6464 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.897556] [ip-0A0C047E:21518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.897350] [ip-0A0C049F:95093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.897549] [ip-0A0C048B:7182 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.897873] [ip-0A0C04BE:42265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.898049] [ip-0A0C04BD:39973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.898724] [ip-0A0C0484:26389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.898833] [ip-0A0C0492:4167 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.899344] [ip-0A0C0435:51474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.899185] [ip-0A0C048A:21216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.901392] [ip-0A0C04BA:74557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.901682] [ip-0A0C0492:4164 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.901898] [ip-0A0C0437:21245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.902950] [ip-0A0C04BA:74552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.903071] [ip-0A0C04D8:32563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.903176] [ip-0A0C0497:56828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.904319] [ip-0A0C0441:17857:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.904413] [ip-0A0C04A9:8476 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.905803] [ip-0A0C04A8:14985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.907735] [ip-0A0C0492:4162 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908206] [ip-0A0C04B3:17092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908024] [ip-0A0C0436:24727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908018] [ip-0A0C0436:24732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908021] [ip-0A0C0436:24729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908353] [ip-0A0C04AC:2566 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.908759] [ip-0A0C0492:4161 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.909357] [ip-0A0C04BA:74555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.909175] [ip-0A0C04B4:18837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.909301] [ip-0A0C04C5:43950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.910925] [ip-0A0C0443:85706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.910784] [ip-0A0C0497:56824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.911689] [ip-0A0C046E:26354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.911096] [ip-0A0C04DB:25052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.912797] [ip-0A0C04CF:65101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.913486] [ip-0A0C048B:7180 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.913778] [ip-0A0C0485:86811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.913955] [ip-0A0C048D:10431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.916326] [ip-0A0C0438:25303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.916629] [ip-0A0C04B0:93348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.917745] [ip-0A0C04B0:93346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.917509] [ip-0A0C04A9:8473 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.918407] [ip-0A0C0461:86049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.919270] [ip-0A0C04AA:81913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.919030] [ip-0A0C048F:14285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.919480] [ip-0A0C04B4:18843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.919728] [ip-0A0C046B:8487 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.921102] [ip-0A0C0491:6810 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.920324] [ip-0A0C04C0:41098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.920826] [ip-0A0C048A:21217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.921933] [ip-0A0C04A4:9878 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.923021] [ip-0A0C04B9:42400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.923332] [ip-0A0C04B1:13011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.924170] [ip-0A0C04A2:97801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.924490] [ip-0A0C0466:27046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.924130] [ip-0A0C04C5:43946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.924551] [ip-0A0C0429:14931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.925091] [ip-0A0C04B7:77587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.925154] [ip-0A0C0487:24289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.925207] [ip-0A0C048A:21221:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.925772] [ip-0A0C04A0:96250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.925799] [ip-0A0C04A0:96253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.926778] [ip-0A0C04D3:30534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.927024] [ip-0A0C049B:95583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.927841] [ip-0A0C0435:51468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.928290] [ip-0A0C04A7:83675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.928892] [ip-0A0C0443:85707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.928914] [ip-0A0C04C2:64218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.929128] [ip-0A0C04B9:42397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.929208] [ip-0A0C049B:95582:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.929834] [ip-0A0C04A4:9873 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.930173] [ip-0A0C04A5:11513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.930376] [ip-0A0C04A4:9872 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.930433] [ip-0A0C04AE:14922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.931146] [ip-0A0C04B0:93353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.930607] [ip-0A0C04B6:76301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.932111] [ip-0A0C048F:14284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.931968] [ip-0A0C04C5:43949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.932223] [ip-0A0C04A4:9876 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.932707] [ip-0A0C04AB:97586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.933257] [ip-0A0C04B1:13016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.934018] [ip-0A0C04D4:29363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.934352] [ip-0A0C04D4:29361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.934667] [ip-0A0C0494:6459 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.935339] [ip-0A0C04A7:83674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.935514] [ip-0A0C0473:23044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.935943] [ip-0A0C04B0:93349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.935399] [ip-0A0C04DB:25058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.936168] [ip-0A0C0484:26387:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.936053] [ip-0A0C04A9:8478 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.939422] [ip-0A0C047E:21522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.939832] [ip-0A0C04C5:43948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.940575] [ip-0A0C04A9:8471 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941210] [ip-0A0C04CD:29465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941766] [ip-0A0C0443:85702:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941487] [ip-0A0C0499:11127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941809] [ip-0A0C04AA:81914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941976] [ip-0A0C046B:8489 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.942297] [ip-0A0C04C9:32183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.941839] [ip-0A0C0449:21939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.942726] [ip-0A0C046C:20252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.942815] [ip-0A0C0435:51467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.942862] [ip-0A0C04B3:17094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.943003] [ip-0A0C048B:7178 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.944085] [ip-0A0C0491:6805 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.943206] [ip-0A0C04A9:8472 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.943500] [ip-0A0C0489:24673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.943681] [ip-0A0C048A:21220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945046] [ip-0A0C04B9:42401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945127] [ip-0A0C0499:11123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945250] [ip-0A0C048A:21219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945061] [ip-0A0C0497:56825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945668] [ip-0A0C04C8:33324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945704] [ip-0A0C04DA:26358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.945695] [ip-0A0C04AD:84119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.946248] [ip-0A0C04C2:64220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.946845] [ip-0A0C0437:21249:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.947061] [ip-0A0C04B1:13017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.947372] [ip-0A0C04B1:13019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.947407] [ip-0A0C0437:21243:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.948696] [ip-0A0C049D:2174 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.948884] [ip-0A0C0487:24290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.948764] [ip-0A0C049D:2176 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949024] [ip-0A0C049B:95577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949143] [ip-0A0C049B:95578:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949440] [ip-0A0C0488:22521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949569] [ip-0A0C0499:11125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949564] [ip-0A0C0488:22523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.949677] [ip-0A0C0473:23045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.950781] [ip-0A0C04CD:29464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.951466] [ip-0A0C04D9:62926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.951248] [ip-0A0C04AF:982  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.951338] [ip-0A0C04AF:985  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.951537] [ip-0A0C04A8:14982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.952485] [ip-0A0C04A5:11510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.952615] [ip-0A0C04B6:76302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.953208] [ip-0A0C0449:21944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954047] [ip-0A0C04B9:42403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954208] [ip-0A0C04B9:42404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954385] [ip-0A0C04C3:40553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954373] [ip-0A0C04B5:10031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954887] [ip-0A0C0482:12206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.954853] [ip-0A0C04BB:81831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955127] [ip-0A0C0483:23410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955047] [ip-0A0C0482:12204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955557] [ip-0A0C04A1:27420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955567] [ip-0A0C0498:19268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955938] [ip-0A0C04A5:11509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.955950] [ip-0A0C0495:9059 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.956529] [ip-0A0C0487:24292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.957179] [ip-0A0C04A5:11511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.958284] [ip-0A0C0437:21247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.958493] [ip-0A0C0488:22525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.958554] [ip-0A0C0497:56829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.959327] [ip-0A0C0435:51473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.959194] [ip-0A0C04D3:30538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.959238] [ip-0A0C0497:56823:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.959583] [ip-0A0C0461:86073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.961595] [ip-0A0C0493:6282 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.961928] [ip-0A0C04B4:18842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.962732] [ip-0A0C045F:22529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.962839] [ip-0A0C0461:86052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.962972] [ip-0A0C045F:22532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.963184] [ip-0A0C04C0:41099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.963590] [ip-0A0C048D:10428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.963463] [ip-0A0C04DB:25059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.964284] [ip-0A0C0496:88741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.963703] [ip-0A0C04DB:25054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.964262] [ip-0A0C0482:12211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.964571] [ip-0A0C0449:21940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.965610] [ip-0A0C0461:86054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.965335] [ip-0A0C0494:6458 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.966041] [ip-0A0C046E:26358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.965701] [ip-0A0C0497:56827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.966038] [ip-0A0C048F:14308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.966407] [ip-0A0C0486:24362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.967438] [ip-0A0C0435:51470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.967537] [ip-0A0C046C:20244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.968173] [ip-0A0C0484:26385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.967985] [ip-0A0C0488:22520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.968523] [ip-0A0C0443:85704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.968482] [ip-0A0C0473:23039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.968582] [ip-0A0C0472:26409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.969053] [ip-0A0C04C8:33328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.968995] [ip-0A0C0499:11119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.969512] [ip-0A0C0443:85710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.969861] [ip-0A0C0499:11121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.970810] [ip-0A0C048B:7185 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.970969] [ip-0A0C049B:95584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.971833] [ip-0A0C048D:10450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.971585] [ip-0A0C0449:21946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.972344] [ip-0A0C0487:24291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.972579] [ip-0A0C0496:88735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.972673] [ip-0A0C04C9:32186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.973158] [ip-0A0C0487:24293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.972548] [ip-0A0C04DB:25053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.973364] [ip-0A0C046E:26353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.973449] [ip-0A0C04CF:65096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.973935] [ip-0A0C04C8:33329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.974919] [ip-0A0C0443:85705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.974355] [ip-0A0C04AD:84114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.975071] [ip-0A0C04D9:62931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.975697] [ip-0A0C0460:24061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.976053] [ip-0A0C0482:12207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.977477] [ip-0A0C0491:6831 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.976935] [ip-0A0C0473:23040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.977201] [ip-0A0C04B3:17089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.977443] [ip-0A0C0473:23042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.977465] [ip-0A0C048C:5770 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.978430] [ip-0A0C046C:20247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.978464] [ip-0A0C04B3:17095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.979744] [ip-0A0C04B3:17093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.979903] [ip-0A0C04D9:62927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.980897] [ip-0A0C04C9:32190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981008] [ip-0A0C04A2:97799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981486] [ip-0A0C0490:10073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981627] [ip-0A0C04AC:2565 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981510] [ip-0A0C0490:10071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981677] [ip-0A0C04BB:81829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.981910] [ip-0A0C04C0:41102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.982571] [ip-0A0C0436:24731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.982318] [ip-0A0C04CF:65097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.982988] [ip-0A0C0494:6463 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.983345] [ip-0A0C04CD:29468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.983475] [ip-0A0C0482:12202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.983693] [ip-0A0C0483:23413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.984741] [ip-0A0C046E:26356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.984819] [ip-0A0C0441:17854:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.984993] [ip-0A0C0441:17855:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.985070] [ip-0A0C0441:17852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.985139] [ip-0A0C0499:11122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.985239] [ip-0A0C048B:7184 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.985242] [ip-0A0C0494:6460 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.986214] [ip-0A0C0496:88738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.987692] [ip-0A0C04A7:83678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.987966] [ip-0A0C04A2:97807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.988224] [ip-0A0C04D8:32565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.988106] [ip-0A0C0494:6465 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.988737] [ip-0A0C0466:27044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.988631] [ip-0A0C046E:26350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.989616] [ip-0A0C048B:7183 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.990731] [ip-0A0C04AC:2562 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.990577] [ip-0A0C04C0:41103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.990884] [ip-0A0C0482:12201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.990738] [ip-0A0C04AD:84118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.991153] [ip-0A0C04AD:84115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.992223] [ip-0A0C0466:27041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.992080] [ip-0A0C0438:25299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.992072] [ip-0A0C048D:10425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.992261] [ip-0A0C048D:10426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.992678] [ip-0A0C048B:7181 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993069] [ip-0A0C046B:8483 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993338] [ip-0A0C046B:8484 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993065] [ip-0A0C04A6:9066 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993488] [ip-0A0C04C0:41097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993839] [ip-0A0C04B7:77588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.993867] [ip-0A0C04C8:33322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.994058] [ip-0A0C045F:22531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.994732] [ip-0A0C04AD:84116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.995125] [ip-0A0C0460:24060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.995579] [ip-0A0C049D:2171 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.995895] [ip-0A0C049D:2177 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.996113] [ip-0A0C049D:2169 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.996250] [ip-0A0C046B:8485 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.996315] [ip-0A0C0472:26415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.996472] [ip-0A0C0472:26412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.996844] [ip-0A0C04BD:39949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.997315] [ip-0A0C04C8:33320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.997350] [ip-0A0C045F:22536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.998101] [ip-0A0C0495:9035 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.998138] [ip-0A0C0495:9036 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.998169] [ip-0A0C048C:5772 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.998583] [ip-0A0C04BD:39943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620487.999937] [ip-0A0C04A7:83676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.000311] [ip-0A0C0466:27048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.001414] [ip-0A0C0498:19269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.004080] [ip-0A0C0491:6826 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.003323] [ip-0A0C04C6:37462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.003487] [ip-0A0C04D3:30531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.004195] [ip-0A0C04BD:39945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.004305] [ip-0A0C04C7:34169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.005510] [ip-0A0C0466:27042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.005559] [ip-0A0C0484:26388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.008794] [ip-0A0C0491:6823 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.008487] [ip-0A0C0495:9038 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.008572] [ip-0A0C0495:9037 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.009155] [ip-0A0C0466:27045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.009058] [ip-0A0C04AF:981  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.010211] [ip-0A0C047E:21520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.010456] [ip-0A0C0484:26383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.009975] [ip-0A0C04A8:14983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.010179] [ip-0A0C04BB:81830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.010713] [ip-0A0C0496:88736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.011023] [ip-0A0C0484:26386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.011685] [ip-0A0C04A6:9067 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.011920] [ip-0A0C0489:24667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.013924] [ip-0A0C0429:14929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.014848] [ip-0A0C04C7:34167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.015342] [ip-0A0C047E:21516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.015700] [ip-0A0C04BD:39946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.016284] [ip-0A0C0496:88740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.017198] [ip-0A0C04AE:14921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.017919] [ip-0A0C04A7:83679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018031] [ip-0A0C04A7:83677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018041] [ip-0A0C045F:22535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018239] [ip-0A0C0493:6280 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018466] [ip-0A0C0483:23412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018582] [ip-0A0C0483:23407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018656] [ip-0A0C0472:26413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.018711] [ip-0A0C0472:26410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.019372] [ip-0A0C047E:21517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.020110] [ip-0A0C0491:6807 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.019519] [ip-0A0C04BD:39948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.019315] [ip-0A0C0486:24360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.019896] [ip-0A0C045F:22530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.020434] [ip-0A0C04D8:32564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.021066] [ip-0A0C04C2:64229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.021248] [ip-0A0C0460:24057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.021340] [ip-0A0C04AE:14926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.021806] [ip-0A0C048C:5768 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.022206] [ip-0A0C04BB:81827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.022498] [ip-0A0C0472:26416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.022681] [ip-0A0C04B4:18839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.023058] [ip-0A0C0483:23408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.025116] [ip-0A0C04A7:83680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.024795] [ip-0A0C04BB:81825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.025706] [ip-0A0C0491:6808 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.027190] [ip-0A0C0460:24063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.027393] [ip-0A0C04AF:979  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.027974] [ip-0A0C0485:86814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.028180] [ip-0A0C0483:23409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.028279] [ip-0A0C04A1:27422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.028470] [ip-0A0C0489:24672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.029219] [ip-0A0C04AC:2569 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.029826] [ip-0A0C04D4:29362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.030383] [ip-0A0C0460:24059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.031887] [ip-0A0C048C:5771 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.032564] [ip-0A0C0436:24733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.032910] [ip-0A0C04A6:9065 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.033226] [ip-0A0C04A6:9070 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.033786] [ip-0A0C0489:24670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.034068] [ip-0A0C04DA:26353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.035881] [ip-0A0C0438:25296:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.036273] [ip-0A0C04AA:81919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.036412] [ip-0A0C04AA:81920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.037091] [ip-0A0C04A1:27430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.037047] [ip-0A0C04C3:40550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.037294] [ip-0A0C046C:20248:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.037429] [ip-0A0C04DA:26354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.038379] [ip-0A0C04C2:64217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.038669] [ip-0A0C04C2:64222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.038615] [ip-0A0C048C:5766 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.038901] [ip-0A0C04CF:65102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.038976] [ip-0A0C04A6:9092 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.040126] [ip-0A0C04AA:81915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.040439] [ip-0A0C048C:5765 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.040735] [ip-0A0C0498:19267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.041566] [ip-0A0C046C:20253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.042093] [ip-0A0C046C:20245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.041968] [ip-0A0C0498:19264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.042475] [ip-0A0C04AA:81916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.042684] [ip-0A0C04AB:97581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.042385] [ip-0A0C04A6:9069 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.042999] [ip-0A0C0436:24734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.043380] [ip-0A0C04C2:64219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.043415] [ip-0A0C04A8:14986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.044570] [ip-0A0C0438:25301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.044583] [ip-0A0C0498:19263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.044758] [ip-0A0C04BC:37843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.044905] [ip-0A0C0498:19265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.045307] [ip-0A0C04D4:29365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.045888] [ip-0A0C04BC:37842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.046214] [ip-0A0C04C9:32184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.046668] [ip-0A0C04B2:82875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.046957] [ip-0A0C0493:6276 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.047649] [ip-0A0C047E:21519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.047748] [ip-0A0C047E:21521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.047897] [ip-0A0C0485:86809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.048636] [ip-0A0C04DA:26351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.050265] [ip-0A0C04AE:14947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.050596] [ip-0A0C046C:20249:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.050613] [ip-0A0C04C6:37484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.050993] [ip-0A0C0486:24358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.052387] [ip-0A0C048F:14279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.054285] [ip-0A0C04B7:77593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.055328] [ip-0A0C04BC:37841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.058020] [ip-0A0C04B2:82880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.057991] [ip-0A0C04C6:37465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.058980] [ip-0A0C0489:24669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.059041] [ip-0A0C0489:24671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.059081] [ip-0A0C04B6:76297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.059874] [ip-0A0C04AB:97588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.060196] [ip-0A0C04AE:14925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.060673] [ip-0A0C0436:24726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.060621] [ip-0A0C0493:6279 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.060429] [ip-0A0C04CF:65095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.060996] [ip-0A0C04CF:65100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.061848] [ip-0A0C04CF:65098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.062230] [ip-0A0C04B5:10036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.066015] [ip-0A0C04AE:14923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.066156] [ip-0A0C0429:14934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.066557] [ip-0A0C0493:6275 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.067453] [ip-0A0C04B4:18838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.067762] [ip-0A0C0485:86810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.067999] [ip-0A0C04C9:32197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.068805] [ip-0A0C04A0:96257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.068843] [ip-0A0C04A0:96252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.069348] [ip-0A0C04C6:37463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.069735] [ip-0A0C0429:14930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.069627] [ip-0A0C04B6:76298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.070170] [ip-0A0C0438:25298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.071101] [ip-0A0C0438:25297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.071243] [ip-0A0C04C3:40556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.071549] [ip-0A0C04C9:32187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.071839] [ip-0A0C04B2:82876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.072623] [ip-0A0C04D3:30537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.073189] [ip-0A0C0436:24728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.073610] [ip-0A0C04C9:32188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.074883] [ip-0A0C0486:24364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.074964] [ip-0A0C0486:24359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.075334] [ip-0A0C04C6:37466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.075819] [ip-0A0C04CD:29466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.075993] [ip-0A0C04B7:77589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.076231] [ip-0A0C04B7:77591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.076422] [ip-0A0C04B7:77590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.076054] [ip-0A0C04C6:37469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.078520] [ip-0A0C04BC:37840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.079702] [ip-0A0C04B4:18836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.080788] [ip-0A0C04AF:983  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.080892] [ip-0A0C04AF:984  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.081402] [ip-0A0C04A2:97804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.082189] [ip-0A0C04B2:82881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.081919] [ip-0A0C04C7:34170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.082434] [ip-0A0C04AF:978  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.083678] [ip-0A0C04D8:32567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.084478] [ip-0A0C0490:10070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.085525] [ip-0A0C0438:25300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.087562] [ip-0A0C04A1:27419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.087751] [ip-0A0C04BC:37838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.087806] [ip-0A0C04BC:37839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.088309] [ip-0A0C04D8:32568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.088657] [ip-0A0C04B5:10030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.089440] [ip-0A0C04D9:62930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.089364] [ip-0A0C048F:14280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.090055] [ip-0A0C04B2:82879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.089841] [ip-0A0C04D8:32562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.090183] [ip-0A0C04B4:18841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.090404] [ip-0A0C04B4:18840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.091354] [ip-0A0C04AC:2563 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.091515] [ip-0A0C04A0:96255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.092039] [ip-0A0C04C3:40549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.093578] [ip-0A0C0486:24361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.093754] [ip-0A0C0486:24365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.094558] [ip-0A0C04C3:40554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.094707] [ip-0A0C04CD:29461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.096268] [ip-0A0C0485:86812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.097625] [ip-0A0C04B2:82909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.097596] [ip-0A0C04A0:96262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.098254] [ip-0A0C0441:17859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.098951] [ip-0A0C0441:17856:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.099464] [ip-0A0C0441:17858:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.099388] [ip-0A0C0493:6277 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.099373] [ip-0A0C04D3:30532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.101247] [ip-0A0C04AB:97582:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.102564] [ip-0A0C0493:6274 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.103737] [ip-0A0C04A8:14981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.104330] [ip-0A0C0485:86807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.104495] [ip-0A0C0429:14935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.106018] [ip-0A0C04D9:62928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.105950] [ip-0A0C04C3:40555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.106616] [ip-0A0C04AC:2570 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.106973] [ip-0A0C0490:10086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.107520] [ip-0A0C04DA:26352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.108127] [ip-0A0C0441:17853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.107963] [ip-0A0C048F:14283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.109785] [ip-0A0C04D8:32566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.109715] [ip-0A0C04A8:14987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.110032] [ip-0A0C04A8:14988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.111333] [ip-0A0C04AB:97584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.111723] [ip-0A0C04A0:96251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.114280] [ip-0A0C0429:14928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.114737] [ip-0A0C04D4:29360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.115587] [ip-0A0C04A2:97802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.115645] [ip-0A0C04A0:96254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.116903] [ip-0A0C04A8:14984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.117160] [ip-0A0C04B6:76296:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.117670] [ip-0A0C04D4:29366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.117927] [ip-0A0C048F:14281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.118209] [ip-0A0C04AC:2567 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.118682] [ip-0A0C0485:86813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.118710] [ip-0A0C0429:14932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.119873] [ip-0A0C04A1:27442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.119630] [ip-0A0C04D3:30535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.119695] [ip-0A0C04B6:76299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.121422] [ip-0A0C04AC:2564 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.122056] [ip-0A0C048F:14282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.122062] [ip-0A0C0429:14927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.122967] [ip-0A0C0485:86808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.126818] [ip-0A0C04D3:30533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.128308] [ip-0A0C04D4:29359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.128435] [ip-0A0C04D4:29364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.129746] [ip-0A0C04A2:97803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.130640] [ip-0A0C04B6:76300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.132203] [ip-0A0C04C7:34171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.132211] [ip-0A0C04D3:30536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.133134] [ip-0A0C04D9:62929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.133366] [ip-0A0C04D9:62932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.133116] [ip-0A0C04B5:10035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.133666] [ip-0A0C04AB:97583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.134139] [ip-0A0C04A2:97805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.135197] [ip-0A0C04CD:29463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.135914] [ip-0A0C04B6:76324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.136487] [ip-0A0C04A2:97800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.137158] [ip-0A0C04AB:97585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.140248] [ip-0A0C04CD:29462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.140904] [ip-0A0C04D9:62925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.141868] [ip-0A0C0490:10069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.142216] [ip-0A0C04B5:10033:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.142894] [ip-0A0C04CD:29467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.151585] [ip-0A0C04DA:26355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.152978] [ip-0A0C04DA:26356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.153157] [ip-0A0C04DA:26357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.153193] [ip-0A0C04AB:97595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.155743] [ip-0A0C0490:10074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.157306] [ip-0A0C04A1:27421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.157735] [ip-0A0C04B5:10032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.158797] [ip-0A0C04C4:42131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.163624] [ip-0A0C0490:10072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.169554] [ip-0A0C04B5:10029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.171175] [ip-0A0C04B5:10034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.171917] [ip-0A0C04A1:27425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.172039] [ip-0A0C04A1:27418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.176440] [ip-0A0C0490:10068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.184725] [ip-0A0C04C7:34168:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.193654] [ip-0A0C04C7:34164:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.194847] [ip-0A0C04C7:34166:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.201202] [ip-0A0C04C7:34165:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.208861] [ip-0A0C04C4:42132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.288314] [ip-0A0C04C4:42133:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.301298] [ip-0A0C04C4:42134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.360763] [ip-0A0C04C4:42130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.361110] [ip-0A0C04C4:42129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.381491] [ip-0A0C04C4:42135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620488.386854] [ip-0A0C04C4:42128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634620489262, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634620489302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634620489303, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634620489303, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620489303, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634620489303, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634620489304, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:15:01] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:51469 - context.c:584] INFO job (ID: 867565144527933112) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51469 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51469 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51470 - context.c:584] INFO job (ID: 867564961115825160) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51470 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51470 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51474 - context.c:584] INFO job (ID: 867564582083279641) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51474 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51474 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51471 - context.c:584] INFO job (ID: 867564390999176654) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51471 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51471 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51472 - context.c:584] INFO job (ID: 867564870292201169) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51472 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51472 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51473 - context.c:584] INFO job (ID: 867564251538322608) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51473 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51473 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51468 - context.c:584] INFO job (ID: 867565234973952359) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51468 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51468 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:51467 - context.c:584] INFO job (ID: 867564321877329522) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:51467 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:51467 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583486, "event_type": "POINT_IN_TIME", "key": "seed", "value": 758746489, "metadata": {"file": "main.py", "lineno": 72}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620583487, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:16:25] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620607593, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634620607607, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620607612, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634620607612, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620610137, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634620610137, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634620610137, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634620610138, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620611889, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 1918.521646985279, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620611890, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620611890, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1918.521646985279, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634620611890, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620611890, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620612542, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5161.923449823582, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620612542, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620612542, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5161.923449823582, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620612542, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620612542, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620613196, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5139.133737670772, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620613197, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620613197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5139.133737670772, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634620613197, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620613197, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620613856, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5099.6164448596965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620613856, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620613857, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5099.6164448596965, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634620613857, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620613857, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620614495, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5269.781765396118, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620614495, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620614495, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5269.781765396118, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634620614495, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620614495, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620615119, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.668384457702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620615119, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620615119, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.668384457702, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634620615119, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620615119, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620615739, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5425.656813918893, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620615739, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620615739, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5425.656813918893, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620615740, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620615740, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620616354, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.018205920863, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620616355, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620616355, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.018205920863, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634620616355, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620616355, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620616969, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.103101737262, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620616970, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620616970, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.103101737262, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634620616970, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620616970, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620617575, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5555.53159466178, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620617575, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620617576, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5555.53159466178, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634620617576, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620617576, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620618185, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5513.209824430727, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620618186, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620618186, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5513.209824430727, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634620618186, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620618186, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620618813, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.535895594161, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620618813, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620618813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.535895594161, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634620618814, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620618814, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620619425, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5501.634906934283, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620619425, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620619425, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5501.634906934283, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634620619425, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620619425, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620620042, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.996871644563, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620620042, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620620042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.996871644563, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634620620042, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620620042, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620620653, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5499.983780488662, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620620654, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620620654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5499.983780488662, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634620620654, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620620654, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620621272, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5437.612473796531, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620621273, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620621273, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5437.612473796531, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634620621273, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620621273, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620621881, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5529.199127277978, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620621881, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620621881, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5529.199127277978, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634620621881, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620621881, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620622496, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.654988795111, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620622496, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620622496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.654988795111, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634620622497, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620622497, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620623105, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5521.7618405792555, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620623106, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620623106, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5521.7618405792555, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634620623106, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620623106, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620623735, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5342.744399882173, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620623735, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620623736, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5342.744399882173, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634620623736, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620623736, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620624358, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.94527025448, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620624359, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620624359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.94527025448, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634620624359, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620624359, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620624980, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.849100128515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620624980, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620624980, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.849100128515, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634620624980, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620624980, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620625603, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.223521516497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620625603, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620625604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.223521516497, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634620625604, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620625604, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620626226, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5396.210263200417, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620626227, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620626227, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5396.210263200417, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634620626227, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620626227, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620626847, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.533705997513, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620626847, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620626848, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.533705997513, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634620626848, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620626848, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620627467, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.678128870575, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620627467, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620627467, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.678128870575, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634620627467, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620627467, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620628092, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.359433497819, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620628093, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620628093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.359433497819, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634620628093, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620628093, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620628709, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5455.227996940428, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620628709, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620628709, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5455.227996940428, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634620628710, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620628710, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620629331, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.011358557093, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620629332, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620629332, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.011358557093, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634620629332, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620629332, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620629949, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5447.002781697613, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620629949, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620629949, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5447.002781697613, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634620629950, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620629950, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620630569, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5427.602222515778, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620630569, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620630569, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5427.602222515778, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634620630569, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620630569, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620631187, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5441.837044473834, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620631187, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620631188, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5441.837044473834, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634620631188, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620631188, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620631812, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.8007747869815, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620631813, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620631813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.8007747869815, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634620631813, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620631813, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620632436, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5395.164953631675, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620632436, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620632436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5395.164953631675, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634620632437, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620632437, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620633056, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.275181429504, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620633056, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620633057, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.275181429504, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634620633057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620633057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620633669, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5489.725678519314, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620633669, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620633669, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5489.725678519314, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634620633670, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620633670, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620634282, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5486.979126069384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620634283, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620634283, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5486.979126069384, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634620634283, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620634283, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620634900, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5447.615495763783, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620634900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620634900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5447.615495763783, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634620634900, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620634900, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620635511, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5507.392609086955, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620635511, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620635511, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5507.392609086955, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634620635511, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620635511, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620636119, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5527.21923975966, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620636120, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620636120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5527.21923975966, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634620636120, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620636120, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620636725, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5557.694005573918, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620636725, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620636725, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5557.694005573918, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634620636725, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620636725, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620637347, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5408.252199798373, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620637347, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620637347, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5408.252199798373, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634620637347, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620637347, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620637964, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.938069930135, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620637964, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620637964, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.938069930135, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634620637964, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620637964, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620638574, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5509.435852929103, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620638575, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620638575, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5509.435852929103, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634620638575, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620638575, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620639186, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.065682485647, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620639186, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620639186, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.065682485647, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634620639186, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620639187, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620639799, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5487.865844445725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620639799, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620639799, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5487.865844445725, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634620639800, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620639800, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620640407, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5532.661372512378, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620640407, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620640407, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5532.661372512378, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634620640408, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620640408, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620641016, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5519.968884507234, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620641017, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620641017, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5519.968884507234, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634620641017, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620641017, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620641624, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5540.711811350531, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620641624, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620641624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5540.711811350531, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634620641624, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620641624, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620642240, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5462.683931806677, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620642240, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620642240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5462.683931806677, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634620642313, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620642313, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620642328, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620642756, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.884219229221344, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620642756, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620642909, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5637.803071398739, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620642910, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620642910, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5637.803071398739, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620643032, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620643033, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620643038, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620643443, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8783307671546936, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620643443, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620643647, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5473.606354774587, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620643647, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620643647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5473.606354774587, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620643684, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620643685, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620643701, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620644131, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8806264400482178, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620644131, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620644312, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.1320533645785, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620644312, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620644312, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.1320533645785, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620644348, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620644348, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620644365, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620644791, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8784549832344055, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620644791, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620644967, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.796592191518, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620644968, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620644968, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.796592191518, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620645010, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620645011, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620645026, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620645431, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8595561981201172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620645431, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620645614, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5569.757197617466, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620645615, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620645615, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5569.757197617466, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620645650, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620645650, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620645668, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620646087, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8923749923706055, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620646088, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620646269, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.0250662438575, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620646269, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620646269, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.0250662438575, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620646306, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620646306, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620646323, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620646743, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8751238584518433, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620646743, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620646924, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5443.0308552165825, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620646924, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620646925, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5443.0308552165825, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620646960, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620646960, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620646976, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620647402, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8759140968322754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620647402, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620647586, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.301668271886, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620647586, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620647587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.301668271886, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620647629, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620647630, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620647645, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620648119, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8884389400482178, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620648119, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620648321, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4861.079126714916, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620648322, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620648322, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4861.079126714916, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620648375, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620648375, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620648391, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620648796, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895071744918823, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620648796, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620648985, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5514.001483665593, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620648985, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620648985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5514.001483665593, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620649021, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620649021, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620649039, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620649450, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8984935283660889, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620649451, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620649640, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.827680756171, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620649640, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620649640, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.827680756171, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620649674, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620649675, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620649691, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620650116, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8932963609695435, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620650116, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620650314, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5260.124127160916, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620650314, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620650314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5260.124127160916, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620650351, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620650351, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620650368, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620650796, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.895698070526123, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620650796, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620650984, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5311.280520000317, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620650985, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620650985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5311.280520000317, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620651021, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620651021, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620651037, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620651467, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8824037909507751, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620651467, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620651649, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5357.88109743927, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620651649, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620651649, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5357.88109743927, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620651685, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620651685, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620651703, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620652120, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8884546756744385, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620652120, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620652297, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.310609177138, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620652297, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620652298, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.310609177138, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620652345, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620652345, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620652361, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620652763, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8864270448684692, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620652763, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620652944, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5610.0279449254085, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620652945, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620652945, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5610.0279449254085, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620652979, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620652979, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620652997, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620653423, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8806982040405273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620653423, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620653614, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5291.718148747818, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620653615, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620653615, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5291.718148747818, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620653652, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620653652, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620653670, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620654093, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8766606450080872, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620654094, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620654295, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5221.328423045502, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620654296, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620654296, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5221.328423045502, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620654331, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620654332, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620654349, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620654776, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8991885185241699, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620654776, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620654949, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5443.829823889813, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620654949, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620654950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5443.829823889813, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620654987, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620654987, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620655004, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620655423, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8858212232589722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620655423, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620655614, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.038240140964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620655615, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620655615, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.038240140964, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620655650, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620655650, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620655667, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620656091, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6855740547180176, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620656091, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620656286, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5289.249480659127, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620656286, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620656286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5289.249480659127, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620656322, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620656322, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620656339, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620656768, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9016997218132019, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620656768, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620656956, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5301.195118474017, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620656956, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620656956, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5301.195118474017, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620656991, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620656991, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620657009, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620657438, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8772815465927124, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620657438, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620657622, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5325.488469956588, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620657623, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620657623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5325.488469956588, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620657658, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620657659, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620657675, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620658105, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8897417783737183, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620658105, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620658321, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5076.745029834681, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620658321, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620658321, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5076.745029834681, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620658397, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620658398, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620658413, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620658810, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8816064596176147, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620658810, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620659000, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5580.018577807835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620659000, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620659000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5580.018577807835, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620659037, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620659037, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620659053, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620659484, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8977246284484863, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620659484, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620659670, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5306.82646924812, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620659671, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620659671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5306.82646924812, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620659705, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620659705, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620659721, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620660143, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9047157764434814, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620660143, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620660330, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5375.653440310893, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620660331, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620660331, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5375.653440310893, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620660366, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620660366, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620660382, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620660807, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8937526941299438, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620660807, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620661043, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4967.174117853559, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620661043, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620661044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4967.174117853559, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620661077, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620661078, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620661093, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620661523, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8963837027549744, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620661524, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620661725, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5195.25106694836, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620661725, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620661725, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5195.25106694836, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620661759, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620661760, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620661775, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620662200, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8903743624687195, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620662200, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620662392, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.009557355451, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620662392, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620662392, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.009557355451, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620662427, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620662427, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620662442, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620662869, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9023298025131226, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620662869, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620663059, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5316.381746595509, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620663059, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620663060, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5316.381746595509, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620663095, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620663095, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620663111, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620663535, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8927874565124512, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620663535, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620663721, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.7520896599335, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620663721, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620663721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.7520896599335, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620663756, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620663756, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620663772, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620664199, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8874166011810303, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620664199, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620664377, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.025820152725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620664378, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620664378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.025820152725, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620664413, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620664413, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620664429, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620664858, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8884626626968384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620664858, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620665039, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5370.13706501767, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620665040, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620665040, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5370.13706501767, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620665075, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620665076, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620665091, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620665522, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.890038251876831, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620665522, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620665706, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5329.576829027552, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620665707, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620665707, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5329.576829027552, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620665742, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620665742, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620665758, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620666190, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9035072326660156, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620666190, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620666374, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5324.295369256907, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620666374, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620666374, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5324.295369256907, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620666409, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620666410, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620666425, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620666857, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8888311386108398, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620666857, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620667047, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5275.226084944964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620667047, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620667047, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5275.226084944964, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620667082, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620667083, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620667097, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620667529, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9018065929412842, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620667529, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620667713, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5326.810961553072, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620667714, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620667714, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5326.810961553072, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620667749, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620667750, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620667765, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620668197, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9104931354522705, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620668197, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620668197, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634620668389, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5258.769779818335, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620668389, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620668389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5258.769779818335, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:54 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:55 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:56 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:56 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:57 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:58 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:59 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:17:59 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:00 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:01 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:02 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:03 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:04 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:05 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:06 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:07 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:14:42 AM
ENDING TIMING RUN AT 2021-10-19 05:18:08 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:14:42 AM
