+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019061345734086825
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019061345734086825
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019061345734086825
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019061345734086825
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07393/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019061345734086825_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C04C4
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C04BE
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04DA
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C04BD
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04CC
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C044D
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:13:48 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634624033.192339] [ip-0A0C042D:27773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.259538] [ip-0A0C0461:20235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.261236] [ip-0A0C0437:52753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.290747] [ip-0A0C04BE:74419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.290931] [ip-0A0C042D:27779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.292346] [ip-0A0C042D:27778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.308674] [ip-0A0C0482:43879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.312220] [ip-0A0C04B2:17441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.316696] [ip-0A0C0483:55491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.317168] [ip-0A0C0435:91509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.317838] [ip-0A0C0461:20239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.318443] [ip-0A0C0461:20241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.318494] [ip-0A0C0493:37874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.318486] [ip-0A0C0493:37868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.324795] [ip-0A0C0497:88857:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.325653] [ip-0A0C04AE:46477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.330083] [ip-0A0C04A0:30813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.331346] [ip-0A0C04BE:74418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.331478] [ip-0A0C04B2:17440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.333167] [ip-0A0C0497:88852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.335416] [ip-0A0C042D:27775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.339375] [ip-0A0C0441:49503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.340765] [ip-0A0C04CC:72567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.341466] [ip-0A0C0492:36475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.338927] [ip-0A0C04BC:69561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.342323] [ip-0A0C04C5:75706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.345235] [ip-0A0C04B9:74451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.351692] [ip-0A0C0437:52754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.352242] [ip-0A0C0482:43880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.352881] [ip-0A0C04B4:50799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.354231] [ip-0A0C0435:91505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.354239] [ip-0A0C0449:53949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.355778] [ip-0A0C04AA:16086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.354925] [ip-0A0C04B2:17442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.357551] [ip-0A0C042D:27781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.358972] [ip-0A0C0483:55488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.359259] [ip-0A0C0483:55492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.361482] [ip-0A0C047E:53091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.361950] [ip-0A0C04AE:46475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.363060] [ip-0A0C049D:42011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.363063] [ip-0A0C049D:42008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.363606] [ip-0A0C046E:58349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.364606] [ip-0A0C04BA:8870 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.366253] [ip-0A0C0492:36476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.366983] [ip-0A0C0449:53952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.369208] [ip-0A0C0482:43877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.369974] [ip-0A0C04C8:72823:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.370244] [ip-0A0C042D:27776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.371111] [ip-0A0C047E:53089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.371626] [ip-0A0C0438:57181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.372365] [ip-0A0C04CF:97267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.380451] [ip-0A0C0494:38030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.383062] [ip-0A0C04AE:46478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.380804] [ip-0A0C0484:58329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.385461] [ip-0A0C0441:49502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.389162] [ip-0A0C0485:21398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.388790] [ip-0A0C0461:20238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.390195] [ip-0A0C04A0:30814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.390563] [ip-0A0C04C5:75707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.391866] [ip-0A0C0499:42753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.392389] [ip-0A0C049D:42005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.393899] [ip-0A0C042D:27774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.395267] [ip-0A0C04B4:50796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.397134] [ip-0A0C04A0:30807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.396557] [ip-0A0C0436:56626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.396774] [ip-0A0C0498:51207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.397931] [ip-0A0C042D:27777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.399565] [ip-0A0C0435:91507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.400070] [ip-0A0C04AF:33008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.400102] [ip-0A0C04AF:33010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.400805] [ip-0A0C04B9:74453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.400930] [ip-0A0C04B9:74456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.402265] [ip-0A0C0499:42748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.402049] [ip-0A0C04CF:97263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.408004] [ip-0A0C04AA:16084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.408193] [ip-0A0C0493:37875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.409295] [ip-0A0C0438:57180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.408083] [ip-0A0C04BC:69560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.409772] [ip-0A0C04CF:97262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.410853] [ip-0A0C04AC:34845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.409052] [ip-0A0C049C:28070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.411142] [ip-0A0C04BE:74417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.409656] [ip-0A0C04BC:69565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.415575] [ip-0A0C04BA:8876 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.415726] [ip-0A0C04AA:16082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.416943] [ip-0A0C0437:52751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.416016] [ip-0A0C0494:38023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.417287] [ip-0A0C0495:40601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.417133] [ip-0A0C0497:88853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.422484] [ip-0A0C0492:36482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.424415] [ip-0A0C0441:49507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.425551] [ip-0A0C04B7:12185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.426020] [ip-0A0C0438:57182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.427255] [ip-0A0C04CC:72565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.428442] [ip-0A0C046E:58344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.428906] [ip-0A0C0490:42495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.430320] [ip-0A0C04D8:72544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.430729] [ip-0A0C0490:42494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.434893] [ip-0A0C04B1:44589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.437468] [ip-0A0C048D:42267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.437459] [ip-0A0C04BE:74415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.438073] [ip-0A0C04C9:71749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.438422] [ip-0A0C0436:56628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.436752] [ip-0A0C0484:58323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.439034] [ip-0A0C0498:51205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.440345] [ip-0A0C04B3:49050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.442721] [ip-0A0C04C8:72828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.442675] [ip-0A0C04B0:27875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.443868] [ip-0A0C0437:52756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.444798] [ip-0A0C04C0:73183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.444831] [ip-0A0C046E:58346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.445375] [ip-0A0C0437:52755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.446237] [ip-0A0C0483:55487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.446975] [ip-0A0C0497:88854:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.449350] [ip-0A0C048A:53059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.449140] [ip-0A0C0482:43876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.449782] [ip-0A0C04C9:71752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.450261] [ip-0A0C046E:58347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.449167] [ip-0A0C049B:30144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.450886] [ip-0A0C04B4:50792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.450934] [ip-0A0C046B:48574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.450917] [ip-0A0C046B:48575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.451335] [ip-0A0C04C8:72824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.452205] [ip-0A0C047E:53087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.455623] [ip-0A0C0443:19922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.456873] [ip-0A0C04AE:46474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.457997] [ip-0A0C04A0:30812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.457672] [ip-0A0C0461:20240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.457708] [ip-0A0C0461:20234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.458226] [ip-0A0C046C:51825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.458380] [ip-0A0C046C:51829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.460671] [ip-0A0C048B:39119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.461327] [ip-0A0C0498:51211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.460996] [ip-0A0C04B2:17446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.464467] [ip-0A0C04CC:72562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.463859] [ip-0A0C04AD:18324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.466594] [ip-0A0C0449:53945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.464612] [ip-0A0C0484:58324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.469146] [ip-0A0C04C4:73754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.469124] [ip-0A0C04C4:73759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.469921] [ip-0A0C0435:91506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.469505] [ip-0A0C0493:37869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.470350] [ip-0A0C04A7:18279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.470717] [ip-0A0C0485:21393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.470133] [ip-0A0C04B2:17443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.471720] [ip-0A0C04BB:16381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.471493] [ip-0A0C0461:20237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.470839] [ip-0A0C049C:28071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.473259] [ip-0A0C04A5:44005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.473766] [ip-0A0C04BA:8872 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.473628] [ip-0A0C0461:20236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.476038] [ip-0A0C0436:56625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.474529] [ip-0A0C049C:28077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.476088] [ip-0A0C0493:37870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.476713] [ip-0A0C04B1:44587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.476881] [ip-0A0C04DA:66261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.478429] [ip-0A0C04CC:72561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.477287] [ip-0A0C04A9:40455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.477866] [ip-0A0C0482:43883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480199] [ip-0A0C0437:52752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480069] [ip-0A0C04B7:12186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480063] [ip-0A0C0489:56635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480139] [ip-0A0C0489:56632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480106] [ip-0A0C0489:56634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480399] [ip-0A0C0464:56354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.481332] [ip-0A0C0437:52758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.481874] [ip-0A0C0487:56335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.480290] [ip-0A0C04BC:69562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.483551] [ip-0A0C04A1:58991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.483271] [ip-0A0C04C5:75708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.482957] [ip-0A0C048F:45882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.482677] [ip-0A0C04B2:17452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.483851] [ip-0A0C0473:54762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.483659] [ip-0A0C044D:53602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.485345] [ip-0A0C048C:37319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.486015] [ip-0A0C048A:53055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.486838] [ip-0A0C04B3:49052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.486478] [ip-0A0C0464:56350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488064] [ip-0A0C048D:42266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.485255] [ip-0A0C0484:58325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488345] [ip-0A0C0466:58927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488468] [ip-0A0C0443:19917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488599] [ip-0A0C04BD:71615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488893] [ip-0A0C0483:55490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.488769] [ip-0A0C04CD:69360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.489494] [ip-0A0C048D:42270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.491155] [ip-0A0C04A0:30811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.490971] [ip-0A0C0496:22919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.490353] [ip-0A0C04BE:74420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.490936] [ip-0A0C04B3:49045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.491815] [ip-0A0C0496:22924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.491109] [ip-0A0C04BE:74423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.491187] [ip-0A0C04D8:72543:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.491484] [ip-0A0C0485:21400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492060] [ip-0A0C04B1:44591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492376] [ip-0A0C048A:53058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492310] [ip-0A0C0437:52757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492531] [ip-0A0C04D9:94965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492561] [ip-0A0C048F:45875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.492769] [ip-0A0C0493:37873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.493419] [ip-0A0C0495:40605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.494064] [ip-0A0C04C5:75709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.494132] [ip-0A0C0441:49501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.495103] [ip-0A0C04C3:72637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.496418] [ip-0A0C04B9:74454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.498171] [ip-0A0C04C8:72825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.498596] [ip-0A0C04C9:71750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.498704] [ip-0A0C040F:26427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.499354] [ip-0A0C04AC:34843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.500077] [ip-0A0C0435:91503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.500688] [ip-0A0C0495:40606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.500131] [ip-0A0C0494:38028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.501407] [ip-0A0C04B4:50797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.503307] [ip-0A0C0482:43881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.504395] [ip-0A0C049D:42012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.503483] [ip-0A0C049B:30146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.504970] [ip-0A0C04B0:27878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.503280] [ip-0A0C04BC:69566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.505082] [ip-0A0C0429:46845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.506397] [ip-0A0C04BD:71610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.506866] [ip-0A0C04BE:74421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.506951] [ip-0A0C04AD:18319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.508210] [ip-0A0C04A7:18281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.507731] [ip-0A0C04DA:66256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.508458] [ip-0A0C0435:91504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.509238] [ip-0A0C0473:54758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.509324] [ip-0A0C0449:53946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.510608] [ip-0A0C04CC:72564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.510374] [ip-0A0C04C0:73184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.510769] [ip-0A0C0492:36481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.510333] [ip-0A0C0482:43882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.511438] [ip-0A0C0483:55493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.511631] [ip-0A0C0482:43878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.513187] [ip-0A0C0483:55489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.513361] [ip-0A0C04AE:46472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.512468] [ip-0A0C0488:55021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.513874] [ip-0A0C049D:42007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.514321] [ip-0A0C0483:55486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.514545] [ip-0A0C0438:57179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.514790] [ip-0A0C0441:49500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.515377] [ip-0A0C049D:42006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.515518] [ip-0A0C04AE:46476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.516250] [ip-0A0C04C5:75713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.516202] [ip-0A0C04B9:74455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.515685] [ip-0A0C04B2:17445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.515893] [ip-0A0C04B2:17448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.516953] [ip-0A0C0497:88856:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.517782] [ip-0A0C04BE:74422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.517095] [ip-0A0C0497:88851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.517711] [ip-0A0C04AF:33011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.518012] [ip-0A0C0493:37871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.518082] [ip-0A0C0493:37872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.518014] [ip-0A0C04DB:64823:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.520012] [ip-0A0C048D:42269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.519950] [ip-0A0C048C:37313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.519683] [ip-0A0C0499:42776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.521805] [ip-0A0C0435:91508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.521875] [ip-0A0C04C4:73758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.520665] [ip-0A0C049C:28072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.523405] [ip-0A0C04A5:43997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.524425] [ip-0A0C04AA:16088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.525240] [ip-0A0C047E:53086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.525846] [ip-0A0C0487:56341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.527429] [ip-0A0C0435:91502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.526827] [ip-0A0C04CF:97269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.528284] [ip-0A0C047E:53090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529244] [ip-0A0C04AE:46473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529931] [ip-0A0C0492:36480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.532765] [ip-0A0C04A2:32788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529808] [ip-0A0C04C3:72642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529283] [ip-0A0C0497:88855:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529910] [ip-0A0C048B:39121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.529445] [ip-0A0C0497:88858:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.531511] [ip-0A0C047E:53088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.532158] [ip-0A0C04C5:75711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.532066] [ip-0A0C040F:26432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.532685] [ip-0A0C049D:42004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.531397] [ip-0A0C04BC:69563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.533292] [ip-0A0C0473:54757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.533793] [ip-0A0C04AC:34846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.533813] [ip-0A0C04B4:50794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.533721] [ip-0A0C049D:42003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.535636] [ip-0A0C04A0:30808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.534542] [ip-0A0C045F:54154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.535039] [ip-0A0C0485:21396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.534520] [ip-0A0C0449:53951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.534898] [ip-0A0C046E:58351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.537958] [ip-0A0C04A0:30809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.537125] [ip-0A0C0485:21397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.537711] [ip-0A0C04CC:72566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.537853] [ip-0A0C04CC:72568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.537922] [ip-0A0C04AE:46479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.538507] [ip-0A0C0449:53950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.539409] [ip-0A0C0441:49504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.540995] [ip-0A0C04A0:30810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.540191] [ip-0A0C0472:58296:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.540244] [ip-0A0C0441:49506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.540009] [ip-0A0C04A9:40458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.541011] [ip-0A0C0441:49505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.541000] [ip-0A0C0449:53947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.541297] [ip-0A0C04AB:31815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.539947] [ip-0A0C04BC:69564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.542485] [ip-0A0C04B9:74452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.541114] [ip-0A0C04BC:69559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.543150] [ip-0A0C0438:57183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.544914] [ip-0A0C04A6:41091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.544331] [ip-0A0C044D:53606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.545368] [ip-0A0C04BB:16383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.546265] [ip-0A0C0496:22923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.545551] [ip-0A0C0466:58931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.546685] [ip-0A0C0492:36484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.546863] [ip-0A0C0492:36478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.546579] [ip-0A0C0449:53948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.549416] [ip-0A0C0499:42751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.549104] [ip-0A0C04A8:46564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.549866] [ip-0A0C0443:19919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.550366] [ip-0A0C04BA:8877 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.550715] [ip-0A0C046B:48573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.551022] [ip-0A0C04C2:96432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.551867] [ip-0A0C04D8:72546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.552270] [ip-0A0C0485:21394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.553113] [ip-0A0C0438:57178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.553555] [ip-0A0C04B4:50793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.553772] [ip-0A0C04BA:8871 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556887] [ip-0A0C04A2:32787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.553551] [ip-0A0C049F:29259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.554026] [ip-0A0C046E:58348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.554158] [ip-0A0C04C5:75710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.554594] [ip-0A0C04AF:33013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.555355] [ip-0A0C04A7:18282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.555325] [ip-0A0C047E:53092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.555319] [ip-0A0C04B9:74450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.555159] [ip-0A0C04CF:97268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.555935] [ip-0A0C04C8:72821:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556516] [ip-0A0C0487:56338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556757] [ip-0A0C04C9:71746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556390] [ip-0A0C04B9:74449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.557471] [ip-0A0C0492:36483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556966] [ip-0A0C04B0:27873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.557069] [ip-0A0C04C5:75712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.556744] [ip-0A0C0494:38027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.557906] [ip-0A0C046E:58350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.558613] [ip-0A0C04CC:72563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.558138] [ip-0A0C04C4:73757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.558661] [ip-0A0C0438:57184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.559305] [ip-0A0C0436:56627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.558992] [ip-0A0C04CF:97265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.560341] [ip-0A0C04B7:12183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.561077] [ip-0A0C0438:57185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.561621] [ip-0A0C048A:53056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.561053] [ip-0A0C046C:51827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.562058] [ip-0A0C047E:53085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.562219] [ip-0A0C046E:58345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.562483] [ip-0A0C04B1:44588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.562482] [ip-0A0C04AA:16089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.559873] [ip-0A0C0484:58327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.562227] [ip-0A0C04B5:42058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.564737] [ip-0A0C04B4:50795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.563782] [ip-0A0C04DB:64827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.565113] [ip-0A0C04B4:50798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.566296] [ip-0A0C048C:37317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.565822] [ip-0A0C04CF:97264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.564415] [ip-0A0C0484:58322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.567548] [ip-0A0C04BA:8869 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.567807] [ip-0A0C048F:45878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.567849] [ip-0A0C0489:56633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.567041] [ip-0A0C049C:28073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.570315] [ip-0A0C04CF:97266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.570840] [ip-0A0C04CD:69364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.571685] [ip-0A0C04BA:8874 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.571708] [ip-0A0C0499:42749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.573223] [ip-0A0C04D8:72541:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.573143] [ip-0A0C0460:55994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.574502] [ip-0A0C04A7:18280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.574668] [ip-0A0C04C8:72826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.574889] [ip-0A0C04DA:66258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.575297] [ip-0A0C04C0:73182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.575587] [ip-0A0C04AA:16087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.575714] [ip-0A0C0466:58926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.575879] [ip-0A0C04AA:16085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.576241] [ip-0A0C04C8:72829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.576294] [ip-0A0C04C8:72822:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.577586] [ip-0A0C04C7:73757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.577210] [ip-0A0C0494:38024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.578224] [ip-0A0C04AA:16083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.577798] [ip-0A0C0498:51210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.579013] [ip-0A0C0494:38029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.580072] [ip-0A0C04BA:8868 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.579056] [ip-0A0C04DB:64828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.580728] [ip-0A0C0488:55016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.580923] [ip-0A0C04C6:69547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.581913] [ip-0A0C0466:58932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.586711] [ip-0A0C04A2:32786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583640] [ip-0A0C04C3:72636:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583809] [ip-0A0C0436:56619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583942] [ip-0A0C0436:56621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.584094] [ip-0A0C04D9:94967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583898] [ip-0A0C04AF:33015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.584348] [ip-0A0C0436:56620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583541] [ip-0A0C049B:30147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.584859] [ip-0A0C04AF:33014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.585528] [ip-0A0C0495:40604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.583927] [ip-0A0C049C:28074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.586357] [ip-0A0C0494:38022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.587914] [ip-0A0C04AB:31816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.588155] [ip-0A0C04B0:27874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.588207] [ip-0A0C0429:46848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.588977] [ip-0A0C0490:42492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.589638] [ip-0A0C04AD:18318:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.589998] [ip-0A0C04AC:34848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.589632] [ip-0A0C04CD:69361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.587719] [ip-0A0C0484:58326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.591797] [ip-0A0C04BB:16380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.591905] [ip-0A0C04C0:73179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.592464] [ip-0A0C0494:38025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.590518] [ip-0A0C0484:58337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.592811] [ip-0A0C04B6:10535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.595972] [ip-0A0C0485:21399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.595492] [ip-0A0C04AF:33007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.595423] [ip-0A0C048F:45877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.595350] [ip-0A0C0498:51209:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.596509] [ip-0A0C04AC:34849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.596633] [ip-0A0C04B7:12188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.596357] [ip-0A0C0490:42499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.597377] [ip-0A0C04D8:72539:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.598211] [ip-0A0C0485:21395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.598327] [ip-0A0C04C4:73756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.599819] [ip-0A0C046C:51831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.600498] [ip-0A0C04BB:16382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.600455] [ip-0A0C04C3:72638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.601475] [ip-0A0C04A7:18278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.601119] [ip-0A0C04B7:12189:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.601573] [ip-0A0C04A1:58994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.601902] [ip-0A0C04D8:72540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.602891] [ip-0A0C04A1:58992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.602688] [ip-0A0C0499:42752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.603288] [ip-0A0C04A4:41468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.603242] [ip-0A0C0443:19920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.603864] [ip-0A0C04C2:96433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.603985] [ip-0A0C0495:40603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.603674] [ip-0A0C04D3:70972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.605309] [ip-0A0C0495:40599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.605719] [ip-0A0C0436:56623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.606197] [ip-0A0C0472:58289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.605747] [ip-0A0C044D:53605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.606124] [ip-0A0C04B5:42063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.607539] [ip-0A0C048B:39124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.607310] [ip-0A0C0489:56631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.607951] [ip-0A0C048B:39118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.608872] [ip-0A0C04D9:94966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.609683] [ip-0A0C04AF:33012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.611410] [ip-0A0C04A6:41090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.610977] [ip-0A0C04A9:40461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.611609] [ip-0A0C0499:42755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.611798] [ip-0A0C049F:29261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.612330] [ip-0A0C0499:42750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.612765] [ip-0A0C04C7:73759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.613310] [ip-0A0C04D9:94969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.613954] [ip-0A0C04A6:41089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.612540] [ip-0A0C0498:51206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.613443] [ip-0A0C04AD:18322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.612887] [ip-0A0C0498:51204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.613949] [ip-0A0C045F:54149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614173] [ip-0A0C046B:48576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614381] [ip-0A0C04B3:49049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614042] [ip-0A0C045F:54147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614676] [ip-0A0C04C0:73181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.615301] [ip-0A0C04B1:44593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614582] [ip-0A0C0498:51208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.616350] [ip-0A0C04B0:27893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.614969] [ip-0A0C049C:28097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.617160] [ip-0A0C048A:53061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.615249] [ip-0A0C049C:28076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.617122] [ip-0A0C04B0:27871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.617622] [ip-0A0C044D:53600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.618863] [ip-0A0C04AC:34842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.617344] [ip-0A0C049B:30145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.619328] [ip-0A0C04A4:41470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.619232] [ip-0A0C04C4:73782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.619206] [ip-0A0C049F:29257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.620089] [ip-0A0C046B:48571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.619872] [ip-0A0C04D3:70974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.620869] [ip-0A0C048D:42271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.621150] [ip-0A0C04A9:40462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.622950] [ip-0A0C04B6:10531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.624262] [ip-0A0C048A:53057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.624437] [ip-0A0C04C9:71747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.624787] [ip-0A0C04C9:71745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.624798] [ip-0A0C0486:56347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.625279] [ip-0A0C04A8:46559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.626243] [ip-0A0C04B3:49046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.625812] [ip-0A0C046C:51832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.627225] [ip-0A0C04AC:34844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.626244] [ip-0A0C04A8:46566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.627138] [ip-0A0C048D:42272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.626334] [ip-0A0C04A8:46565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.626910] [ip-0A0C04A5:43999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.627727] [ip-0A0C0487:56337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.627686] [ip-0A0C04B7:12187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.627962] [ip-0A0C04BD:71611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.628593] [ip-0A0C046B:48577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.629485] [ip-0A0C04B1:44592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.629125] [ip-0A0C0490:42493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.630308] [ip-0A0C04BB:16385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.631351] [ip-0A0C04AC:34847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.630690] [ip-0A0C0490:42498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.631703] [ip-0A0C04B1:44590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.631465] [ip-0A0C048C:37316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.633565] [ip-0A0C0429:46852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.633951] [ip-0A0C0466:58928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.634016] [ip-0A0C04B0:27877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.634018] [ip-0A0C048C:37314:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.634183] [ip-0A0C046B:48570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.635363] [ip-0A0C048B:39120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.635754] [ip-0A0C048D:42268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.635541] [ip-0A0C0464:56348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.636204] [ip-0A0C0495:40602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.636125] [ip-0A0C04A5:44000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.637320] [ip-0A0C048D:42265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.637648] [ip-0A0C04C0:73185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.637768] [ip-0A0C04B7:12190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.638320] [ip-0A0C0495:40600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.638555] [ip-0A0C0464:56355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.638411] [ip-0A0C0488:55015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.639375] [ip-0A0C0472:58294:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.639788] [ip-0A0C04A1:58993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.639676] [ip-0A0C04B7:12184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.640951] [ip-0A0C0496:22926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.639945] [ip-0A0C04AB:31810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641078] [ip-0A0C04A1:58995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.640901] [ip-0A0C04C4:73755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641989] [ip-0A0C048A:53054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641341] [ip-0A0C0489:56630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.642044] [ip-0A0C04AD:18320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641944] [ip-0A0C0429:46846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641425] [ip-0A0C0490:42496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.641938] [ip-0A0C0460:55990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.642512] [ip-0A0C048A:53060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.642400] [ip-0A0C040F:26429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.642536] [ip-0A0C04C4:73753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.643086] [ip-0A0C046B:48572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.643790] [ip-0A0C04C9:71748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.643518] [ip-0A0C04A5:43998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.644085] [ip-0A0C04C9:71751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.644085] [ip-0A0C0486:56343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.644062] [ip-0A0C04DA:66257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.643718] [ip-0A0C04C6:69548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.644208] [ip-0A0C04DA:66259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.645526] [ip-0A0C04D4:68940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.646734] [ip-0A0C04B0:27872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.647291] [ip-0A0C0490:42497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648022] [ip-0A0C046C:51828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.647926] [ip-0A0C048F:45883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648335] [ip-0A0C04BD:71613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648819] [ip-0A0C04B1:44594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648943] [ip-0A0C040F:26426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648476] [ip-0A0C049B:30143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.648647] [ip-0A0C049B:30149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.650036] [ip-0A0C04CD:69366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.650315] [ip-0A0C04CD:69365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.650791] [ip-0A0C04C0:73180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.650606] [ip-0A0C0489:56628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.650771] [ip-0A0C04A9:40456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.652065] [ip-0A0C04BB:16378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.652013] [ip-0A0C0466:58933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.652130] [ip-0A0C0486:56345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.652484] [ip-0A0C04BB:16379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.653425] [ip-0A0C0496:22927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.652853] [ip-0A0C048C:37315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.653860] [ip-0A0C04C0:73178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.653503] [ip-0A0C0443:19921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.653932] [ip-0A0C04D8:72547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.653597] [ip-0A0C0489:56629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.655249] [ip-0A0C04BB:16384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.656192] [ip-0A0C04D8:72542:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.656435] [ip-0A0C045F:54148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.657236] [ip-0A0C046C:51830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.657513] [ip-0A0C04C2:96427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.657559] [ip-0A0C0473:54759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.659328] [ip-0A0C0473:54763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.659571] [ip-0A0C046C:51833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.660282] [ip-0A0C04A7:18286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.660347] [ip-0A0C04B3:49047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.661999] [ip-0A0C04BD:71609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.662349] [ip-0A0C04B5:42061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.662488] [ip-0A0C04DB:64824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.663751] [ip-0A0C045F:54153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.663968] [ip-0A0C048F:45881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.665021] [ip-0A0C04A5:44002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.666334] [ip-0A0C04A6:41088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.666709] [ip-0A0C04B3:49048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.666389] [ip-0A0C048F:45876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.666766] [ip-0A0C0429:46847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.666472] [ip-0A0C0488:55019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.667214] [ip-0A0C04A5:44001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.667759] [ip-0A0C04D4:68936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.667805] [ip-0A0C0443:19918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.667857] [ip-0A0C044D:53604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.668879] [ip-0A0C04B3:49051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.670042] [ip-0A0C04A7:18283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.669837] [ip-0A0C04A1:58996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.670186] [ip-0A0C04A7:18284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.670309] [ip-0A0C04DB:64826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.671236] [ip-0A0C0443:19916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.671332] [ip-0A0C0443:19923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.671342] [ip-0A0C0464:56347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.671583] [ip-0A0C0464:56353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.672593] [ip-0A0C0487:56339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.672520] [ip-0A0C0473:54761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.672706] [ip-0A0C048F:45879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.673524] [ip-0A0C048B:39116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.673735] [ip-0A0C0473:54760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.673413] [ip-0A0C04A9:40457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.673951] [ip-0A0C04AD:18323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.674227] [ip-0A0C0466:58929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.674289] [ip-0A0C0466:58930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.674429] [ip-0A0C048B:39122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.674554] [ip-0A0C048B:39117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.675572] [ip-0A0C04A1:58990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.675396] [ip-0A0C04C3:72635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.676666] [ip-0A0C0496:22922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.676122] [ip-0A0C04D9:94972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.675584] [ip-0A0C049B:30148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.677689] [ip-0A0C04AD:18317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.680745] [ip-0A0C04A2:32789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.677553] [ip-0A0C0464:56352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.677735] [ip-0A0C048C:37312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.677802] [ip-0A0C04CD:69363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.677814] [ip-0A0C048C:37318:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.678787] [ip-0A0C0473:54771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.680547] [ip-0A0C04C3:72641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.679386] [ip-0A0C049B:30150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.680399] [ip-0A0C04D3:70977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.680796] [ip-0A0C04AD:18321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.680655] [ip-0A0C0488:55018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.681598] [ip-0A0C04C7:73761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.681668] [ip-0A0C04C7:73755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.682444] [ip-0A0C04A5:44004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.683788] [ip-0A0C0496:22920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.682795] [ip-0A0C04A4:41463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.684148] [ip-0A0C04D9:94970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.683546] [ip-0A0C044D:53603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.684334] [ip-0A0C0487:56340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.683825] [ip-0A0C044D:53601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.685716] [ip-0A0C0496:22921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.684488] [ip-0A0C0429:46851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.684601] [ip-0A0C0491:38391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.684605] [ip-0A0C0491:38396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.685635] [ip-0A0C04A1:58997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.685834] [ip-0A0C04DA:66254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.685665] [ip-0A0C04A9:40454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.685810] [ip-0A0C04A9:40460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.686426] [ip-0A0C04DA:66255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.686080] [ip-0A0C044D:53607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.687769] [ip-0A0C04DA:66260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.691118] [ip-0A0C04BD:71614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.691425] [ip-0A0C040F:26430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.691926] [ip-0A0C04D9:94968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.692264] [ip-0A0C04BD:71612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.692436] [ip-0A0C04BD:71608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.693871] [ip-0A0C0488:55020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.695327] [ip-0A0C0464:56349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.695665] [ip-0A0C0487:56342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.695921] [ip-0A0C04CD:69362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.695997] [ip-0A0C04CD:69367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.696057] [ip-0A0C0429:46850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.696114] [ip-0A0C0429:46849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.699491] [ip-0A0C040F:26431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.700072] [ip-0A0C04C3:72640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.700175] [ip-0A0C040F:26425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.700386] [ip-0A0C04C2:96426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.700828] [ip-0A0C040F:26428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.700214] [ip-0A0C0488:55014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.702012] [ip-0A0C0487:56336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.703821] [ip-0A0C04C3:72639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.704443] [ip-0A0C04B5:42062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.710679] [ip-0A0C04D9:94971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.711720] [ip-0A0C04A6:41085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.712865] [ip-0A0C0488:55017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.715009] [ip-0A0C045F:54177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.716050] [ip-0A0C0472:58292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.715449] [ip-0A0C04C6:69551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.716220] [ip-0A0C0472:58290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.717417] [ip-0A0C045F:54152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.717642] [ip-0A0C045F:54150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.717114] [ip-0A0C04DB:64825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.719004] [ip-0A0C04DB:64822:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.721357] [ip-0A0C04A6:41087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.720890] [ip-0A0C04AB:31809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.721377] [ip-0A0C04DB:64837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.722218] [ip-0A0C0460:56020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.722069] [ip-0A0C049F:29262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.722334] [ip-0A0C049F:29258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.727960] [ip-0A0C04C6:69549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.730286] [ip-0A0C04D3:70971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.734274] [ip-0A0C04A2:32783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.732869] [ip-0A0C04A6:41092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.734884] [ip-0A0C04A6:41086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.734840] [ip-0A0C0472:58295:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.736916] [ip-0A0C04C2:96429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.742251] [ip-0A0C04A2:32785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.740535] [ip-0A0C04A8:46563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.740887] [ip-0A0C04A8:46562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.742361] [ip-0A0C04AB:31813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.742242] [ip-0A0C04A8:46560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.743517] [ip-0A0C04B6:10532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.744485] [ip-0A0C04AB:31811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.747329] [ip-0A0C0472:58291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.747109] [ip-0A0C0460:55992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.749372] [ip-0A0C04C2:96431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.749362] [ip-0A0C0460:55997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.749083] [ip-0A0C04A8:46561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.753186] [ip-0A0C049F:29260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.753242] [ip-0A0C049F:29263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.753692] [ip-0A0C04B5:42064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.753803] [ip-0A0C04C6:69552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.754436] [ip-0A0C04C6:69546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.759250] [ip-0A0C04A2:32790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.759338] [ip-0A0C04A2:32784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.757330] [ip-0A0C0472:58293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.762505] [ip-0A0C04AB:31814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.762881] [ip-0A0C04C2:96430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.762708] [ip-0A0C04AB:31812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.763006] [ip-0A0C04C2:96428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.765832] [ip-0A0C04B5:42060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.765959] [ip-0A0C049F:29256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.766423] [ip-0A0C04C7:73758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.768191] [ip-0A0C04D3:70975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.770743] [ip-0A0C0460:55995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.772140] [ip-0A0C04A4:41467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.771622] [ip-0A0C04B6:10530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.773236] [ip-0A0C04C6:69550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.774509] [ip-0A0C04C6:69545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.779864] [ip-0A0C04A4:41465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.781109] [ip-0A0C04B6:10534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.782716] [ip-0A0C04C7:73756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.783735] [ip-0A0C04B5:42065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.784265] [ip-0A0C04D3:70976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.786133] [ip-0A0C04B6:10533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.787036] [ip-0A0C04B5:42059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.788158] [ip-0A0C04C7:73760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.788132] [ip-0A0C0460:55993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.789723] [ip-0A0C0486:56348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.791394] [ip-0A0C04D4:68934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.791887] [ip-0A0C0460:55991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.792831] [ip-0A0C04C7:73762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.795786] [ip-0A0C04D3:70970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.796395] [ip-0A0C0486:56342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.799822] [ip-0A0C04A4:41469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.799805] [ip-0A0C04D3:70973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.800078] [ip-0A0C04B6:10536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.803986] [ip-0A0C04B6:10529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.807489] [ip-0A0C0491:38395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.807837] [ip-0A0C04A4:41464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.807972] [ip-0A0C04A4:41466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.814210] [ip-0A0C04D4:68939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.818593] [ip-0A0C0486:56371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.832534] [ip-0A0C0486:56346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.832737] [ip-0A0C0486:56344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.833145] [ip-0A0C0491:38397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.833798] [ip-0A0C0491:38392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.837007] [ip-0A0C04D4:68935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.837241] [ip-0A0C04D4:68938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.837822] [ip-0A0C04D4:68933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.849798] [ip-0A0C04D4:68937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.868596] [ip-0A0C0491:38394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.868822] [ip-0A0C0491:38393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624033.883868] [ip-0A0C0491:38398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634624034786, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634624034827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634624034827, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634624034828, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624034828, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634624034828, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634624034828, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:05] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:06] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:07] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:14:08] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:91502 - context.c:584] INFO job (ID: 867564458054670034) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91502 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91502 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91507 - context.c:584] INFO job (ID: 867564220469669448) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91507 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91507 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91503 - context.c:584] INFO job (ID: 867564319790453175) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91503 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91503 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91509 - context.c:584] INFO job (ID: 867565210265597208) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91509 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91509 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91504 - context.c:584] INFO job (ID: 867564526669932565) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91504 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91504 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91505 - context.c:584] INFO job (ID: 867564182526289660) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91505 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91505 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91508 - context.c:584] INFO job (ID: 867564616492930838) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91508 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91508 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:91506 - context.c:584] INFO job (ID: 867564349099979070) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:91506 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:91506 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130276, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4269211110, "metadata": {"file": "main.py", "lineno": 72}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130276, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130276, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130276, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130276, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624130277, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:15:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624154288, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634624154311, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624154315, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634624154315, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624156932, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634624156932, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634624156933, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624156933, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624158491, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2156.2545522312016, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624158492, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624158492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2156.2545522312016, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634624158492, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624158492, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624159180, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4888.822320973264, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624159180, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624159180, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4888.822320973264, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624159181, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624159181, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624159851, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5015.695847889778, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624159851, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624159851, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5015.695847889778, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624159851, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624159852, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624160497, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5205.359966757468, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624160498, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624160498, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5205.359966757468, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634624160499, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624160499, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624161116, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5447.407031178431, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624161116, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624161116, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5447.407031178431, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634624161116, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624161117, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624161734, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5447.659717575514, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624161734, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624161734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5447.659717575514, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634624161734, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624161734, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624162361, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.45022700694, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624162361, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624162362, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.45022700694, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624162362, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624162362, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624162978, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.636260207529, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624162979, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624162979, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.636260207529, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634624162979, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624162979, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624163599, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.886336831776, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624163599, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624163599, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.886336831776, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634624163600, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624163600, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624164217, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.620613266646, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624164217, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624164217, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.620613266646, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634624164218, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624164218, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624164840, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5402.310462062513, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624164840, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624164840, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5402.310462062513, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634624164840, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624164840, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624165459, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5429.346125855892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624165460, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624165460, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5429.346125855892, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634624165460, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624165460, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624166087, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5358.995557766152, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624166088, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624166088, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5358.995557766152, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634624166088, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624166088, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624166703, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5463.018509243768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624166703, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624166703, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5463.018509243768, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634624166704, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624166704, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624167327, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.188705070904, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624167328, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624167328, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.188705070904, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634624167328, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624167328, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624167959, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5326.798881032395, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624167959, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624167960, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5326.798881032395, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634624167960, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624167960, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624168577, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.476897388007, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624168577, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624168577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.476897388007, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634624168577, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624168577, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624169207, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5341.403863450043, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624169207, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624169207, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5341.403863450043, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634624169207, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624169208, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624169827, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.916677737793, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624169827, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624169828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.916677737793, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634624169828, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624169828, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624170454, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.480492731086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624170455, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624170455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.480492731086, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634624170455, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624170455, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624171082, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.062921047182, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624171083, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624171083, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.062921047182, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634624171083, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624171083, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624171706, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5389.761013207014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624171707, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624171707, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5389.761013207014, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634624171707, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624171707, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624172333, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.180886289767, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624172333, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624172333, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.180886289767, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634624172333, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624172333, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624172954, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.37097723965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624172954, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624172954, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.37097723965, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634624172955, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624172955, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624173576, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.356421533793, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624173576, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624173576, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.356421533793, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634624173576, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624173576, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624174200, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5386.901455773498, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624174201, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624174201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5386.901455773498, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634624174201, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624174201, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624174826, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.2834890454515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624174826, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624174827, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.2834890454515, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634624174827, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624174827, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624175441, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5474.814146626855, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624175441, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624175441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5474.814146626855, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634624175441, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624175441, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624176059, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5439.077312228801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624176060, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624176060, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5439.077312228801, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634624176060, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624176060, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624176684, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.66097725304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624176685, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624176685, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.66097725304, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634624176685, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624176685, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624177306, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5414.968421659318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624177306, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624177307, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5414.968421659318, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634624177307, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624177307, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624177918, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5502.182638496839, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624177918, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624177918, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5502.182638496839, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634624177918, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624177918, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624178538, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.164431744424, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624178539, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624178539, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.164431744424, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634624178539, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624178539, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624179159, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5419.495470121724, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624179160, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624179160, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5419.495470121724, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634624179160, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624179160, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624179767, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5532.259573203549, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624179768, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624179768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5532.259573203549, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634624179768, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624179768, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624180377, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5518.656806893278, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624180378, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624180378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5518.656806893278, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634624180378, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624180378, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624180995, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5445.323262496682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624180996, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624180996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5445.323262496682, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634624180996, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624180996, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624181609, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.615948955499, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624181610, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624181610, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.615948955499, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634624181610, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624181610, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624182218, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5527.258259984892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624182219, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624182219, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5527.258259984892, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634624182219, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624182219, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624182836, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.090915665269, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624182836, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624182836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.090915665269, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634624182836, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624182837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624183443, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5539.387678013489, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624183444, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624183444, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5539.387678013489, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634624183444, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624183444, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624184054, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5507.741295803359, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624184055, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624184055, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5507.741295803359, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634624184055, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624184055, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624184671, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.218412763261, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624184672, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624184672, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.218412763261, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634624184672, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624184672, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624185279, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5542.4071029085635, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624185280, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624185280, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5542.4071029085635, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634624185280, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624185280, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624185893, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5485.815075649172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624185893, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624185893, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5485.815075649172, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634624185893, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624185894, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624186508, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5471.566227451358, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624186508, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624186508, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5471.566227451358, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634624186508, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624186509, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624187116, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5528.238281847603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624187117, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624187117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5528.238281847603, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634624187117, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624187117, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624187729, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5490.119184245895, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624187730, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624187730, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5490.119184245895, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634624187730, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624187731, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624188340, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5515.542311548821, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624188340, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624188340, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5515.542311548821, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634624188341, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624188341, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624188953, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5488.885390494433, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624188954, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624188954, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5488.885390494433, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634624189024, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624189025, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624189041, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624189473, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8804759979248047, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624189473, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624189641, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.255321041487, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624189642, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624189642, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.255321041487, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624189733, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624189733, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624189748, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624190236, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.889795184135437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624190236, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624190467, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4580.05058157107, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624190467, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624190467, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4580.05058157107, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624190543, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624190543, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624190557, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624190955, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8792738914489746, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624190955, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624191144, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5594.085778942325, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624191145, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624191145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5594.085778942325, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624191215, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624191215, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624191230, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624191628, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8881312608718872, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624191628, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624191815, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5598.1701156548115, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624191816, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624191816, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5598.1701156548115, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624191890, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624191890, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624191905, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624192303, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8903703689575195, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624192303, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624192495, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5557.187758530266, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624192496, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624192496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5557.187758530266, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624192595, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624192595, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624192609, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624193007, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.868883490562439, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624193008, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624193202, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5537.894431989877, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624193202, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624193203, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5537.894431989877, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624193280, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624193281, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624193295, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624193692, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9032294750213623, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624193693, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624193880, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5610.313811229177, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624193880, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624193880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5610.313811229177, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624193959, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624193959, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624193973, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624194372, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.878724217414856, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624194372, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624194556, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5630.689553495872, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624194557, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624194557, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5630.689553495872, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624194632, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624194632, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624194647, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624195045, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8837732076644897, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624195045, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624195231, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5606.724730303266, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624195232, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624195232, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5606.724730303266, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624195309, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624195309, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624195324, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624195722, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8868951201438904, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624195722, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624195921, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5494.329373722945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624195921, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624195921, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5494.329373722945, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624196000, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624196000, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624196015, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624196413, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8447976112365723, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624196413, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624196604, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5567.624983407184, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624196604, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624196604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5567.624983407184, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624196701, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624196701, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624196716, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624197115, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8922580480575562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624197116, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624197310, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5521.81376497914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624197310, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624197310, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5521.81376497914, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624197389, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624197389, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624197404, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624197801, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8867905139923096, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624197802, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624197996, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5535.697385709488, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624197997, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624197997, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5535.697385709488, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624198070, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624198071, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624198085, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624198489, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8915426135063171, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624198490, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624198694, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.953107272692, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624198695, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624198695, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.953107272692, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624198774, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624198775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624198789, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624199188, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8877153396606445, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624199188, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624199375, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5597.47638038747, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624199375, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624199376, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5597.47638038747, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624199454, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624199455, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624199469, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624199867, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897042989730835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624199868, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624200062, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5532.4658952884265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624200063, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624200063, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5532.4658952884265, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624200142, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624200142, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624200158, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624200555, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897181510925293, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624200555, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624200751, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5520.520272922352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624200751, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624200751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5520.520272922352, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624200831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624200831, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624200846, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624201244, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9004397392272949, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624201244, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624201438, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5535.30384179708, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624201439, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624201439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5535.30384179708, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624201517, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624201517, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624201530, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624201930, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.884661078453064, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624201930, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624202128, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5504.101632968538, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624202128, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624202128, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5504.101632968538, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624202205, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624202205, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624202219, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624202617, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9107964038848877, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624202617, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624202618, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634624202816, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5501.817472004597, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624202816, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624202816, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5501.817472004597, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
ENDING TIMING RUN AT 2021-10-19 06:16:48 AM
RESULT,image_segmentation,,180,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:49 AM
RESULT,image_segmentation,,181,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:50 AM
RESULT,image_segmentation,,182,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:50 AM
RESULT,image_segmentation,,182,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:50 AM
RESULT,image_segmentation,,182,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:52 AM
RESULT,image_segmentation,,184,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:53 AM
RESULT,image_segmentation,,185,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:55 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:56 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:57 AM
RESULT,image_segmentation,,189,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:58 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:16:59 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:00 AM
RESULT,image_segmentation,,192,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:01 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
ENDING TIMING RUN AT 2021-10-19 06:17:02 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 06:13:48 AM
