+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019052027692796601
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019052027692796601
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019052027692796601
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019052027692796601
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07356/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019052027692796601_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C0475
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C0459
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:20:30 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634620834.989037] [ip-0A0C040E:30333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.004378] [ip-0A0C0410:59009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.033063] [ip-0A0C040B:86083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.050272] [ip-0A0C040E:30332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.065111] [ip-0A0C0410:59016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.086490] [ip-0A0C040B:86085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.088614] [ip-0A0C0465:14462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.099372] [ip-0A0C042C:35561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.108832] [ip-0A0C040A:44017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.113602] [ip-0A0C042C:35559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.115828] [ip-0A0C045E:23772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.117208] [ip-0A0C040E:30326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.127592] [ip-0A0C0410:59012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.148104] [ip-0A0C0409:1246 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.148116] [ip-0A0C0409:1249 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.149851] [ip-0A0C0450:95751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.152083] [ip-0A0C040E:30331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.153131] [ip-0A0C045E:23770:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.155720] [ip-0A0C043B:27178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.156473] [ip-0A0C0410:59010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.159801] [ip-0A0C045E:23795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.161161] [ip-0A0C040B:86079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.168595] [ip-0A0C0450:95752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.170968] [ip-0A0C0465:14461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.174418] [ip-0A0C0442:22332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.174968] [ip-0A0C043B:27182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.177760] [ip-0A0C040B:86080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.177998] [ip-0A0C041C:48126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.178079] [ip-0A0C0410:59011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.178277] [ip-0A0C040A:44008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.180634] [ip-0A0C041F:46098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.181868] [ip-0A0C046D:11373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.182866] [ip-0A0C0518:40934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.184043] [ip-0A0C040C:54669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.184896] [ip-0A0C040E:30327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.185077] [ip-0A0C040E:30330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.188445] [ip-0A0C0479:5001 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.189320] [ip-0A0C0465:14466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.189677] [ip-0A0C0410:59013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.189999] [ip-0A0C041B:60381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.190217] [ip-0A0C0432:58346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.190780] [ip-0A0C040E:30329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.192713] [ip-0A0C0410:59014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.194298] [ip-0A0C041F:46099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.194878] [ip-0A0C040E:30328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.194861] [ip-0A0C0410:59017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.202386] [ip-0A0C043D:18831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.205165] [ip-0A0C044F:20465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.205737] [ip-0A0C0409:1251 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.207350] [ip-0A0C040B:86086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.209388] [ip-0A0C0518:40936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.212153] [ip-0A0C0442:22330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.212559] [ip-0A0C040B:86084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.212899] [ip-0A0C0442:22328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.212924] [ip-0A0C0476:2631 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.214278] [ip-0A0C045A:26353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.214921] [ip-0A0C0426:40814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.215361] [ip-0A0C041B:60379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.218587] [ip-0A0C044F:20469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.220190] [ip-0A0C0465:14465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.220289] [ip-0A0C040A:44010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.223613] [ip-0A0C040C:54664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.226254] [ip-0A0C0476:2628 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.226773] [ip-0A0C0419:46052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.226934] [ip-0A0C0423:36365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.234673] [ip-0A0C042C:35560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.234385] [ip-0A0C047F:7748 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.238176] [ip-0A0C040B:86081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.238353] [ip-0A0C040B:86082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.241087] [ip-0A0C043A:27684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.241867] [ip-0A0C0426:40810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.241970] [ip-0A0C0440:21883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.241754] [ip-0A0C0432:58349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.244033] [ip-0A0C0423:36362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.245088] [ip-0A0C0479:5007 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.244839] [ip-0A0C046D:11371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.244993] [ip-0A0C046D:11370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.245188] [ip-0A0C046D:11372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.245180] [ip-0A0C0422:62721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.246143] [ip-0A0C0432:58351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.247815] [ip-0A0C041D:39198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.249457] [ip-0A0C042F:37435:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.251387] [ip-0A0C0419:46055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.251237] [ip-0A0C0440:21879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.251695] [ip-0A0C0479:5002 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.253173] [ip-0A0C0450:95749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.253236] [ip-0A0C044A:21783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.254167] [ip-0A0C045D:13994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.255036] [ip-0A0C040A:44015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.255953] [ip-0A0C045A:26359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.256082] [ip-0A0C043B:27176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.257514] [ip-0A0C041C:48123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.258376] [ip-0A0C0416:29441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.259579] [ip-0A0C041B:60384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.260932] [ip-0A0C0456:29980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.262060] [ip-0A0C0475:8703 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.263159] [ip-0A0C040C:54671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.267218] [ip-0A0C0407:47184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.268272] [ip-0A0C0423:36366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.269067] [ip-0A0C0409:1250 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.271471] [ip-0A0C0417:72934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.272147] [ip-0A0C041E:42140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.276776] [ip-0A0C0456:29977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.276690] [ip-0A0C0421:44209:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.277607] [ip-0A0C042C:35557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.279075] [ip-0A0C043D:18830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.281022] [ip-0A0C0422:62719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.281604] [ip-0A0C0481:2331 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.282091] [ip-0A0C0408:66520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.282822] [ip-0A0C040A:44009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.282932] [ip-0A0C040A:44011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.284876] [ip-0A0C0463:16938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.285037] [ip-0A0C0409:1248 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.285290] [ip-0A0C0439:19800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.285982] [ip-0A0C0465:14463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286388] [ip-0A0C0453:28010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286610] [ip-0A0C0518:40932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286670] [ip-0A0C042C:35556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286498] [ip-0A0C0421:44211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286766] [ip-0A0C0474:8675 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286393] [ip-0A0C043A:27681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.286928] [ip-0A0C0465:14464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.287831] [ip-0A0C0417:72935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.288358] [ip-0A0C0470:7628 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.290368] [ip-0A0C0445:32949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.290780] [ip-0A0C0450:95750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.290952] [ip-0A0C0458:30366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.290996] [ip-0A0C0458:30367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.291425] [ip-0A0C042E:41409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.292742] [ip-0A0C045D:13993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.293145] [ip-0A0C040C:54666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.293495] [ip-0A0C0458:30365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.296038] [ip-0A0C043F:23552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.297820] [ip-0A0C0447:29621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.298275] [ip-0A0C042C:35554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.299811] [ip-0A0C0445:32953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.300879] [ip-0A0C045E:23768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.301742] [ip-0A0C0465:14459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.301918] [ip-0A0C041C:48124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.302685] [ip-0A0C045E:23767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.302814] [ip-0A0C047A:9490 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.303237] [ip-0A0C041A:65695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.303267] [ip-0A0C0434:31026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.304706] [ip-0A0C045B:30265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.304860] [ip-0A0C0409:1253 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.304714] [ip-0A0C045B:30266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.305264] [ip-0A0C0470:7630 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.305767] [ip-0A0C0465:14460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.305737] [ip-0A0C043B:27180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.307863] [ip-0A0C047F:7749 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.307458] [ip-0A0C043A:27678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.307764] [ip-0A0C040A:44018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.307816] [ip-0A0C040A:44012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.308473] [ip-0A0C041D:39196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.309290] [ip-0A0C042C:35558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.309805] [ip-0A0C042C:35555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.311322] [ip-0A0C041D:39200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.311389] [ip-0A0C0431:32986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.314770] [ip-0A0C041C:48127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.315166] [ip-0A0C0454:26968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.316004] [ip-0A0C042F:37438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.316989] [ip-0A0C042E:41412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.317294] [ip-0A0C0428:31329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.317732] [ip-0A0C041D:39197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.317578] [ip-0A0C0409:1247 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.319068] [ip-0A0C0409:1279 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.320115] [ip-0A0C041F:46095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.320462] [ip-0A0C045E:23769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.320298] [ip-0A0C0453:28000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.320311] [ip-0A0C0475:8693 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.320412] [ip-0A0C0475:8698 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.321980] [ip-0A0C045E:23765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.321816] [ip-0A0C0416:29445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.322243] [ip-0A0C045E:23766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.322443] [ip-0A0C047A:9493 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.322899] [ip-0A0C041B:60383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.323644] [ip-0A0C0477:9252 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.324129] [ip-0A0C0442:22334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.324161] [ip-0A0C0433:36300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.325706] [ip-0A0C0480:3576 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.326191] [ip-0A0C042A:33059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.326234] [ip-0A0C0412:21841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.327213] [ip-0A0C0407:47189:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.328967] [ip-0A0C041F:46101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.329511] [ip-0A0C0447:29623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.329796] [ip-0A0C047B:11081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.329781] [ip-0A0C047B:11093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.331024] [ip-0A0C0477:9248 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.330995] [ip-0A0C0459:22853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.331264] [ip-0A0C043B:27177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.332637] [ip-0A0C0463:16945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.332966] [ip-0A0C0451:24913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.333419] [ip-0A0C042E:41413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.334800] [ip-0A0C0420:43515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.334821] [ip-0A0C0420:43513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.335626] [ip-0A0C0440:21878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.335167] [ip-0A0C045C:23356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.335718] [ip-0A0C044A:21790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.336226] [ip-0A0C0451:24910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.336255] [ip-0A0C0442:22335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.336981] [ip-0A0C044C:24684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.337587] [ip-0A0C0421:44213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.337877] [ip-0A0C0417:72938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.338256] [ip-0A0C043B:27181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.338626] [ip-0A0C0463:16939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.338625] [ip-0A0C0442:22333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.339097] [ip-0A0C041A:65690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.340167] [ip-0A0C0479:5021 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.339924] [ip-0A0C0422:62726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.340508] [ip-0A0C0481:2328 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.340769] [ip-0A0C0412:21837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.341673] [ip-0A0C0426:40811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.341873] [ip-0A0C043F:23561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.342422] [ip-0A0C0518:40933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.342735] [ip-0A0C0423:36367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.342486] [ip-0A0C047C:14433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.344907] [ip-0A0C0432:58350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345531] [ip-0A0C0450:95746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345361] [ip-0A0C0447:29618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345639] [ip-0A0C0428:31331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345678] [ip-0A0C0434:31028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345885] [ip-0A0C0446:30263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.346187] [ip-0A0C0450:95748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.346088] [ip-0A0C045A:26358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.345885] [ip-0A0C0446:30265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.346078] [ip-0A0C0448:34107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.346355] [ip-0A0C043F:23551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.347379] [ip-0A0C041F:46094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.347536] [ip-0A0C0446:30266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.347949] [ip-0A0C0416:29440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.348173] [ip-0A0C043C:28446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.348705] [ip-0A0C0407:47187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.348186] [ip-0A0C043C:28441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.348977] [ip-0A0C0408:66522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.348816] [ip-0A0C0458:30363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.350453] [ip-0A0C043D:18829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.350842] [ip-0A0C0450:95753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.351893] [ip-0A0C0480:3583 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.351816] [ip-0A0C047F:7742 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.353494] [ip-0A0C0450:95747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.354558] [ip-0A0C0474:8678 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.354130] [ip-0A0C042F:37434:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.354895] [ip-0A0C043B:27183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.356722] [ip-0A0C0411:41923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.357933] [ip-0A0C0440:21882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.358245] [ip-0A0C043D:18851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.357849] [ip-0A0C045C:23355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.358095] [ip-0A0C0433:36297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.358532] [ip-0A0C043B:27179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.358701] [ip-0A0C044A:21784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.359008] [ip-0A0C043D:18827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.359149] [ip-0A0C0463:16943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.359442] [ip-0A0C045D:13995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.360518] [ip-0A0C043C:28438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.362169] [ip-0A0C045B:30268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.362859] [ip-0A0C0419:46056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.362919] [ip-0A0C045A:26354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.363095] [ip-0A0C0442:22331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.363969] [ip-0A0C046D:11366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.364236] [ip-0A0C0457:22878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.364577] [ip-0A0C046D:11368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.365713] [ip-0A0C0439:19799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.366619] [ip-0A0C0476:2626 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.368831] [ip-0A0C0439:19801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.368880] [ip-0A0C042A:33063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.369607] [ip-0A0C0518:40931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.369768] [ip-0A0C0425:45680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.370004] [ip-0A0C0462:53729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.370187] [ip-0A0C044F:20462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.370420] [ip-0A0C0418:38869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.371061] [ip-0A0C0442:22329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.371893] [ip-0A0C0456:29976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.372338] [ip-0A0C0418:38874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.372998] [ip-0A0C0518:40954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.373868] [ip-0A0C0518:40935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.373614] [ip-0A0C040C:54670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.373772] [ip-0A0C040C:54665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.374175] [ip-0A0C044A:21789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.374240] [ip-0A0C046D:11367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.374921] [ip-0A0C041C:48128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.375566] [ip-0A0C044F:20466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.375894] [ip-0A0C0416:29439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.376192] [ip-0A0C041B:60382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.376463] [ip-0A0C041B:60385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.376817] [ip-0A0C0457:22877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.377355] [ip-0A0C0432:58353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.378385] [ip-0A0C0518:40937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.379023] [ip-0A0C040C:54668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.379200] [ip-0A0C040C:54667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.379856] [ip-0A0C044C:24689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.380402] [ip-0A0C041B:60386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.380031] [ip-0A0C0469:12184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.379995] [ip-0A0C047D:5243 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.380507] [ip-0A0C046D:11369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.380017] [ip-0A0C047D:5248 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.381004] [ip-0A0C0467:12562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.382279] [ip-0A0C0471:11831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.382208] [ip-0A0C042B:40139:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.382495] [ip-0A0C041F:46100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.383056] [ip-0A0C041F:46096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.383481] [ip-0A0C041F:46097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.383334] [ip-0A0C0413:74240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.383348] [ip-0A0C0413:74236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.382849] [ip-0A0C0432:58352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.385025] [ip-0A0C043D:18839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.384913] [ip-0A0C041C:48129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.385911] [ip-0A0C0431:32983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.385478] [ip-0A0C043A:27683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.386363] [ip-0A0C041A:65692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.386475] [ip-0A0C047F:7747 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.386692] [ip-0A0C0476:2630 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.386959] [ip-0A0C041C:48122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.387720] [ip-0A0C0411:41920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.387537] [ip-0A0C042F:37437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.388150] [ip-0A0C0454:26969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.388716] [ip-0A0C0455:21526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.389190] [ip-0A0C0479:5004 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.389354] [ip-0A0C0426:40815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.389898] [ip-0A0C041C:48125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.390029] [ip-0A0C043E:16129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.391955] [ip-0A0C044F:20463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.393368] [ip-0A0C0479:5003 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.393588] [ip-0A0C044B:25891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.393949] [ip-0A0C0426:40809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.393790] [ip-0A0C0475:8695 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.395016] [ip-0A0C041B:60380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.395510] [ip-0A0C0479:5000 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.395848] [ip-0A0C0433:36298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.396424] [ip-0A0C047F:7744 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.396777] [ip-0A0C0423:36364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.396452] [ip-0A0C0476:2627 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.399232] [ip-0A0C0430:66291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.400519] [ip-0A0C0414:59056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.401162] [ip-0A0C0433:36304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.402993] [ip-0A0C0430:66288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.403429] [ip-0A0C0453:28003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.403933] [ip-0A0C0462:53733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.403921] [ip-0A0C0458:30361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.404221] [ip-0A0C045D:13997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.404929] [ip-0A0C041A:65691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.404803] [ip-0A0C0416:29438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.405132] [ip-0A0C0432:58347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.406156] [ip-0A0C044F:20468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.406783] [ip-0A0C047A:9492 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.406994] [ip-0A0C0479:5006 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.406962] [ip-0A0C0432:58348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.407316] [ip-0A0C0452:26096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.409029] [ip-0A0C0427:39178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.410157] [ip-0A0C043D:18833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.410227] [ip-0A0C043D:18828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.411033] [ip-0A0C0419:46058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.411688] [ip-0A0C0481:2330 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.412057] [ip-0A0C041E:42136:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.413072] [ip-0A0C0434:31053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.413743] [ip-0A0C0423:36368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.413814] [ip-0A0C0423:36361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.414023] [ip-0A0C0440:21880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.414027] [ip-0A0C044F:20467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.414948] [ip-0A0C0481:2332 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.415517] [ip-0A0C045B:30269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.415866] [ip-0A0C0459:22848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416413] [ip-0A0C0454:26964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416360] [ip-0A0C047F:7745 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416413] [ip-0A0C0477:9247 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416863] [ip-0A0C0420:43519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416839] [ip-0A0C044F:20464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.417034] [ip-0A0C0420:43518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.416770] [ip-0A0C0459:22851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.417124] [ip-0A0C0452:26100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.417567] [ip-0A0C042A:33061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.417994] [ip-0A0C0474:8676 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.417972] [ip-0A0C0475:8701 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.418505] [ip-0A0C0423:36363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.419250] [ip-0A0C044C:24691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.419833] [ip-0A0C045A:26357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.420147] [ip-0A0C0408:66518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.420337] [ip-0A0C043A:27682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.420788] [ip-0A0C0476:2656 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.421608] [ip-0A0C041D:39195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.421430] [ip-0A0C0476:2632 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.421802] [ip-0A0C0476:2629 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.422130] [ip-0A0C041E:42138:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.422496] [ip-0A0C0419:46057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.423793] [ip-0A0C0456:29998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.423671] [ip-0A0C0419:46054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.423361] [ip-0A0C0422:62722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.423925] [ip-0A0C0439:19804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.424217] [ip-0A0C0448:34100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.423954] [ip-0A0C045C:23361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.424606] [ip-0A0C0411:41922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.424984] [ip-0A0C045A:26360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.424607] [ip-0A0C043A:27679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.425712] [ip-0A0C0440:21881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.427277] [ip-0A0C0419:46053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.428013] [ip-0A0C0407:47185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.428236] [ip-0A0C0475:8723 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.429440] [ip-0A0C0456:29981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.430143] [ip-0A0C0456:29983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.429943] [ip-0A0C045A:26356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.430441] [ip-0A0C045A:26355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.430871] [ip-0A0C0453:28004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.430438] [ip-0A0C0417:72933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.430532] [ip-0A0C0417:72937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.431083] [ip-0A0C0445:32951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.431264] [ip-0A0C0445:32952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.431315] [ip-0A0C047F:7746 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.431281] [ip-0A0C042B:40144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.433219] [ip-0A0C0470:7631 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.434407] [ip-0A0C0440:21877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.434552] [ip-0A0C0408:66521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.434821] [ip-0A0C0440:21884:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.434817] [ip-0A0C0425:45682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.435242] [ip-0A0C047F:7743 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.435570] [ip-0A0C0419:46051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.435393] [ip-0A0C043C:28439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.435763] [ip-0A0C0426:40812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.436126] [ip-0A0C0426:40813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.436026] [ip-0A0C042E:41411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.435970] [ip-0A0C042F:37433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.437661] [ip-0A0C0463:16942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.437602] [ip-0A0C0412:21840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.438218] [ip-0A0C0475:8697 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.438376] [ip-0A0C047C:14432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.438542] [ip-0A0C047C:14436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.438624] [ip-0A0C0481:2326 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.439238] [ip-0A0C0426:40838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.438966] [ip-0A0C0421:44207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.439935] [ip-0A0C044A:21787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.439473] [ip-0A0C043A:27680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.440350] [ip-0A0C0458:30368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.441223] [ip-0A0C045D:13998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.441234] [ip-0A0C0470:7632 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.442125] [ip-0A0C041E:42139:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.442707] [ip-0A0C0456:29978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.443137] [ip-0A0C041D:39202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.443360] [ip-0A0C041D:39199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.443215] [ip-0A0C0462:53730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.443386] [ip-0A0C0475:8696 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.446806] [ip-0A0C041D:39201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.446529] [ip-0A0C0458:30362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.446946] [ip-0A0C0434:31024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.447314] [ip-0A0C041E:42142:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.448396] [ip-0A0C043A:27685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.448809] [ip-0A0C0458:30364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.450267] [ip-0A0C043F:23555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.450644] [ip-0A0C046A:17495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.451223] [ip-0A0C0446:30264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.450643] [ip-0A0C046A:17494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.451624] [ip-0A0C0431:32990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.452421] [ip-0A0C0456:29979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.452377] [ip-0A0C0445:32956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.452423] [ip-0A0C042E:41414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.452673] [ip-0A0C0439:19803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.453216] [ip-0A0C0439:19798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454024] [ip-0A0C0446:30262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454441] [ip-0A0C0416:29443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454515] [ip-0A0C042B:40138:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454183] [ip-0A0C042F:37440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454698] [ip-0A0C0416:29444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.454987] [ip-0A0C0463:16937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.455211] [ip-0A0C045B:30270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.455237] [ip-0A0C042F:37439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.455722] [ip-0A0C0422:62723:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456420] [ip-0A0C045D:13991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456503] [ip-0A0C044A:21788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456670] [ip-0A0C044A:21785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456540] [ip-0A0C0430:66292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456515] [ip-0A0C0481:2325 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456913] [ip-0A0C0417:72940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.456881] [ip-0A0C042F:37436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.458251] [ip-0A0C0425:45681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.458223] [ip-0A0C0447:29625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.458489] [ip-0A0C0447:29622:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.459794] [ip-0A0C0463:16941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.460040] [ip-0A0C0416:29442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.460856] [ip-0A0C0471:11833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.460915] [ip-0A0C0474:8673 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.461126] [ip-0A0C0421:44212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.461423] [ip-0A0C044A:21786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.463513] [ip-0A0C0408:66519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.464466] [ip-0A0C0471:11832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.464321] [ip-0A0C0422:62725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.464543] [ip-0A0C0421:44208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.465527] [ip-0A0C0408:66523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.466262] [ip-0A0C045D:13996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.466275] [ip-0A0C0439:19797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.467100] [ip-0A0C0414:59062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.467272] [ip-0A0C0477:9250 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.467452] [ip-0A0C047A:9491 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.467157] [ip-0A0C0422:62720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.467535] [ip-0A0C047B:11082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.468300] [ip-0A0C0469:12187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.468861] [ip-0A0C0481:2327 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.469493] [ip-0A0C045D:13992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.469783] [ip-0A0C045B:30263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.469826] [ip-0A0C041E:42143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.469940] [ip-0A0C0453:27999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470023] [ip-0A0C0474:8674 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.469984] [ip-0A0C0424:31346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470522] [ip-0A0C0481:2329 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470460] [ip-0A0C0422:62724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470785] [ip-0A0C0451:24911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470913] [ip-0A0C0463:16940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.471224] [ip-0A0C0420:43511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.470851] [ip-0A0C0451:24907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.471372] [ip-0A0C0407:47183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.471499] [ip-0A0C0407:47188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.471392] [ip-0A0C0417:72939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.472399] [ip-0A0C042A:33065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.472771] [ip-0A0C0417:72936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.473483] [ip-0A0C044E:19462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.474154] [ip-0A0C0427:39181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.474975] [ip-0A0C0421:44210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475180] [ip-0A0C0439:19827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475130] [ip-0A0C0447:29617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475378] [ip-0A0C041E:42141:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475431] [ip-0A0C0414:59059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475493] [ip-0A0C0421:44214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475927] [ip-0A0C045B:30264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.475912] [ip-0A0C0454:26966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.476004] [ip-0A0C045B:30267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.478171] [ip-0A0C0470:7626 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.478498] [ip-0A0C041E:42137:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.478860] [ip-0A0C047A:9494 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.478965] [ip-0A0C047A:9496 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.480507] [ip-0A0C0418:38871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.481375] [ip-0A0C0474:8682 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.481840] [ip-0A0C0411:41924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.481788] [ip-0A0C0434:31025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.482633] [ip-0A0C042B:40137:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483119] [ip-0A0C0454:26962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.482904] [ip-0A0C0455:21529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483174] [ip-0A0C0408:66524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483307] [ip-0A0C0431:32987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483465] [ip-0A0C0408:66517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483557] [ip-0A0C0447:29619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483733] [ip-0A0C0455:21530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.483989] [ip-0A0C044C:24685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.484575] [ip-0A0C0428:31333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.484632] [ip-0A0C0428:31326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.485020] [ip-0A0C0431:32985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.485134] [ip-0A0C0459:22846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.486004] [ip-0A0C0480:3582 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.486328] [ip-0A0C0480:3577 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.486859] [ip-0A0C0448:34104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.487053] [ip-0A0C0407:47190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.487185] [ip-0A0C0446:30268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.487597] [ip-0A0C0433:36299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.488121] [ip-0A0C0407:47186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.488139] [ip-0A0C0446:30261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489031] [ip-0A0C0477:9245 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489297] [ip-0A0C043F:23558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489424] [ip-0A0C044B:25894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489464] [ip-0A0C043F:23559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489489] [ip-0A0C044E:19461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.489921] [ip-0A0C043F:23557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.490083] [ip-0A0C0453:28002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.490607] [ip-0A0C0453:27997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.490632] [ip-0A0C042E:41408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.491020] [ip-0A0C0446:30267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.491223] [ip-0A0C047B:11083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.491114] [ip-0A0C045C:23358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.491739] [ip-0A0C042E:41410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.491747] [ip-0A0C042E:41438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.492390] [ip-0A0C044B:25893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.493831] [ip-0A0C042A:33062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494217] [ip-0A0C0453:27998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494466] [ip-0A0C0420:43512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494312] [ip-0A0C0412:21838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494541] [ip-0A0C043E:16123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494502] [ip-0A0C0413:74237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.494936] [ip-0A0C044C:24688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.495430] [ip-0A0C0457:22872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.495840] [ip-0A0C0424:31348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.496007] [ip-0A0C0434:31029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.496897] [ip-0A0C0470:7625 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.497587] [ip-0A0C047B:11077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.497855] [ip-0A0C0447:29620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.498181] [ip-0A0C0424:31342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.499269] [ip-0A0C0445:32954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.499365] [ip-0A0C0445:32950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.499776] [ip-0A0C0434:31027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.500036] [ip-0A0C0474:8677 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.500205] [ip-0A0C0471:11836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.500315] [ip-0A0C0470:7629 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.500949] [ip-0A0C0452:26097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502082] [ip-0A0C0474:8679 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502015] [ip-0A0C043F:23553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502209] [ip-0A0C0459:22847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502450] [ip-0A0C043E:16128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502653] [ip-0A0C0445:32965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502938] [ip-0A0C043E:16125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.502952] [ip-0A0C0477:9246 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.503874] [ip-0A0C0420:43514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.503748] [ip-0A0C0448:34103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.504370] [ip-0A0C0428:31332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.505310] [ip-0A0C0420:43516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.505399] [ip-0A0C047C:14437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.505555] [ip-0A0C047B:11079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.505935] [ip-0A0C0451:24931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.506504] [ip-0A0C041A:65693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.507241] [ip-0A0C041A:65688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.507149] [ip-0A0C047A:9495 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.507896] [ip-0A0C042A:33064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.507868] [ip-0A0C0434:31030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.509970] [ip-0A0C047A:9497 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.510093] [ip-0A0C0477:9251 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.510245] [ip-0A0C0418:38870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.511162] [ip-0A0C0477:9249 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.511698] [ip-0A0C0433:36301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.512105] [ip-0A0C0470:7627 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.512480] [ip-0A0C0480:3578 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.512985] [ip-0A0C047B:11080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.513203] [ip-0A0C0454:26963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.513028] [ip-0A0C0459:22852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.513221] [ip-0A0C043C:28440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.513172] [ip-0A0C0413:74239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.513935] [ip-0A0C0454:26967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.514479] [ip-0A0C042A:33058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.514421] [ip-0A0C043C:28444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.514642] [ip-0A0C0454:26965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515094] [ip-0A0C041A:65694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515175] [ip-0A0C041A:65689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.514987] [ip-0A0C0467:12560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515015] [ip-0A0C0412:21842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515378] [ip-0A0C043C:28443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515664] [ip-0A0C0425:45679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.515655] [ip-0A0C0469:12185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.516510] [ip-0A0C0412:21836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.516593] [ip-0A0C0467:12559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.516921] [ip-0A0C0480:3579 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.516924] [ip-0A0C0433:36314:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.517505] [ip-0A0C0457:22871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.517841] [ip-0A0C0412:21839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.520220] [ip-0A0C044C:24690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.521423] [ip-0A0C044B:25896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.521503] [ip-0A0C043C:28445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.521856] [ip-0A0C0428:31328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.522423] [ip-0A0C047C:14435:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.522671] [ip-0A0C0451:24908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.523449] [ip-0A0C042A:33060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.525542] [ip-0A0C047C:14431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.526386] [ip-0A0C045C:23360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.527487] [ip-0A0C047B:11084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.528035] [ip-0A0C044E:19459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.528464] [ip-0A0C0427:39176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.528928] [ip-0A0C0452:26101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.528953] [ip-0A0C0459:22849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.530120] [ip-0A0C0433:36303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.530794] [ip-0A0C0462:53732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.530891] [ip-0A0C0462:53734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.531222] [ip-0A0C0451:24914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.531290] [ip-0A0C0459:22850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.531292] [ip-0A0C0451:24909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.532772] [ip-0A0C0428:31327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.533317] [ip-0A0C0428:31330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.533547] [ip-0A0C0431:32989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.533421] [ip-0A0C0412:21835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.534090] [ip-0A0C0431:32984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.535262] [ip-0A0C0431:32988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.535247] [ip-0A0C0457:22876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.537150] [ip-0A0C0448:34117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.537305] [ip-0A0C0448:34105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.537820] [ip-0A0C0411:41925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.537596] [ip-0A0C0413:74238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.538001] [ip-0A0C045C:23354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.538538] [ip-0A0C0427:39177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539153] [ip-0A0C043E:16124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539248] [ip-0A0C0457:22873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539592] [ip-0A0C0425:45678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539557] [ip-0A0C0480:3581 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539835] [ip-0A0C0480:3580 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.539992] [ip-0A0C045C:23357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.541003] [ip-0A0C044C:24686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.543587] [ip-0A0C045C:23359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.544129] [ip-0A0C0418:38873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.544697] [ip-0A0C044C:24687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.547263] [ip-0A0C0455:21527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.547842] [ip-0A0C0467:12564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.548377] [ip-0A0C0462:53731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.549449] [ip-0A0C047D:5245 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.550656] [ip-0A0C0457:22875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.551048] [ip-0A0C0425:45676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.551373] [ip-0A0C0448:34102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.552443] [ip-0A0C047D:5242 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.552993] [ip-0A0C0471:11834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.554032] [ip-0A0C043E:16122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.554964] [ip-0A0C042B:40143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.555533] [ip-0A0C0448:34101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.555930] [ip-0A0C0457:22874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.556668] [ip-0A0C0452:26106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.556769] [ip-0A0C0418:38872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.556946] [ip-0A0C0411:41926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.557061] [ip-0A0C0411:41919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.557217] [ip-0A0C0411:41921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.557336] [ip-0A0C047C:14454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.557429] [ip-0A0C0469:12188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.558892] [ip-0A0C0413:74234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.560829] [ip-0A0C0462:53750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.561623] [ip-0A0C0462:53728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.561594] [ip-0A0C0430:66294:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.561825] [ip-0A0C0469:12183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.562043] [ip-0A0C047C:14434:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.562380] [ip-0A0C0418:38875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.562167] [ip-0A0C047D:5247 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.562545] [ip-0A0C0418:38876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.567712] [ip-0A0C0413:74235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.568027] [ip-0A0C0413:74241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.568605] [ip-0A0C0427:39175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.568843] [ip-0A0C044B:25897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.569210] [ip-0A0C0469:12182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.572367] [ip-0A0C047D:5250 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.573128] [ip-0A0C047D:5246 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.574324] [ip-0A0C0455:21524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.575085] [ip-0A0C0425:45677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.575037] [ip-0A0C047D:5244 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.577271] [ip-0A0C0425:45675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.577269] [ip-0A0C0455:21525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.578341] [ip-0A0C044B:25895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.580710] [ip-0A0C0471:11845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.582120] [ip-0A0C043E:16126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.582270] [ip-0A0C043E:16127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.582496] [ip-0A0C0430:66293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.582846] [ip-0A0C0467:12558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.586564] [ip-0A0C0427:39179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.588945] [ip-0A0C0452:26098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.588980] [ip-0A0C0452:26095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.590427] [ip-0A0C042B:40141:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.590586] [ip-0A0C042B:40140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.592468] [ip-0A0C0414:59061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.594600] [ip-0A0C042B:40142:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.594270] [ip-0A0C046A:17497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.594943] [ip-0A0C046A:17499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.595075] [ip-0A0C0469:12180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.595446] [ip-0A0C0467:12563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.596237] [ip-0A0C0471:11837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.596849] [ip-0A0C044B:25890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.597418] [ip-0A0C0452:26094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.597665] [ip-0A0C0471:11835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.597647] [ip-0A0C0414:59057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.597812] [ip-0A0C0430:66287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.598459] [ip-0A0C044B:25892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.599829] [ip-0A0C0430:66289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.599662] [ip-0A0C0469:12181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.600832] [ip-0A0C0430:66290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.601894] [ip-0A0C0455:21523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.601972] [ip-0A0C0455:21528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.602723] [ip-0A0C0467:12561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.602957] [ip-0A0C0467:12565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.605045] [ip-0A0C0414:59058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.611742] [ip-0A0C0427:39182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.611808] [ip-0A0C0427:39180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.612278] [ip-0A0C046A:17498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.615020] [ip-0A0C0424:31343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.615467] [ip-0A0C0424:31344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.626405] [ip-0A0C0414:59063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.628065] [ip-0A0C044E:19456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.629463] [ip-0A0C0414:59060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.634146] [ip-0A0C046A:17496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.635674] [ip-0A0C046A:17492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.639735] [ip-0A0C046A:17493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.641571] [ip-0A0C0424:31345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.645804] [ip-0A0C0424:31347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.645900] [ip-0A0C0424:31341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.659121] [ip-0A0C044E:19463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.674790] [ip-0A0C044E:19457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.675077] [ip-0A0C044E:19460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620835.676303] [ip-0A0C044E:19458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634620836574, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634620836613, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634620836613, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634620836614, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620836614, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634620836614, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634620836614, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:47] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:48] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:20:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:1247 - context.c:584] INFO job (ID: 867537832088428160) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1247 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1247 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1246 - context.c:584] INFO job (ID: 867538379228781884) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1246 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1246 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1250 - context.c:584] INFO job (ID: 867537893815680628) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1250 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1250 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1249 - context.c:584] INFO job (ID: 867538478472782664) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1249 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1249 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1253 - context.c:584] INFO job (ID: 867538200328006837) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1253 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1253 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1248 - context.c:584] INFO job (ID: 867538450271769597) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1248 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1248 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1279 - context.c:584] INFO job (ID: 867538302600960659) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1279 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1279 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:1251 - context.c:584] INFO job (ID: 867538859391918314) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:1251 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:1251 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1628481196, "metadata": {"file": "main.py", "lineno": 72}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931475, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620931476, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:22:13] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620955440, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634620955477, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620955481, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634620955481, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620958104, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634620958105, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634620958105, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620958105, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620959579, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2280.2625086402622, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620959579, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620959579, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2280.2625086402622, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634620959579, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620959579, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620960248, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5024.026454737865, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620960249, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620960249, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5024.026454737865, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620960249, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620960249, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620960903, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5135.78325199987, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620960904, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620960904, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5135.78325199987, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634620960904, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620960904, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620961545, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5244.157873291297, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620961545, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620961545, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5244.157873291297, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634620961546, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620961546, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620962180, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5301.580009013489, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620962180, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620962180, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5301.580009013489, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634620962180, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620962180, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620962808, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5355.686131589358, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620962808, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620962808, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5355.686131589358, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634620962809, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620962809, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620963441, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.792021082708, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620963441, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620963441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.792021082708, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620963441, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620963441, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620964080, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5258.691288315538, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620964081, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620964081, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5258.691288315538, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634620964081, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620964081, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620964704, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5399.131425874327, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620964704, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620964704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5399.131425874327, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634620964704, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620964704, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620965328, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5390.278447547826, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620965328, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620965328, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5390.278447547826, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634620965328, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620965329, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620965952, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.091144310293, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620965952, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620965952, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.091144310293, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634620965952, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620965952, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620966577, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5380.026646530339, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620966577, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620966577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5380.026646530339, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634620966578, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620966578, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620967206, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5352.895138362665, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620967206, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620967206, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5352.895138362665, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634620967206, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620967206, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620967825, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.454429747999, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620967825, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620967825, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.454429747999, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634620967825, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620967825, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620968451, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5375.188013285347, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620968451, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620968451, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5375.188013285347, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634620968451, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620968451, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620969069, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5440.958835091345, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620969069, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620969069, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5440.958835091345, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634620969070, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620969070, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620969687, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.458650991046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620969687, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620969687, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.458650991046, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634620969688, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620969688, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620970310, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5402.637684659662, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620970310, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620970310, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5402.637684659662, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634620970310, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620970310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620970933, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5397.90717101763, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620970933, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620970933, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5397.90717101763, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634620970934, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620970934, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620971557, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5392.821709869998, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620971557, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620971557, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5392.821709869998, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634620971557, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620971558, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620972178, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5414.005266154393, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620972179, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620972179, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5414.005266154393, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634620972179, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620972179, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620972800, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.324627607286, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620972801, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620972801, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.324627607286, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634620972801, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620972801, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620973423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.906666032931, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620973423, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620973424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.906666032931, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634620973424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620973424, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620974046, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.5006171198465, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620974046, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620974047, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.5006171198465, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634620974047, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620974047, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620974672, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5373.927459464448, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620974672, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620974672, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5373.927459464448, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634620974673, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620974673, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620975297, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.591078705843, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620975297, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620975297, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.591078705843, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634620975297, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620975298, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620975916, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.497227955162, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620975916, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620975917, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.497227955162, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634620975917, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620975917, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620976540, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5390.878465919849, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620976540, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620976541, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5390.878465919849, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634620976541, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620976541, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620977157, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.079160775519, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620977157, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620977158, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.079160775519, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634620977158, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620977158, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620977778, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5415.954814974515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620977779, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620977779, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5415.954814974515, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634620977779, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620977779, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620978403, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5383.1523741604615, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620978404, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620978404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5383.1523741604615, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634620978404, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620978404, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620979022, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5437.839073416759, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620979022, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620979022, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5437.839073416759, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634620979022, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620979023, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620979644, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.247862959278, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620979644, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620979644, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.247862959278, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634620979645, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620979645, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620980261, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.918902440912, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620980261, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620980261, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.918902440912, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634620980262, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620980262, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620980880, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5436.20923574471, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620980880, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620980880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5436.20923574471, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634620980880, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620980880, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620981500, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.633856837672, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620981500, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620981501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.633856837672, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634620981501, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620981501, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620982119, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.148334993355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620982120, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620982120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.148334993355, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634620982120, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620982120, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620982752, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5320.446086773995, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620982752, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620982752, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5320.446086773995, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634620982752, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620982753, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620983380, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5358.861064466671, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620983380, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620983380, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5358.861064466671, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634620983380, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620983380, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620984005, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5375.887209397691, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620984006, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620984006, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5375.887209397691, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634620984006, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620984006, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620984623, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5449.814821259939, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620984623, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620984623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5449.814821259939, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634620984623, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620984623, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620985242, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.00115149301, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620985243, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620985243, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.00115149301, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634620985243, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620985243, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620985865, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.47816344228, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620985865, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620985865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.47816344228, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634620985865, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620985865, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620986490, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5379.808946764067, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620986490, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620986491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5379.808946764067, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634620986491, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620986491, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620987114, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.926318623162, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620987115, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620987115, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.926318623162, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634620987115, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620987115, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620987737, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5403.808140493491, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620987737, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620987737, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5403.808140493491, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634620987738, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620987738, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620988362, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.2518909484625, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620988362, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620988363, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.2518909484625, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634620988363, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620988363, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620988981, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.186408346512, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620988981, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620988981, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.186408346512, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634620988982, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620988982, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620989609, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5356.215365074153, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620989609, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620989609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5356.215365074153, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634620989610, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620989610, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620990228, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.2041760524435, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620990229, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620990229, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.2041760524435, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634620990296, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620990296, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620990313, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620990740, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8830412030220032, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620990740, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620990912, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.416698262855, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620990913, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620990913, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.416698262855, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620990992, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620990993, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620990994, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620991515, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8887677192687988, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620991515, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620991751, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4440.108960645749, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620991751, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620991751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4440.108960645749, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620991791, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620991791, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620991809, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620992240, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8756747245788574, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620992240, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620992435, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5214.7556278508355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620992436, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620992436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5214.7556278508355, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620992510, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620992511, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620992526, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620992940, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8676791191101074, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620992940, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620993143, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.459354421629, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620993143, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620993143, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.459354421629, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620993178, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620993178, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620993195, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620993593, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8983275890350342, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620993593, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620993791, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5488.98586941281, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620993791, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620993791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5488.98586941281, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620993827, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620993827, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620993844, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620994242, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9060630798339844, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620994242, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620994437, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5516.192134309425, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620994437, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620994437, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5516.192134309425, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620994473, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620994473, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620994490, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620994899, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8690955638885498, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620994899, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620995094, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5408.839619209034, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620995094, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620995095, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5408.839619209034, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620995130, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620995130, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620995149, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620995546, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8999006748199463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620995546, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620995742, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.805644750563, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620995742, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620995742, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.805644750563, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620995776, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620995777, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620995794, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620996193, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8860452771186829, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620996193, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620996395, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.081332852293, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620996395, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620996396, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.081332852293, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620996435, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620996435, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620996452, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620996857, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8907773494720459, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620996857, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620997054, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.991301327966, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620997054, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620997054, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.991301327966, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620997089, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620997089, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620997105, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620997505, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8764071464538574, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620997505, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620997699, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5513.718876212214, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620997699, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620997699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5513.718876212214, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620997735, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620997735, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620997753, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620998150, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8830435276031494, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620998150, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620998360, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5377.5467339467605, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620998360, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620998360, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5377.5467339467605, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620998416, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620998416, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620998432, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620998830, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.883613109588623, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620998830, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620999022, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5542.925921191585, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620999023, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620999023, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5542.925921191585, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620999052, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620999052, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620999069, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620999496, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8726980686187744, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620999496, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620999689, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5274.256725931687, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620999690, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620999690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5274.256725931687, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620999725, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620999725, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620999741, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621000154, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8884927034378052, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621000155, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621000344, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.797561093949, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621000345, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621000345, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.797561093949, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621000373, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621000373, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621000391, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621000795, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8925947546958923, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621000795, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621000995, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.905619933276, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621000995, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621000995, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.905619933276, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621001031, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621001031, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621001050, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621001446, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8900555372238159, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621001446, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621001633, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5577.381814382528, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621001634, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621001634, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5577.381814382528, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621001669, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621001669, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621001688, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621002090, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.899531364440918, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621002091, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621002282, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5481.950555766257, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621002282, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621002282, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5481.950555766257, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621002318, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621002319, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621002337, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621002733, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8887994289398193, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621002733, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621002922, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5571.961541107508, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621002922, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621002922, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5571.961541107508, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621002958, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621002958, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621002977, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621003382, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8926204442977905, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621003383, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621003588, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5333.0599001645005, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621003588, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621003589, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5333.0599001645005, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621003624, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621003624, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621003642, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621004038, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8775066137313843, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621004038, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621004238, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5478.741024017205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621004238, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621004238, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5478.741024017205, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621004274, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621004275, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621004291, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621004688, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.900826096534729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621004688, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621004881, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5542.274144110769, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621004881, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621004881, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5542.274144110769, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621004918, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621004919, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621004935, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621005333, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8955709338188171, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621005333, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621005528, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5515.050188898098, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621005528, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621005528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5515.050188898098, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621005563, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621005564, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621005581, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621005982, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8957229256629944, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621005982, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621006175, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5498.8807191965225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621006175, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621006175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5498.8807191965225, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621006201, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621006202, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621006217, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621006657, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8799283504486084, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621006658, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621006852, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5171.189385252444, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621006852, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621006852, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5171.189385252444, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621006888, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621006888, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621006906, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621007303, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8830940127372742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621007303, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621007516, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5350.897257054422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621007517, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621007517, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5350.897257054422, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621007552, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621007552, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621007570, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621007966, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8975054621696472, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621007966, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621008159, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5537.974950958202, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621008159, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621008160, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5537.974950958202, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621008194, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621008195, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621008211, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621008608, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8498085141181946, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621008609, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621008847, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5151.569955739855, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621008847, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621008848, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5151.569955739855, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621008883, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621008883, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621008902, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621009297, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8969087600708008, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621009298, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621009503, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5427.1946369100615, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621009503, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621009503, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5427.1946369100615, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621009541, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621009541, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621009559, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621009955, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.89176344871521, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621009955, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621010172, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5329.264442342752, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621010172, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621010172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5329.264442342752, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621010208, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621010208, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621010227, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621010623, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9096522331237793, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621010624, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621010624, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634621010817, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5517.6456143428095, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621010818, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621010818, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5517.6456143428095, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:37 AM
RESULT,image_segmentation,,187,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:38 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:38 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:38 AM
RESULT,image_segmentation,,188,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:40 AM
RESULT,image_segmentation,,190,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:41 AM
RESULT,image_segmentation,,191,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:43 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,193,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:44 AM
RESULT,image_segmentation,,194,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:45 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:46 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:47 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:48 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:49 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
ENDING TIMING RUN AT 2021-10-19 05:23:50 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:20:30 AM
