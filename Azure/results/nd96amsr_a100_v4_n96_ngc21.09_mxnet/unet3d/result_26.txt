+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019055902210613173
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019055902210613173
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019055902210613173
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019055902210613173
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07389/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019055902210613173_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C044D
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C04BD
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C04DA
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C04C4
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C04BE
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C04CC
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:59:04 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634623149.622439] [ip-0A0C047E:44277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.646023] [ip-0A0C0466:50039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.654142] [ip-0A0C047E:44283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.667948] [ip-0A0C0497:79911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.672321] [ip-0A0C049D:30881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.691736] [ip-0A0C045F:45366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.696158] [ip-0A0C049D:30877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.699763] [ip-0A0C04BE:65439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.708929] [ip-0A0C04BE:65438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.718062] [ip-0A0C04C9:60556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.722448] [ip-0A0C046B:36986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.724429] [ip-0A0C047E:44282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.730870] [ip-0A0C04B4:41906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.730419] [ip-0A0C0497:79886:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.730474] [ip-0A0C0497:79883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.732549] [ip-0A0C04BD:62715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.732654] [ip-0A0C046B:36987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.742304] [ip-0A0C04C9:60557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.744779] [ip-0A0C0443:11015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.747281] [ip-0A0C0464:47453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.749449] [ip-0A0C0466:50042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.749568] [ip-0A0C0466:50043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.750213] [ip-0A0C049D:30879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.753528] [ip-0A0C04BD:62719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.755313] [ip-0A0C0443:11017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.755553] [ip-0A0C0495:31778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.764664] [ip-0A0C0437:43950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.767741] [ip-0A0C0461:11408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.769036] [ip-0A0C0461:11409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.770180] [ip-0A0C048A:44158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.772180] [ip-0A0C047E:44279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.772970] [ip-0A0C0437:43949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.773870] [ip-0A0C0466:50040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.774734] [ip-0A0C0464:47451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.780714] [ip-0A0C04B4:41904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.781679] [ip-0A0C0472:49394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.781860] [ip-0A0C04AD:9531 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.781865] [ip-0A0C04AD:9528 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.782322] [ip-0A0C046E:49362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.782310] [ip-0A0C046E:49361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.782992] [ip-0A0C0489:47710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.785401] [ip-0A0C04C0:64176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.786713] [ip-0A0C0464:47459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.787904] [ip-0A0C04C9:60554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.787375] [ip-0A0C0493:29065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.787502] [ip-0A0C049C:19253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.792904] [ip-0A0C040F:17486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.793534] [ip-0A0C04A9:31519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.794910] [ip-0A0C0482:35051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.795694] [ip-0A0C0495:31776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.797143] [ip-0A0C048D:33393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.796280] [ip-0A0C049C:19250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.797990] [ip-0A0C0472:49392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.798447] [ip-0A0C0498:42333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.800438] [ip-0A0C0495:31774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.802410] [ip-0A0C04CD:58111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.804233] [ip-0A0C048A:44162:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.803787] [ip-0A0C04DB:53637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.806021] [ip-0A0C045F:45364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.808864] [ip-0A0C0498:42327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.811322] [ip-0A0C04B0:18927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.813147] [ip-0A0C049D:30882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.813608] [ip-0A0C0441:40606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.815189] [ip-0A0C0482:35044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.816356] [ip-0A0C04BD:62718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.816662] [ip-0A0C045F:45391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.817146] [ip-0A0C04BA:97436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.817000] [ip-0A0C04DB:53616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.820990] [ip-0A0C047E:44281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.821539] [ip-0A0C047E:44298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.821780] [ip-0A0C0488:46145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.822942] [ip-0A0C0494:29238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.823831] [ip-0A0C04C8:61692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.823465] [ip-0A0C0488:46147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.824258] [ip-0A0C04CD:58112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.825936] [ip-0A0C0493:29066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.826351] [ip-0A0C0441:40609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.829561] [ip-0A0C047E:44280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.829575] [ip-0A0C04C5:66876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.831572] [ip-0A0C047E:44278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.832674] [ip-0A0C0466:50045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.833870] [ip-0A0C046B:36988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.837170] [ip-0A0C048D:33389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.839318] [ip-0A0C0496:14123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.837826] [ip-0A0C0497:79885:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.839107] [ip-0A0C0497:79880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.840974] [ip-0A0C04C9:60561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.842581] [ip-0A0C0466:50044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.845968] [ip-0A0C04A0:21893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.846979] [ip-0A0C04A7:9390 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.846512] [ip-0A0C0473:45896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.846873] [ip-0A0C0493:29069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.851310] [ip-0A0C04B4:41905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.851505] [ip-0A0C045F:45367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.852155] [ip-0A0C04BA:97433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.853344] [ip-0A0C0489:47716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.854948] [ip-0A0C04BE:65443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.855758] [ip-0A0C04BE:65436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.856244] [ip-0A0C0466:50046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.857893] [ip-0A0C0466:50041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.858296] [ip-0A0C0473:45898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.858164] [ip-0A0C0489:47712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.858649] [ip-0A0C04A9:31516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.859600] [ip-0A0C04AB:22862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.859586] [ip-0A0C04AB:22872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.859620] [ip-0A0C04AB:22864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.861370] [ip-0A0C0472:49397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.862280] [ip-0A0C04C8:61693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.863552] [ip-0A0C04D3:59705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.865352] [ip-0A0C048A:44161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.869232] [ip-0A0C04A0:21888:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.867946] [ip-0A0C049F:20466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.868279] [ip-0A0C045F:45363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.869363] [ip-0A0C04B1:35862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.869037] [ip-0A0C0482:35047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.870817] [ip-0A0C049D:30876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.871153] [ip-0A0C0497:79884:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.872423] [ip-0A0C04B0:18926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.872715] [ip-0A0C0497:79882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.873825] [ip-0A0C04D3:59701:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.875729] [ip-0A0C049D:30883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.879647] [ip-0A0C0496:14126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.880394] [ip-0A0C04B0:18930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.880336] [ip-0A0C0449:44946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.881501] [ip-0A0C04C5:66874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.881039] [ip-0A0C04DB:53612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.881992] [ip-0A0C049D:30880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.882243] [ip-0A0C048B:30226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.883433] [ip-0A0C0497:79881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.885060] [ip-0A0C045F:45365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.886382] [ip-0A0C049D:30878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.887324] [ip-0A0C0494:29237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.888726] [ip-0A0C045F:45368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.889527] [ip-0A0C04BE:65440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.890505] [ip-0A0C04BD:62717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.891438] [ip-0A0C0435:80416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.891854] [ip-0A0C0437:43954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.895084] [ip-0A0C0495:31775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.896184] [ip-0A0C046B:36991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.898599] [ip-0A0C0498:42322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.899039] [ip-0A0C042D:18772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.900752] [ip-0A0C0487:47328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.900737] [ip-0A0C04BA:97434:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.901066] [ip-0A0C04B4:41903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.901797] [ip-0A0C0443:11020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.901904] [ip-0A0C0443:11021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.902238] [ip-0A0C045F:45362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.902579] [ip-0A0C04C6:60482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.903366] [ip-0A0C04C0:64175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.904230] [ip-0A0C0438:48311:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.906304] [ip-0A0C04A7:9388 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.905357] [ip-0A0C04DA:54948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.906566] [ip-0A0C04B3:40178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.906616] [ip-0A0C04B3:40176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.907014] [ip-0A0C04C3:63644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.907883] [ip-0A0C04BD:62721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.908040] [ip-0A0C04C9:60560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.909304] [ip-0A0C04C9:60555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.909547] [ip-0A0C04BE:65442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.910622] [ip-0A0C040F:17489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.910188] [ip-0A0C0494:29243:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.910598] [ip-0A0C0461:11410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.911334] [ip-0A0C0473:45894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.911417] [ip-0A0C04AD:9530 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.913065] [ip-0A0C04CC:61241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.913095] [ip-0A0C04C9:60558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.913579] [ip-0A0C0491:29569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.914360] [ip-0A0C0437:43948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.914356] [ip-0A0C046E:49364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.914411] [ip-0A0C04C4:64943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.914933] [ip-0A0C0493:29071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.915481] [ip-0A0C04C9:60562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.915902] [ip-0A0C0472:49393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.915756] [ip-0A0C0449:44941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926492] [ip-0A0C04A2:23927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926537] [ip-0A0C04A2:23930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.916626] [ip-0A0C046B:36992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.917734] [ip-0A0C0490:33572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.918656] [ip-0A0C04BE:65437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.918292] [ip-0A0C0461:11406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.917310] [ip-0A0C04BC:60648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.919998] [ip-0A0C04BE:65441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.920229] [ip-0A0C046E:49369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.921490] [ip-0A0C0485:12408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.921665] [ip-0A0C0486:47344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.921536] [ip-0A0C0485:12410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.921286] [ip-0A0C04C3:63647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.921541] [ip-0A0C0464:47456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.922777] [ip-0A0C046B:36989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.922905] [ip-0A0C048D:33395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.923262] [ip-0A0C04B4:41901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.923503] [ip-0A0C04B9:65470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.923881] [ip-0A0C04BA:97431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.923838] [ip-0A0C04A9:31520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.925515] [ip-0A0C04C5:66882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926308] [ip-0A0C04CC:61246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.925189] [ip-0A0C04A8:37787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926316] [ip-0A0C04B1:35836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926425] [ip-0A0C048D:33394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926518] [ip-0A0C0443:11019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.927061] [ip-0A0C046B:36990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.928244] [ip-0A0C04BD:62714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.928489] [ip-0A0C04C0:64180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.926710] [ip-0A0C0484:49454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.929668] [ip-0A0C04CD:58115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.930560] [ip-0A0C04B1:35837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.930342] [ip-0A0C04AD:9527 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.932424] [ip-0A0C0437:43952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.932409] [ip-0A0C04A8:37793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.933227] [ip-0A0C046B:36993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.933434] [ip-0A0C0436:47706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.933880] [ip-0A0C04C8:61698:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.933680] [ip-0A0C0464:47455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.934398] [ip-0A0C046E:49363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.935390] [ip-0A0C0464:47452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.935204] [ip-0A0C049C:19254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.936716] [ip-0A0C0437:43953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.936638] [ip-0A0C0441:40603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.936964] [ip-0A0C049F:20465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.937067] [ip-0A0C049F:20471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.938626] [ip-0A0C04DA:54946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.939482] [ip-0A0C048A:44159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.939929] [ip-0A0C0461:11429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.941448] [ip-0A0C049C:19251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.942888] [ip-0A0C04B0:18928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.943514] [ip-0A0C0464:47457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.943902] [ip-0A0C04BD:62716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.943600] [ip-0A0C0464:47454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.944345] [ip-0A0C04B4:41920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.943816] [ip-0A0C04A5:35038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.944389] [ip-0A0C04AD:9533 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.944611] [ip-0A0C04BD:62720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.945667] [ip-0A0C0486:47347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.945331] [ip-0A0C04A1:50175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.945196] [ip-0A0C046E:49368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.944280] [ip-0A0C049C:19249:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947352] [ip-0A0C04A0:21895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.946543] [ip-0A0C040F:17493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.946871] [ip-0A0C040F:17491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947303] [ip-0A0C04C0:64177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947186] [ip-0A0C0472:49391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947295] [ip-0A0C04D4:57771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947282] [ip-0A0C0472:49395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948018] [ip-0A0C0492:27572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.947471] [ip-0A0C0461:11407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948601] [ip-0A0C04CD:58108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948976] [ip-0A0C04C4:64940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.949065] [ip-0A0C0495:31772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948899] [ip-0A0C0460:47078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.949115] [ip-0A0C0443:11016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948952] [ip-0A0C0460:47079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.949292] [ip-0A0C0443:11022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.948291] [ip-0A0C049B:21194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.950466] [ip-0A0C0495:31771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.950902] [ip-0A0C04B4:41902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.950722] [ip-0A0C0443:11018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.950504] [ip-0A0C0489:47714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.951383] [ip-0A0C04B4:41907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.951121] [ip-0A0C048B:30224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.951030] [ip-0A0C044D:44806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.951548] [ip-0A0C0438:48313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.951662] [ip-0A0C0441:40608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.952158] [ip-0A0C04DB:53615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.953537] [ip-0A0C04AA:7252 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.953880] [ip-0A0C04C7:62614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.954688] [ip-0A0C0461:11405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.955384] [ip-0A0C04A9:31514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.956271] [ip-0A0C04D4:57766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.956001] [ip-0A0C048B:30227:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.955974] [ip-0A0C0498:42324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.956770] [ip-0A0C042D:18774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.957004] [ip-0A0C0482:35049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.957994] [ip-0A0C04A1:50170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.957708] [ip-0A0C048F:37101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.957660] [ip-0A0C0461:11412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.958308] [ip-0A0C04B0:18929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.959905] [ip-0A0C0487:47322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.960046] [ip-0A0C040F:17490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.960319] [ip-0A0C04AA:7250 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.960732] [ip-0A0C0472:49390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.960378] [ip-0A0C049F:20470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.961690] [ip-0A0C046E:49366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.960921] [ip-0A0C04DB:53613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.962062] [ip-0A0C04D8:61210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.962302] [ip-0A0C0491:29573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.963019] [ip-0A0C0489:47717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.964109] [ip-0A0C048A:44155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.964373] [ip-0A0C0472:49396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.964127] [ip-0A0C0482:35046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.964341] [ip-0A0C0493:29064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.965317] [ip-0A0C04AB:22867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.964810] [ip-0A0C0494:29244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.965295] [ip-0A0C04A9:31515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.966711] [ip-0A0C0483:46154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.968573] [ip-0A0C048A:44156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.967402] [ip-0A0C049C:19252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.968486] [ip-0A0C04C6:60480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.969168] [ip-0A0C04AC:25912:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.970074] [ip-0A0C0496:14119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.969764] [ip-0A0C0437:43951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.969629] [ip-0A0C048C:28518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.971012] [ip-0A0C04CC:61244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.970736] [ip-0A0C048D:33392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.971863] [ip-0A0C048A:44157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.971887] [ip-0A0C04D3:59702:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.972285] [ip-0A0C0473:45897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.971976] [ip-0A0C0494:29240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973052] [ip-0A0C0429:37977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973087] [ip-0A0C0429:37973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973141] [ip-0A0C0429:37975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973414] [ip-0A0C046C:43043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.974000] [ip-0A0C0437:43955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973470] [ip-0A0C046C:43041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.973466] [ip-0A0C046C:43046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.974348] [ip-0A0C04C0:64181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.974218] [ip-0A0C0487:47323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.974960] [ip-0A0C0495:31773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.974617] [ip-0A0C04C6:60476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.975144] [ip-0A0C0495:31777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.976091] [ip-0A0C04A7:9383 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.975187] [ip-0A0C046E:49365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.975622] [ip-0A0C0449:44940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.976810] [ip-0A0C04BB:7489 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.976848] [ip-0A0C04BB:7483 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.976886] [ip-0A0C04AD:9529 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.976895] [ip-0A0C04BC:60659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.979083] [ip-0A0C04B6:1369 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.979901] [ip-0A0C04BA:97432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.981610] [ip-0A0C04A7:9384 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.981524] [ip-0A0C04C0:64179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.981025] [ip-0A0C04A5:35040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.980607] [ip-0A0C04BC:60647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.983004] [ip-0A0C04C5:66878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.983715] [ip-0A0C0492:27575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.982764] [ip-0A0C0488:46151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.983118] [ip-0A0C0499:33870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.982933] [ip-0A0C0488:46148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.984019] [ip-0A0C04AC:25886:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.983507] [ip-0A0C0498:42323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.984177] [ip-0A0C04C7:62613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985166] [ip-0A0C0498:42321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985827] [ip-0A0C04C8:61691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985755] [ip-0A0C04AD:9532 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985904] [ip-0A0C0493:29067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985299] [ip-0A0C049C:19248:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.986993] [ip-0A0C04C0:64174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.985751] [ip-0A0C049C:19255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.986922] [ip-0A0C04AD:9526 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.987746] [ip-0A0C04C2:87421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.988253] [ip-0A0C0435:80402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.988772] [ip-0A0C0460:47074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.989429] [ip-0A0C048A:44160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.989744] [ip-0A0C04A9:31521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.990872] [ip-0A0C04AB:22861:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.991656] [ip-0A0C0441:40604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.991842] [ip-0A0C04DA:54949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.992410] [ip-0A0C048D:33391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.993304] [ip-0A0C04C4:64946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.993296] [ip-0A0C04C8:61694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.994069] [ip-0A0C040F:17492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.994672] [ip-0A0C04A6:32080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.994368] [ip-0A0C04C3:63650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.993449] [ip-0A0C04DB:53611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.995487] [ip-0A0C0449:44947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.996699] [ip-0A0C04A0:21890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.996078] [ip-0A0C0436:47705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.996484] [ip-0A0C04CD:58109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.996287] [ip-0A0C0494:29242:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.996085] [ip-0A0C04DB:53614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.997357] [ip-0A0C0488:46143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.998672] [ip-0A0C0482:35048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.999410] [ip-0A0C04BA:97437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623149.999685] [ip-0A0C04C5:66875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.000541] [ip-0A0C048D:33390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.001430] [ip-0A0C0496:14124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.000956] [ip-0A0C048D:33388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.000767] [ip-0A0C0489:47715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.001479] [ip-0A0C04AE:37691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.001618] [ip-0A0C04AF:24133:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.002688] [ip-0A0C0491:29565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003429] [ip-0A0C04A9:31518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003360] [ip-0A0C0488:46144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003929] [ip-0A0C0441:40605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003628] [ip-0A0C044D:44816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003894] [ip-0A0C04D9:86024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003932] [ip-0A0C04A9:31517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.003857] [ip-0A0C0498:42328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.004854] [ip-0A0C04B0:18931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.005200] [ip-0A0C04C0:64178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.004586] [ip-0A0C0482:35043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.004628] [ip-0A0C0482:35045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.004777] [ip-0A0C04B7:3027 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.005570] [ip-0A0C044D:44810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.005101] [ip-0A0C04DB:53618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.006122] [ip-0A0C0493:29070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.005926] [ip-0A0C0489:47713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.006221] [ip-0A0C0493:29068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.006825] [ip-0A0C0473:45899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.006950] [ip-0A0C0499:33866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.008573] [ip-0A0C0436:47708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.008431] [ip-0A0C0488:46150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.008627] [ip-0A0C0488:46146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.009777] [ip-0A0C04B3:40181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.009665] [ip-0A0C0441:40607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.010648] [ip-0A0C04BA:97430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.010790] [ip-0A0C040F:17487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.011148] [ip-0A0C0435:80404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.011307] [ip-0A0C040F:17488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.012062] [ip-0A0C04BA:97435:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.013180] [ip-0A0C04A7:9387 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.013443] [ip-0A0C0496:14125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.012527] [ip-0A0C0489:47711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.013946] [ip-0A0C04C8:61696:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.014353] [ip-0A0C04C5:66879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.014300] [ip-0A0C04B0:18933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.014743] [ip-0A0C04B5:33087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.016140] [ip-0A0C04AC:25889:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.016589] [ip-0A0C0435:80405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.016388] [ip-0A0C0441:40610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.017041] [ip-0A0C0438:48307:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.016730] [ip-0A0C0498:42320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.017683] [ip-0A0C04B0:18932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.018229] [ip-0A0C04C8:61697:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.019199] [ip-0A0C04A0:21889:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.018336] [ip-0A0C048F:37102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.018472] [ip-0A0C04CD:58107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.018718] [ip-0A0C04C8:61690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.018808] [ip-0A0C04C5:66877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.019034] [ip-0A0C04CD:58113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.020692] [ip-0A0C04C5:66880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.022288] [ip-0A0C04A6:32079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.022579] [ip-0A0C04AB:22860:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.023946] [ip-0A0C04AB:22859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.023761] [ip-0A0C0490:33568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.025125] [ip-0A0C04CD:58110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.025452] [ip-0A0C04AB:22866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026099] [ip-0A0C04CF:88271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.025314] [ip-0A0C04BC:60649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026498] [ip-0A0C04B6:1372 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026192] [ip-0A0C0494:29239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026087] [ip-0A0C04CF:88278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.027003] [ip-0A0C04D8:61216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026996] [ip-0A0C0473:45892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.027304] [ip-0A0C0473:45895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.026505] [ip-0A0C04B2:8575 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.027067] [ip-0A0C0494:29241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.028898] [ip-0A0C04C6:60484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.030784] [ip-0A0C0496:14122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.031422] [ip-0A0C0496:14121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.030943] [ip-0A0C04C4:64942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.033535] [ip-0A0C0496:14120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.033798] [ip-0A0C04A7:9386 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.034600] [ip-0A0C04A1:50173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.034618] [ip-0A0C0484:49449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.036812] [ip-0A0C04D3:59703:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.036729] [ip-0A0C042D:18775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.034902] [ip-0A0C0484:49447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.037430] [ip-0A0C04D3:59698:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.038061] [ip-0A0C0473:45893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.040086] [ip-0A0C04A7:9385 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.040176] [ip-0A0C04A7:9389 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.039696] [ip-0A0C042D:18773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.040297] [ip-0A0C0438:48310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.042080] [ip-0A0C04AF:24135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.043823] [ip-0A0C04A0:21891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.044428] [ip-0A0C0490:33567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.046986] [ip-0A0C0486:47343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.047179] [ip-0A0C04A0:21892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.046564] [ip-0A0C0487:47324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.046845] [ip-0A0C04C6:60479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.047339] [ip-0A0C048B:30225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.047502] [ip-0A0C048B:30228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.048530] [ip-0A0C0491:29568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.049369] [ip-0A0C0492:27571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.049455] [ip-0A0C04B3:40177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.051300] [ip-0A0C04C3:63649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.052132] [ip-0A0C0435:80407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.052848] [ip-0A0C04CC:61243:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.052508] [ip-0A0C04B9:65471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.053137] [ip-0A0C04C3:63645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.053321] [ip-0A0C04AE:37696:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.052822] [ip-0A0C0490:33571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.063772] [ip-0A0C04A2:23925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.053353] [ip-0A0C04B6:1373 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.054845] [ip-0A0C04A0:21894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.053998] [ip-0A0C04D8:61213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.054128] [ip-0A0C04B5:33086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.054467] [ip-0A0C04B5:33085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.055115] [ip-0A0C04D3:59699:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.054130] [ip-0A0C0484:49453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.056532] [ip-0A0C0438:48308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.058523] [ip-0A0C04D3:59700:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.059104] [ip-0A0C04D3:59704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.061066] [ip-0A0C0460:47075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.061218] [ip-0A0C044D:44808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.062330] [ip-0A0C04B1:35839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.064098] [ip-0A0C0485:12412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.064197] [ip-0A0C0485:12416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.063496] [ip-0A0C048F:37099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.065299] [ip-0A0C0490:33566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.066815] [ip-0A0C048C:28523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.066970] [ip-0A0C049F:20468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.067122] [ip-0A0C049F:20467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.068465] [ip-0A0C042D:18777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.079264] [ip-0A0C04A2:23926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.069101] [ip-0A0C048C:28522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.069129] [ip-0A0C049F:20469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.068543] [ip-0A0C049B:21183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.069709] [ip-0A0C049F:20472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.070896] [ip-0A0C04C2:87420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.070921] [ip-0A0C04C3:63648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.071514] [ip-0A0C04B1:35843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.071979] [ip-0A0C04C6:60481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.073189] [ip-0A0C0487:47321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.073331] [ip-0A0C04DA:54944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.074473] [ip-0A0C0436:47709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.075957] [ip-0A0C0486:47345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.075047] [ip-0A0C04B9:65469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.075426] [ip-0A0C04C2:87415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.076786] [ip-0A0C0435:80403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.076835] [ip-0A0C0435:80408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.077405] [ip-0A0C0499:33871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.078098] [ip-0A0C048B:30222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.078187] [ip-0A0C048B:30229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.077829] [ip-0A0C049B:21181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.077450] [ip-0A0C04BC:60645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.078703] [ip-0A0C04A5:35036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.079611] [ip-0A0C0435:80406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.078960] [ip-0A0C04B7:3029 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.080534] [ip-0A0C0486:47340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.080271] [ip-0A0C04B3:40182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.080214] [ip-0A0C048B:30223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.080717] [ip-0A0C0449:44942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.080842] [ip-0A0C04A8:37792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.081103] [ip-0A0C04A8:37786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.082769] [ip-0A0C04A1:50169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.082899] [ip-0A0C0483:46155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.084413] [ip-0A0C0491:29571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.086331] [ip-0A0C04CC:61245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.086906] [ip-0A0C04D9:86028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.087783] [ip-0A0C04D8:61217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.088694] [ip-0A0C0487:47325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.087164] [ip-0A0C0484:49448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.090187] [ip-0A0C04B1:35838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.091268] [ip-0A0C0499:33864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.091988] [ip-0A0C0438:48312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.092568] [ip-0A0C0449:44944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.092929] [ip-0A0C04C4:64945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.094564] [ip-0A0C04B1:35840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.094373] [ip-0A0C04AA:7255 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.094955] [ip-0A0C04C3:63643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.095158] [ip-0A0C04C3:63646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.095314] [ip-0A0C042D:18771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.095875] [ip-0A0C04BB:7480 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.095431] [ip-0A0C04B9:65477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.096553] [ip-0A0C04A6:32081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.095845] [ip-0A0C046C:43047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.097358] [ip-0A0C0485:12411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.096872] [ip-0A0C048C:28520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.097136] [ip-0A0C0449:44943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.097579] [ip-0A0C04B1:35841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.097693] [ip-0A0C04AE:37689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.108855] [ip-0A0C04A2:23929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.099144] [ip-0A0C04D4:57769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.099612] [ip-0A0C0487:47326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.100649] [ip-0A0C0429:37976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.100909] [ip-0A0C04D4:57767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.100457] [ip-0A0C04C6:60478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101083] [ip-0A0C04B3:40180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101001] [ip-0A0C0438:48306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.100779] [ip-0A0C0429:37972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.100915] [ip-0A0C0429:37970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101386] [ip-0A0C0438:48309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.102002] [ip-0A0C0487:47339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101814] [ip-0A0C0449:44945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.102706] [ip-0A0C04CC:61247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101946] [ip-0A0C046C:43040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.102747] [ip-0A0C04B3:40175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.101886] [ip-0A0C04B2:8573 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.103859] [ip-0A0C048F:37104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.104313] [ip-0A0C04BB:7485 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.104002] [ip-0A0C0429:37971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.104333] [ip-0A0C04A8:37789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.105533] [ip-0A0C04CC:61240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.104231] [ip-0A0C049B:21184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.106923] [ip-0A0C0486:47341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.105292] [ip-0A0C0484:49450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.105416] [ip-0A0C0484:49452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.107714] [ip-0A0C04AA:7251 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.107762] [ip-0A0C04C6:60477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.108471] [ip-0A0C04D9:86025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.108534] [ip-0A0C0429:37974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.108954] [ip-0A0C04AC:25891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.108799] [ip-0A0C0491:29567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.109876] [ip-0A0C04A5:35034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110057] [ip-0A0C04A5:35041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110723] [ip-0A0C04B3:40179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.121128] [ip-0A0C04A2:23928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110766] [ip-0A0C04AF:24137:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110719] [ip-0A0C04DA:54950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110613] [ip-0A0C046C:43044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.110494] [ip-0A0C04A8:37785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.111228] [ip-0A0C04DA:54951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.113392] [ip-0A0C0485:12413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.112602] [ip-0A0C04B9:65467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.111960] [ip-0A0C049B:21188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.114068] [ip-0A0C04A1:50174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.115270] [ip-0A0C04CC:61242:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.114925] [ip-0A0C04DA:54945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.115584] [ip-0A0C04A6:32077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.115341] [ip-0A0C04DA:54947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.116598] [ip-0A0C042D:18800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.117455] [ip-0A0C042D:18776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118591] [ip-0A0C0485:12409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.128293] [ip-0A0C04A2:23931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.117830] [ip-0A0C0491:29566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.117867] [ip-0A0C04C4:64947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118265] [ip-0A0C04A1:50167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.117960] [ip-0A0C0460:47073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.128697] [ip-0A0C04A2:23932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118321] [ip-0A0C0483:46158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.119073] [ip-0A0C0485:12415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118706] [ip-0A0C04D4:57773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118823] [ip-0A0C0491:29564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118811] [ip-0A0C0490:33570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.119320] [ip-0A0C046C:43042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.121207] [ip-0A0C0486:47346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.118856] [ip-0A0C0484:49451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.120791] [ip-0A0C04B9:65472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.123458] [ip-0A0C0486:47342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.122505] [ip-0A0C046C:43045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.123457] [ip-0A0C0460:47080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.125355] [ip-0A0C0492:27569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.125553] [ip-0A0C0492:27568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.124941] [ip-0A0C0460:47077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.124787] [ip-0A0C04A8:37788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.125251] [ip-0A0C048F:37098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.126011] [ip-0A0C04D4:57768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.127511] [ip-0A0C04A6:32083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.127866] [ip-0A0C04D9:86027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.127951] [ip-0A0C04C4:64944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.128292] [ip-0A0C0483:46160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.128928] [ip-0A0C04AC:25890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.127510] [ip-0A0C04BC:60646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.129710] [ip-0A0C0492:27574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.130176] [ip-0A0C0436:47711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.129453] [ip-0A0C04BC:60650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.130481] [ip-0A0C044D:44817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.130482] [ip-0A0C0490:33565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.130761] [ip-0A0C0490:33569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.129881] [ip-0A0C04BC:60652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.131602] [ip-0A0C04C7:62616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.131512] [ip-0A0C04B9:65468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.132529] [ip-0A0C0460:47076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.132809] [ip-0A0C04B9:65476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.133407] [ip-0A0C048C:28521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.134297] [ip-0A0C04A8:37791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.135037] [ip-0A0C04B6:1370 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.135626] [ip-0A0C04C4:64941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.135478] [ip-0A0C04AA:7254 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.136042] [ip-0A0C04B7:3028 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.135422] [ip-0A0C049B:21182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.136898] [ip-0A0C04A4:32612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.137437] [ip-0A0C0499:33865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.137985] [ip-0A0C0436:47710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.138232] [ip-0A0C04D8:61212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.138582] [ip-0A0C04C7:62619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.138668] [ip-0A0C0483:46157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.139191] [ip-0A0C04BB:7484 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.139737] [ip-0A0C0492:27570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.139614] [ip-0A0C04C2:87418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.140054] [ip-0A0C04BB:7482 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.140808] [ip-0A0C048F:37103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.140730] [ip-0A0C04A5:35039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.141442] [ip-0A0C04A1:50176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.141416] [ip-0A0C04A1:50168:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.142801] [ip-0A0C0492:27577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.142222] [ip-0A0C0436:47707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.143268] [ip-0A0C04D4:57772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.143541] [ip-0A0C04D4:57770:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.144254] [ip-0A0C04D8:61209:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.144384] [ip-0A0C04AF:24160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.144948] [ip-0A0C04C7:62617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.145190] [ip-0A0C04A5:35035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.146965] [ip-0A0C048C:28519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.147579] [ip-0A0C04B7:3032 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.148117] [ip-0A0C048C:28517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.148871] [ip-0A0C0436:47704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.149079] [ip-0A0C0483:46156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.150473] [ip-0A0C04A5:35037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.154638] [ip-0A0C04A4:32616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.155237] [ip-0A0C044D:44811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.156048] [ip-0A0C04BB:7481 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.155911] [ip-0A0C044D:44807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.156078] [ip-0A0C044D:44809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.156054] [ip-0A0C049B:21185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.158720] [ip-0A0C04B2:8576 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.159337] [ip-0A0C0499:33869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.159604] [ip-0A0C04AA:7257 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.160257] [ip-0A0C04BB:7487 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.160984] [ip-0A0C049B:21186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.163236] [ip-0A0C04AC:25888:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.163322] [ip-0A0C04D9:86029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.164613] [ip-0A0C0499:33867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.165227] [ip-0A0C04AA:7256 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.165476] [ip-0A0C04C7:62615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.166075] [ip-0A0C04C7:62620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.166891] [ip-0A0C0483:46159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.166639] [ip-0A0C04CF:88273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.167380] [ip-0A0C04AE:37693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.168268] [ip-0A0C04CF:88274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.169249] [ip-0A0C048C:28524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.169317] [ip-0A0C04D8:61211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.169668] [ip-0A0C04AC:25885:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.169505] [ip-0A0C04D8:61214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.169927] [ip-0A0C048F:37097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.170030] [ip-0A0C048F:37096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.171696] [ip-0A0C04AC:25887:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.171759] [ip-0A0C04C7:62618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.172240] [ip-0A0C0483:46161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.172254] [ip-0A0C04B5:33091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.173650] [ip-0A0C0499:33868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.175141] [ip-0A0C04CF:88272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.175815] [ip-0A0C04D9:86031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.176222] [ip-0A0C04AF:24131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.176379] [ip-0A0C04C2:87414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.176548] [ip-0A0C04AE:37692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.177776] [ip-0A0C04C2:87417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.178676] [ip-0A0C04C2:87416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.180038] [ip-0A0C04AA:7253 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.180375] [ip-0A0C04B6:1371 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.185158] [ip-0A0C04A6:32082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.185993] [ip-0A0C04A6:32078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.189093] [ip-0A0C04B7:3037 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.189226] [ip-0A0C04B7:3035 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.188771] [ip-0A0C04B2:8601 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.191173] [ip-0A0C04A4:32617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.191597] [ip-0A0C04D9:86030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.191386] [ip-0A0C04B7:3030 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.192036] [ip-0A0C04D9:86032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.192393] [ip-0A0C04C2:87419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.192821] [ip-0A0C04B5:33089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.197512] [ip-0A0C04A6:32076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.197236] [ip-0A0C04B6:1376 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.197182] [ip-0A0C04B5:33090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.197636] [ip-0A0C04CF:88270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.200421] [ip-0A0C04B6:1368 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.201907] [ip-0A0C04B7:3031 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.205955] [ip-0A0C04CF:88277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.206341] [ip-0A0C04CF:88276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.206479] [ip-0A0C04B5:33088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.207299] [ip-0A0C04B5:33084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.208632] [ip-0A0C04AF:24132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.208827] [ip-0A0C04AF:24136:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.210546] [ip-0A0C04B6:1367 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.211109] [ip-0A0C04AF:24134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.211569] [ip-0A0C04AE:37695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.217744] [ip-0A0C04AE:37690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.218861] [ip-0A0C04AE:37694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.217936] [ip-0A0C04B2:8571 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.221068] [ip-0A0C04B2:8570 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.243990] [ip-0A0C04B2:8574 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.248657] [ip-0A0C04B2:8572 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.285472] [ip-0A0C04A4:32613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.318989] [ip-0A0C04A4:32614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.328534] [ip-0A0C04A4:32611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.334273] [ip-0A0C04A4:32618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623150.336640] [ip-0A0C04A4:32615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634623151237, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634623151277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634623151278, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634623151278, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623151278, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634623151278, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634623151279, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:59:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:80407 - context.c:584] INFO job (ID: 867565245106667642) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80407 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80407 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80406 - context.c:584] INFO job (ID: 867564426189828886) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80406 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80406 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80402 - context.c:584] INFO job (ID: 867564526906178710) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80402 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80402 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80416 - context.c:584] INFO job (ID: 867565142654216679) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80416 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80416 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80405 - context.c:584] INFO job (ID: 867564960315506943) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80405 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80405 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80408 - context.c:584] INFO job (ID: 867564791838988934) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80408 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80408 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80403 - context.c:584] INFO job (ID: 867564620119031133) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80403 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80403 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:80404 - context.c:584] INFO job (ID: 867564974985194314) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:80404 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:80404 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243275, "event_type": "POINT_IN_TIME", "key": "seed", "value": 907591195, "metadata": {"file": "main.py", "lineno": 72}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243276, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243276, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243276, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243276, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623243277, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:43] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:00:44] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623267369, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634623267401, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623267405, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634623267406, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623270011, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634623270011, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634623270011, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623270012, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623271798, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 1881.5840655646302, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623271798, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623271798, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1881.5840655646302, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634623271799, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623271799, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623272458, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5098.216218152114, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623272458, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623272458, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5098.216218152114, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623272458, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623272458, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623273107, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5182.475624510449, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623273107, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623273107, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5182.475624510449, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623273107, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623273107, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623273744, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5277.48206245113, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623273744, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623273745, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5277.48206245113, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634623273745, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623273745, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623274377, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.812087283463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623274377, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623274377, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.812087283463, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634623274377, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623274377, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623274999, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.632943082247, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623274999, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623274999, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.632943082247, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634623275000, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623275000, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623275619, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.649684747706, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623275619, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623275619, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.649684747706, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623275619, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623275619, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623276251, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5323.386315895637, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623276251, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623276251, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5323.386315895637, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634623276251, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623276251, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623276872, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5411.741224602333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623276872, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623276872, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5411.741224602333, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634623276872, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623276873, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623277495, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5395.24344107592, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623277496, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623277496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5395.24344107592, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634623277496, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623277496, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623278125, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.837071602331, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623278125, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623278126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.837071602331, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634623278126, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623278126, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623278745, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.0600454955475, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623278745, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623278745, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.0600454955475, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634623278745, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623278745, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623279364, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5433.1200518142705, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623279364, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623279364, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5433.1200518142705, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634623279364, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623279364, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623279991, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.464162854839, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623279991, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623279991, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.464162854839, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634623279991, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623279992, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623280609, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5441.055466262461, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623280609, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623280610, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5441.055466262461, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634623280610, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623280610, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623281225, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5459.797257561, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623281225, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623281226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5459.797257561, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634623281226, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623281226, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623281838, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5487.7589957220525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623281839, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623281839, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5487.7589957220525, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634623281839, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623281839, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623282455, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5455.06117988873, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623282455, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623282455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5455.06117988873, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634623282456, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623282456, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623283072, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.54430460496, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623283073, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623283073, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.54430460496, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634623283073, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623283073, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623283689, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5454.719131261735, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623283690, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623283690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5454.719131261735, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634623283690, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623283690, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623284307, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.950889750165, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623284307, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623284308, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.950889750165, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634623284308, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623284308, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623284922, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5472.590351246441, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623284922, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623284923, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5472.590351246441, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634623284923, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623284923, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623285537, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.756887816291, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623285538, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623285538, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.756887816291, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634623285538, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623285538, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623286149, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5499.874312996776, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623286150, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623286150, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5499.874312996776, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634623286150, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623286150, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623286767, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.122531313472, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623286767, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623286768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.122531313472, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634623286768, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623286768, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623287378, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5512.618924588682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623287378, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623287378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5512.618924588682, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634623287378, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623287378, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623287991, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.225650673022, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623287992, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623287992, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.225650673022, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634623287992, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623287992, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623288603, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5502.333015259725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623288603, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623288604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5502.333015259725, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634623288604, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623288604, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623289214, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5508.509853891916, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623289214, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623289214, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5508.509853891916, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634623289214, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623289214, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623289835, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.4042477184, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623289835, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623289836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.4042477184, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634623289836, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623289836, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623290448, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5486.17384953885, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623290449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623290449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5486.17384953885, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634623290449, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623290449, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623291067, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5439.776431598182, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623291067, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623291067, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5439.776431598182, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634623291067, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623291067, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623291678, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5501.853986472638, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623291678, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623291678, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5501.853986472638, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634623291678, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623291678, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623292288, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5516.326003863384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623292288, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623292288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5516.326003863384, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634623292288, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623292288, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623292894, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5544.138329047235, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623292895, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623292895, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5544.138329047235, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634623292895, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623292895, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623293505, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5506.800803696818, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623293505, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623293506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5506.800803696818, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634623293506, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623293506, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623294117, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.593475727004, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623294118, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623294118, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.593475727004, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634623294118, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623294118, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623294731, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5483.155633197287, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623294731, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623294731, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5483.155633197287, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634623294731, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623294731, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623295347, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5457.2834287550895, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623295347, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623295347, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5457.2834287550895, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634623295348, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623295348, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623295958, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5508.21273878081, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623295958, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623295958, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5508.21273878081, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634623295958, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623295958, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623296569, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5506.20267150937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623296569, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623296569, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5506.20267150937, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634623296569, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623296569, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623297181, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5500.254248269079, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623297181, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623297181, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5500.254248269079, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634623297181, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623297181, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623297790, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5517.220073592367, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623297791, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623297791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5517.220073592367, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634623297791, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623297791, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623298408, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.026894137332, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623298408, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623298408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.026894137332, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634623298408, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623298408, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623299018, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5512.603830266617, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623299018, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623299018, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5512.603830266617, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634623299019, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623299019, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623299633, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5467.202066950898, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623299634, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623299634, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5467.202066950898, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634623299634, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623299634, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623300249, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5468.076039324906, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623300249, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623300249, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5468.076039324906, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634623300249, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623300249, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623300862, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.929507500245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623300863, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623300863, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.929507500245, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634623300863, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623300863, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623301477, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5474.920491858049, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623301478, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623301478, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5474.920491858049, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634623301478, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623301478, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623302098, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5423.470678219508, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623302098, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623302098, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5423.470678219508, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634623302168, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623302168, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623302185, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623302614, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8627978563308716, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623302614, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623302769, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5594.636527662818, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623302769, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623302769, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5594.636527662818, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623302823, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623302823, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623302825, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623303301, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8048654794692993, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623303301, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623303498, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4984.092845866129, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623303498, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623303498, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4984.092845866129, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623303535, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623303535, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623303551, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623303978, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8792179822921753, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623303978, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623304163, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5349.877551020408, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623304164, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623304164, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5349.877551020408, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623304315, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623304315, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623304330, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623304728, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8767985105514526, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623304728, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623304941, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.222236519809, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623304942, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623304942, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.222236519809, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623304985, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623304985, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623305000, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623305433, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8756848573684692, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623305433, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623305626, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5240.96728091422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623305627, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623305627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5240.96728091422, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623305661, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623305661, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623305678, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623306181, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8597198724746704, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623306181, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623306394, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4582.545134107341, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623306395, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623306395, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4582.545134107341, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623306491, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623306492, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623306506, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623306903, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8681230545043945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623306904, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623307100, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5524.328940116517, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623307100, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623307100, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5524.328940116517, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623307151, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623307151, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623307166, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623307576, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8631573915481567, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623307576, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623307760, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5515.987024210619, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623307761, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623307761, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5515.987024210619, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623307793, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623307793, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623307811, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623308219, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8886582851409912, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623308219, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623308410, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.640314107361, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623308410, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623308410, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.640314107361, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623308446, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623308446, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623308462, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623308880, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8759118914604187, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623308880, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623309083, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5279.914039921383, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623309083, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623309083, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5279.914039921383, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623309111, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623309111, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623309129, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623309559, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8914623260498047, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623309559, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623309746, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5298.2912964756415, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623309746, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623309746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5298.2912964756415, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623309773, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623309773, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623309790, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623310218, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8704767823219299, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623310218, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623310406, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5308.635588511718, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623310407, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623310407, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5308.635588511718, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623310443, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623310443, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623310459, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623310880, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895940780639648, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623310880, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623311062, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.063667958726, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623311062, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623311062, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.063667958726, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623311096, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623311096, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623311112, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623311534, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8902177214622498, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623311534, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623311715, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.043923917239, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623311715, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623311715, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.043923917239, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623311750, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623311751, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623311767, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623312181, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8793593049049377, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623312181, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623312373, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5399.938248591186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623312373, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623312373, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5399.938248591186, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623312403, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623312404, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623312420, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623312836, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8850733041763306, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623312837, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623313025, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.229582267264, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623313025, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623313025, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.229582267264, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623313057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623313057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623313073, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623313594, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8809900879859924, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623313594, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623313810, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4462.9256343810475, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623313810, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623313811, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4462.9256343810475, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623313844, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623313844, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623313860, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623314265, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8920431733131409, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623314266, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623314447, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5575.045261837445, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623314448, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623314448, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5575.045261837445, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623314483, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623314483, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623314499, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623314912, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8803907036781311, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623314912, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623315090, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5541.766343809955, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623315090, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623315090, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5541.766343809955, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623315128, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623315128, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623315145, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623315565, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8896312713623047, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623315565, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623315758, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.57598297426, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623315758, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623315758, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.57598297426, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623315793, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623315793, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623315810, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623316213, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897865891456604, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623316213, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623316393, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5603.69054345557, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623316394, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623316394, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5603.69054345557, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623316431, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623316432, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623316447, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623316850, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8818217515945435, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623316851, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623317036, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5556.346409058042, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623317037, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623317037, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5556.346409058042, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623317072, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623317072, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623317091, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623317509, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8973475098609924, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623317509, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623317696, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5389.705358857018, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623317696, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623317696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5389.705358857018, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623317733, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623317733, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623317751, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623318172, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8870341181755066, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623318172, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623318356, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.31000152342, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623318356, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623318356, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.31000152342, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623318392, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623318392, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623318409, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623318811, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011183977127075, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623318812, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623318993, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5596.640568874938, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623318993, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623318993, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5596.640568874938, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623319028, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623319028, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623319046, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623319448, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8907903432846069, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623319449, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623319631, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5573.984641233258, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623319631, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623319632, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5573.984641233258, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623319667, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623319667, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623319685, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623320086, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8447744250297546, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623320087, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623320267, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5601.467707875003, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623320267, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623320268, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5601.467707875003, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623320303, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623320303, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623320320, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623320732, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8926467895507812, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623320732, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623320960, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5114.357044639743, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623320960, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623320960, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5114.357044639743, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623320995, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623320995, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623321012, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623321414, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001749753952026, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623321414, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623321612, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.0644350366965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623321612, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623321612, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.0644350366965, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623321646, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623321646, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623321664, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623322083, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8868975639343262, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623322083, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623322305, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5101.106067346266, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623322306, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623322306, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5101.106067346266, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623322341, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623322341, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623322355, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623322777, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8855454921722412, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623322778, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623322960, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.825348673745, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623322960, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623322960, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.825348673745, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623322994, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623322995, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623323011, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623323411, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9004020690917969, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623323411, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623323590, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5642.855145407152, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623323591, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623323591, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5642.855145407152, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623323623, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623323624, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623323640, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623324042, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9012743234634399, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623324042, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623324225, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5590.623876598052, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623324225, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623324225, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5590.623876598052, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623324260, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623324261, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623324277, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623324698, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8875794410705566, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623324698, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623324880, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.645015835732, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623324880, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623324880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.645015835732, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623324916, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623324916, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623324933, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623325355, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8972406387329102, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623325355, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623325536, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5425.531486332285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623325536, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623325536, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5425.531486332285, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623325571, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623325572, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623325587, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623325989, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011172652244568, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623325989, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623326181, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5518.20518151757, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623326181, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623326181, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5518.20518151757, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623326211, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623326211, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623326227, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623326643, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9045275449752808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623326643, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623326817, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5546.193655482474, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623326818, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623326818, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5546.193655482474, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623326860, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623326860, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623326876, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623327285, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8988237977027893, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623327285, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623327471, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.933963078406, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623327471, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623327471, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.933963078406, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623327505, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623327505, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623327523, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623327923, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8816975355148315, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623327923, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623328103, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5616.948276776024, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623328104, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623328104, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5616.948276776024, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623328139, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623328139, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623328155, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623328558, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9029461145401001, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623328558, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623328735, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5642.53206561625, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623328735, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623328735, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5642.53206561625, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623328770, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623328770, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623328787, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623329207, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910021185874939, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623329207, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623329383, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.577556496236, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623329384, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623329384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.577556496236, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623329420, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623329420, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623329435, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623329838, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8791769742965698, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623329838, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623330019, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5611.609508094368, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623330019, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623330019, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5611.609508094368, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623330054, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623330055, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623330072, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623330475, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9008331298828125, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623330475, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623330653, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5617.4229275858315, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623330654, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623330654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5617.4229275858315, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623330689, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623330689, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623330707, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623331135, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9052062034606934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623331135, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623331320, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5329.933599259029, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623331320, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623331320, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5329.933599259029, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623331355, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623331356, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623331374, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623331802, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8735942840576172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623331802, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623331986, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5331.413596997461, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623331987, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623331987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5331.413596997461, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623332021, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623332021, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623332038, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623332438, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9056340456008911, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623332438, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623332616, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5651.750961184084, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623332617, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623332617, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5651.750961184084, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623332647, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623332647, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623332664, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623333089, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9075677394866943, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623333089, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623333270, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.7426445776355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623333271, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623333271, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.7426445776355, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623333300, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623333300, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623333317, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623333734, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9046555757522583, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623333734, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623333911, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5502.16760127262, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623333912, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623333912, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5502.16760127262, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623333946, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623333946, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623333962, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623334394, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9058519601821899, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623334394, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623334579, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5310.383909148699, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623334580, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623334580, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5310.383909148699, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623334610, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623334611, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623334628, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1980, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623335039, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9087485074996948, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623335039, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1980, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623335040, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634623335226, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.698117298673, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623335227, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623335227, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.698117298673, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1980}}
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:21 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:22 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:24 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:25 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:27 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:28 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:29 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:30 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:31 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:32 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:33 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
ENDING TIMING RUN AT 2021-10-19 06:02:34 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:59:04 AM
