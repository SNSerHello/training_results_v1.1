+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019061549768100547
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019061549768100547
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061549768100547
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061549768100547
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07370/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061549768100547_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C0475
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C047F
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:15:53 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634624157.960753] [ip-0A0C040E:62114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624157.987526] [ip-0A0C040B:20279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624157.996680] [ip-0A0C040C:86711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.014664] [ip-0A0C0409:40806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.031235] [ip-0A0C0410:90671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.051468] [ip-0A0C040C:86710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.052506] [ip-0A0C040C:86706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.059306] [ip-0A0C040B:20282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.061675] [ip-0A0C040E:62112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.061750] [ip-0A0C040E:62116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.061653] [ip-0A0C042E:73185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.070211] [ip-0A0C040B:20284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.082170] [ip-0A0C0409:40817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.086055] [ip-0A0C0430:8441 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.092093] [ip-0A0C0409:40810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.093513] [ip-0A0C042A:64619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.093386] [ip-0A0C0410:90674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.095320] [ip-0A0C0408:658  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.098121] [ip-0A0C043D:50390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.098998] [ip-0A0C044A:53201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.098556] [ip-0A0C040B:20302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.102601] [ip-0A0C0467:44402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.104457] [ip-0A0C0442:53752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.105505] [ip-0A0C0417:7279 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.108483] [ip-0A0C043B:59612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.109894] [ip-0A0C040E:62108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.112038] [ip-0A0C0408:661  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.115105] [ip-0A0C0457:54372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.117260] [ip-0A0C045A:58253:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.120106] [ip-0A0C0409:40804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.120576] [ip-0A0C0411:73893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.121738] [ip-0A0C046D:42897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.124618] [ip-0A0C042C:67412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.126971] [ip-0A0C040A:75596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.126994] [ip-0A0C040A:75595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.128886] [ip-0A0C0431:64570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.126437] [ip-0A0C042E:73182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.135127] [ip-0A0C045A:58258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.134991] [ip-0A0C040E:62111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.135886] [ip-0A0C0430:8440 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.137432] [ip-0A0C046D:42892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.137886] [ip-0A0C043B:59603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.139258] [ip-0A0C0480:43514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.140905] [ip-0A0C040C:86705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.143369] [ip-0A0C0457:54366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.148450] [ip-0A0C0463:48740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.149237] [ip-0A0C0467:44398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.149238] [ip-0A0C047B:50783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.150860] [ip-0A0C0411:73895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.151500] [ip-0A0C0456:61845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.155656] [ip-0A0C0480:43519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.158952] [ip-0A0C040E:62113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.158804] [ip-0A0C043D:50389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.158802] [ip-0A0C043D:50387:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.159484] [ip-0A0C042B:72070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.159976] [ip-0A0C041E:74013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.160509] [ip-0A0C040E:62109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.161829] [ip-0A0C0424:62860:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.161909] [ip-0A0C044C:56199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.161286] [ip-0A0C0442:53757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.165174] [ip-0A0C040E:62110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.165340] [ip-0A0C0453:59897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.166525] [ip-0A0C041E:74011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.166623] [ip-0A0C0410:90672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.166142] [ip-0A0C0442:53755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.168986] [ip-0A0C0411:73890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.171579] [ip-0A0C041F:77993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.171811] [ip-0A0C040B:20278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.173758] [ip-0A0C045E:55600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.174213] [ip-0A0C040C:86709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.174409] [ip-0A0C0409:40812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.175449] [ip-0A0C044E:51336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.175514] [ip-0A0C0417:7278 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.177293] [ip-0A0C0432:98144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.177334] [ip-0A0C0432:98145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.178635] [ip-0A0C0463:48744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.180062] [ip-0A0C0431:64571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.180268] [ip-0A0C0431:64574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.180371] [ip-0A0C0430:8437 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.179642] [ip-0A0C042F:69264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.180327] [ip-0A0C0440:53392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.180972] [ip-0A0C042C:67416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.182741] [ip-0A0C043B:59600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.184160] [ip-0A0C041B:92039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.187387] [ip-0A0C040B:20280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.188798] [ip-0A0C040B:20283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.188903] [ip-0A0C040B:20281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.189956] [ip-0A0C040C:86707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.190330] [ip-0A0C042B:72069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.191203] [ip-0A0C040C:86708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.193109] [ip-0A0C0416:61164:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.193083] [ip-0A0C0409:40808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.195326] [ip-0A0C047A:49175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.195899] [ip-0A0C0456:61840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.196296] [ip-0A0C0467:44399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.196516] [ip-0A0C0455:52994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.196500] [ip-0A0C0455:52997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.196432] [ip-0A0C0417:7273 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.197339] [ip-0A0C042A:64645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.197372] [ip-0A0C044F:52019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.197631] [ip-0A0C0457:54371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.197865] [ip-0A0C0410:90673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.199930] [ip-0A0C045B:62127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.200935] [ip-0A0C040C:86712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.203736] [ip-0A0C045A:58255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.204875] [ip-0A0C0465:46243:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.204728] [ip-0A0C0410:90675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.205978] [ip-0A0C0419:78419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.206700] [ip-0A0C0412:53548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.207009] [ip-0A0C0422:5122 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.207803] [ip-0A0C0409:40805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.207888] [ip-0A0C0409:40819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.209825] [ip-0A0C0412:53549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.210901] [ip-0A0C0476:34329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.211167] [ip-0A0C042E:73178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.213018] [ip-0A0C042F:69267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.213727] [ip-0A0C044C:56198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.213379] [ip-0A0C0420:75215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.215526] [ip-0A0C0414:90662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.214968] [ip-0A0C042C:67411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.215610] [ip-0A0C0433:68184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.215587] [ip-0A0C041B:92034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.216737] [ip-0A0C045C:54989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.216760] [ip-0A0C045C:54987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.220192] [ip-0A0C045B:62129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.219507] [ip-0A0C0410:90670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.220473] [ip-0A0C043D:50384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.221312] [ip-0A0C0418:70893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.223250] [ip-0A0C0447:61036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.223757] [ip-0A0C042A:64621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.226213] [ip-0A0C0465:46248:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.226117] [ip-0A0C0451:56811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.227738] [ip-0A0C044B:57534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.229243] [ip-0A0C0424:62857:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.229781] [ip-0A0C0432:98143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.229512] [ip-0A0C0424:62858:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.229973] [ip-0A0C042B:72071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.231674] [ip-0A0C047F:47506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.233124] [ip-0A0C041E:74007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.234643] [ip-0A0C0446:62063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.235462] [ip-0A0C042A:64625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.236416] [ip-0A0C044A:53206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.236842] [ip-0A0C0408:660  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.237251] [ip-0A0C0423:67756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.239701] [ip-0A0C0475:40574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.242180] [ip-0A0C0414:90658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.241533] [ip-0A0C0434:62832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.241931] [ip-0A0C043D:50382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.243285] [ip-0A0C0446:62064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.244444] [ip-0A0C0410:90676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.245349] [ip-0A0C047A:49177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.244993] [ip-0A0C0410:90677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.245843] [ip-0A0C0407:78782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.244828] [ip-0A0C042E:73179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.247638] [ip-0A0C0430:8435 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.247803] [ip-0A0C044A:53202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.247668] [ip-0A0C0445:64650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.249335] [ip-0A0C0476:34327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.249922] [ip-0A0C044A:53217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.249564] [ip-0A0C047B:50780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.249861] [ip-0A0C047B:50782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.251039] [ip-0A0C0450:29815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.252247] [ip-0A0C0465:46249:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.251572] [ip-0A0C0442:53758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.252848] [ip-0A0C0430:8439 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.252820] [ip-0A0C045C:54995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.254096] [ip-0A0C045E:55575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.254475] [ip-0A0C0408:657  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.254703] [ip-0A0C046D:42895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.254515] [ip-0A0C041A:97651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.255217] [ip-0A0C0433:68182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.255868] [ip-0A0C0458:62225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.255781] [ip-0A0C040A:75592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.256573] [ip-0A0C041B:92035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.254850] [ip-0A0C042E:73183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.257125] [ip-0A0C040A:75591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.259917] [ip-0A0C043C:60343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.261255] [ip-0A0C0467:44400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.261215] [ip-0A0C043D:50383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.262232] [ip-0A0C0456:61842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.262528] [ip-0A0C0463:48746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.261925] [ip-0A0C0422:5131 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.262607] [ip-0A0C0452:57905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.262695] [ip-0A0C0417:7274 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.263406] [ip-0A0C0420:75213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.263936] [ip-0A0C0453:59894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.263769] [ip-0A0C0462:85687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.265421] [ip-0A0C044C:56203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.266921] [ip-0A0C0433:68179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.266986] [ip-0A0C047D:44675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.267410] [ip-0A0C0476:34325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.268662] [ip-0A0C042A:64616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.268640] [ip-0A0C044F:52022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.270010] [ip-0A0C0411:73888:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.271056] [ip-0A0C0458:62224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.271712] [ip-0A0C044A:53199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.272176] [ip-0A0C045B:62126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.269372] [ip-0A0C042E:73180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.272786] [ip-0A0C0423:67751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.272500] [ip-0A0C0425:77182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.270854] [ip-0A0C042E:73181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.273616] [ip-0A0C0417:7275 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.274305] [ip-0A0C041B:92037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.274418] [ip-0A0C0471:43679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.274886] [ip-0A0C0481:42305:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.274945] [ip-0A0C0418:70889:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.276567] [ip-0A0C047B:50786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.276616] [ip-0A0C0467:44397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.274634] [ip-0A0C042E:73184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.276685] [ip-0A0C043A:59515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.277572] [ip-0A0C0456:61843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.277504] [ip-0A0C041C:80026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.278000] [ip-0A0C0453:59898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.278357] [ip-0A0C0447:61039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.278302] [ip-0A0C041F:77994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.278771] [ip-0A0C0462:85685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.279467] [ip-0A0C044B:57533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.279659] [ip-0A0C0430:8436 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.279191] [ip-0A0C043B:59602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.279652] [ip-0A0C0447:61041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.279637] [ip-0A0C043D:50385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.280017] [ip-0A0C0408:659  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.280327] [ip-0A0C045E:55570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.281118] [ip-0A0C0518:72829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.280932] [ip-0A0C0408:655  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.280744] [ip-0A0C041F:77991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.281974] [ip-0A0C0431:64572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.282987] [ip-0A0C046D:42894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.282975] [ip-0A0C0451:56818:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.283311] [ip-0A0C0452:57908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.283272] [ip-0A0C0425:77157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.283284] [ip-0A0C047D:44672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.284499] [ip-0A0C045A:58256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.285947] [ip-0A0C0518:72835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.286292] [ip-0A0C040A:75589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.286824] [ip-0A0C040A:75617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.287366] [ip-0A0C0470:39101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.287838] [ip-0A0C0480:43513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.287678] [ip-0A0C0418:70888:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.288636] [ip-0A0C045D:45627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.289366] [ip-0A0C0428:62898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.288359] [ip-0A0C0442:53753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.289938] [ip-0A0C0407:78783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.290468] [ip-0A0C0426:72676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.290561] [ip-0A0C0467:44403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.290656] [ip-0A0C0467:44401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.291380] [ip-0A0C0411:73892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.291413] [ip-0A0C043D:50388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.291345] [ip-0A0C0457:54369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.291505] [ip-0A0C0412:53554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.291442] [ip-0A0C0413:8616 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.292240] [ip-0A0C0412:53552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.292389] [ip-0A0C0459:54289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.293178] [ip-0A0C0416:61165:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.293205] [ip-0A0C0457:54368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.294298] [ip-0A0C0450:29812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.294930] [ip-0A0C045D:45618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295386] [ip-0A0C042A:64618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295387] [ip-0A0C0479:44929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295228] [ip-0A0C0453:59899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295242] [ip-0A0C041A:97646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295945] [ip-0A0C042C:67410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295568] [ip-0A0C040A:75593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.295838] [ip-0A0C043B:59598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.297146] [ip-0A0C042A:64617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.297092] [ip-0A0C0430:8438 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.296447] [ip-0A0C0417:7280 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.297910] [ip-0A0C0431:64579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.297983] [ip-0A0C045A:58257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.297431] [ip-0A0C042F:69271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.298017] [ip-0A0C044E:51337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.298856] [ip-0A0C0426:72679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.298279] [ip-0A0C044E:51339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.298940] [ip-0A0C0452:57906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.299123] [ip-0A0C046D:42891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.299381] [ip-0A0C0454:58733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.299376] [ip-0A0C0430:8442 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.299311] [ip-0A0C0457:54370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.298780] [ip-0A0C0442:53754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.300796] [ip-0A0C047F:47507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.300837] [ip-0A0C047F:47508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.301534] [ip-0A0C0428:62899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.300216] [ip-0A0C0442:53759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.301507] [ip-0A0C0458:62226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.301497] [ip-0A0C0448:66389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.301766] [ip-0A0C044C:56205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.302550] [ip-0A0C0408:654  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.302655] [ip-0A0C0451:56812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.303000] [ip-0A0C043C:60344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.303481] [ip-0A0C0408:656  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.304156] [ip-0A0C044C:56204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.304282] [ip-0A0C046D:42890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.305664] [ip-0A0C044A:53200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.305944] [ip-0A0C042A:64620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.305876] [ip-0A0C042F:69266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.306918] [ip-0A0C0470:39099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.307042] [ip-0A0C044B:57546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.307145] [ip-0A0C0440:53384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.308282] [ip-0A0C0440:53390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.308720] [ip-0A0C0424:62856:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.309032] [ip-0A0C0411:73889:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.308522] [ip-0A0C0417:7276 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.309065] [ip-0A0C0481:42299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.309154] [ip-0A0C0411:73894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.309533] [ip-0A0C047B:50784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.310253] [ip-0A0C0467:44396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.310517] [ip-0A0C0480:43518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.310144] [ip-0A0C0442:53756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.311163] [ip-0A0C041D:70693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.312487] [ip-0A0C045E:55571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.311645] [ip-0A0C0417:7277 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.313869] [ip-0A0C0416:61167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.313450] [ip-0A0C040A:75590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.314687] [ip-0A0C045A:58254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.315219] [ip-0A0C045A:58251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.315814] [ip-0A0C0474:40125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.316313] [ip-0A0C0479:44930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.317252] [ip-0A0C0445:64648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.317934] [ip-0A0C0439:51328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318425] [ip-0A0C0432:98141:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318251] [ip-0A0C0455:52990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318422] [ip-0A0C0420:75211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318142] [ip-0A0C043A:59519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318377] [ip-0A0C043B:59599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.318829] [ip-0A0C041C:80024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.320327] [ip-0A0C042C:67414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.321027] [ip-0A0C043B:59605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.321864] [ip-0A0C041E:74009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.322219] [ip-0A0C0451:56816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.322240] [ip-0A0C0420:75216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.323467] [ip-0A0C0411:73891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.323252] [ip-0A0C0427:71004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.323995] [ip-0A0C045A:58252:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.324744] [ip-0A0C041E:74006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.324771] [ip-0A0C042B:72068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.325262] [ip-0A0C0431:64573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.325914] [ip-0A0C0432:98146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.325931] [ip-0A0C0428:62897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.326540] [ip-0A0C0431:64569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.326759] [ip-0A0C0423:67755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.326696] [ip-0A0C0431:64576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.326626] [ip-0A0C0445:64671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.327891] [ip-0A0C0455:52991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.328236] [ip-0A0C043E:47966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.328251] [ip-0A0C0476:34326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.329263] [ip-0A0C044A:53198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.330035] [ip-0A0C042F:69265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.330675] [ip-0A0C046D:42896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.330924] [ip-0A0C0463:48743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.331011] [ip-0A0C045D:45617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.331431] [ip-0A0C045C:54991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.331152] [ip-0A0C046A:49318:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.332584] [ip-0A0C0463:48741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333064] [ip-0A0C0407:78788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333304] [ip-0A0C044A:53204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333121] [ip-0A0C047B:50779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333668] [ip-0A0C0480:43515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333818] [ip-0A0C046D:42893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.334038] [ip-0A0C042C:67413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.333785] [ip-0A0C0457:54365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.334449] [ip-0A0C0471:43680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.335626] [ip-0A0C0424:62859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.335601] [ip-0A0C0440:53388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.335578] [ip-0A0C0434:62834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.335717] [ip-0A0C0457:54367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.336363] [ip-0A0C047A:49176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.336915] [ip-0A0C043B:59601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.337674] [ip-0A0C041F:77989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.338299] [ip-0A0C041C:80023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.338388] [ip-0A0C0462:85688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.339411] [ip-0A0C0423:67750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.339099] [ip-0A0C0413:8609 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.340804] [ip-0A0C047B:50781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.340852] [ip-0A0C047C:54238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.341586] [ip-0A0C0433:68186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.342760] [ip-0A0C0416:61160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.342689] [ip-0A0C047B:50785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.343071] [ip-0A0C044E:51342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.343363] [ip-0A0C0427:71002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.343409] [ip-0A0C0453:59895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.343961] [ip-0A0C041D:70690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.345970] [ip-0A0C0446:62071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.345265] [ip-0A0C042B:72065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.345517] [ip-0A0C0419:78430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.345454] [ip-0A0C0421:76089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.345768] [ip-0A0C042B:72064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.346449] [ip-0A0C0419:78420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.346707] [ip-0A0C0456:61844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.346559] [ip-0A0C047D:44677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.346806] [ip-0A0C043F:55043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.347581] [ip-0A0C041A:97644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.348045] [ip-0A0C044C:56200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.348389] [ip-0A0C0480:43516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.349718] [ip-0A0C041B:92041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.349721] [ip-0A0C0445:64645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.351285] [ip-0A0C041E:74012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.352879] [ip-0A0C045E:55576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.352650] [ip-0A0C044F:52020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.352813] [ip-0A0C042B:72067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.353099] [ip-0A0C0450:29810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.353233] [ip-0A0C0480:43512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.353338] [ip-0A0C0475:40570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.353856] [ip-0A0C044E:51338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.353832] [ip-0A0C0422:5124 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.354076] [ip-0A0C0422:5127 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.354901] [ip-0A0C0455:52992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.355792] [ip-0A0C042C:67415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.355887] [ip-0A0C042C:67417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.356625] [ip-0A0C0432:98142:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.356490] [ip-0A0C0440:53393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.356947] [ip-0A0C0476:34347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.358348] [ip-0A0C045E:55574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.358997] [ip-0A0C044C:56201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.358698] [ip-0A0C045C:54990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.359209] [ip-0A0C041B:92036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.359588] [ip-0A0C0480:43517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.359570] [ip-0A0C0475:40572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.359976] [ip-0A0C0479:44928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.360527] [ip-0A0C0456:61841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.360899] [ip-0A0C041E:74008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.361168] [ip-0A0C0477:49009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.361475] [ip-0A0C0453:59896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.361872] [ip-0A0C042B:72066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.362520] [ip-0A0C0432:98140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.362583] [ip-0A0C0463:48742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.362478] [ip-0A0C044C:56202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.362266] [ip-0A0C0453:59900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.362648] [ip-0A0C0424:62861:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.363569] [ip-0A0C0424:62863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.363670] [ip-0A0C0471:43706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.365748] [ip-0A0C0470:39103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.365748] [ip-0A0C044B:57538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.366216] [ip-0A0C0463:48739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.366803] [ip-0A0C0454:58737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.367279] [ip-0A0C0463:48745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.367113] [ip-0A0C041F:77990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.367636] [ip-0A0C041C:80022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.368655] [ip-0A0C0416:61166:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.368833] [ip-0A0C047F:47502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.369338] [ip-0A0C0426:72677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.369593] [ip-0A0C0453:59901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.370812] [ip-0A0C041E:74010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.370768] [ip-0A0C0469:43740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.370789] [ip-0A0C0469:43743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.372411] [ip-0A0C0432:98147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.371701] [ip-0A0C042F:69268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.373993] [ip-0A0C047A:49178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.374043] [ip-0A0C045C:54988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376093] [ip-0A0C043F:55044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376318] [ip-0A0C0465:46250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376402] [ip-0A0C0433:68180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376556] [ip-0A0C0433:68183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376436] [ip-0A0C0465:46247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376884] [ip-0A0C0419:78421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.376704] [ip-0A0C044E:51340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.377554] [ip-0A0C0456:61838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.377432] [ip-0A0C0455:52995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.377741] [ip-0A0C0418:70891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.379723] [ip-0A0C0424:62864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.379800] [ip-0A0C044F:52024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.379948] [ip-0A0C044E:51341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.380351] [ip-0A0C0440:53391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.380965] [ip-0A0C0456:61839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.380680] [ip-0A0C042F:69270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.380689] [ip-0A0C041F:77988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.381673] [ip-0A0C0420:75210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383357] [ip-0A0C0414:90659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383054] [ip-0A0C045E:55572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383195] [ip-0A0C045B:62125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.382482] [ip-0A0C042F:69263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383433] [ip-0A0C0455:52996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383781] [ip-0A0C047A:49172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383726] [ip-0A0C0440:53386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.384461] [ip-0A0C045E:55573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.383803] [ip-0A0C046A:49317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.386081] [ip-0A0C0414:90664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.384547] [ip-0A0C041F:77987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.385763] [ip-0A0C0458:62229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.386029] [ip-0A0C0434:62836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.386265] [ip-0A0C041B:92064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.386410] [ip-0A0C041B:92038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.386201] [ip-0A0C0412:53553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.387236] [ip-0A0C0465:46246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.388439] [ip-0A0C0414:90665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.387320] [ip-0A0C0477:49011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.388062] [ip-0A0C0455:52993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.387729] [ip-0A0C0459:54285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.388500] [ip-0A0C044B:57537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.388949] [ip-0A0C045B:62128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.388805] [ip-0A0C045C:54992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.389422] [ip-0A0C0439:51329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.389243] [ip-0A0C041F:77992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.389514] [ip-0A0C044E:51335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.390148] [ip-0A0C043C:60338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.390214] [ip-0A0C044F:52018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.390364] [ip-0A0C0474:40122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.390459] [ip-0A0C041A:97649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.391318] [ip-0A0C0419:78424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.391072] [ip-0A0C045C:54993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.391420] [ip-0A0C0434:62831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.391728] [ip-0A0C0447:61042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.392432] [ip-0A0C043F:55042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.392347] [ip-0A0C0412:53551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393407] [ip-0A0C043C:60339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393539] [ip-0A0C044F:52023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393576] [ip-0A0C0475:40575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393788] [ip-0A0C0475:40571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393661] [ip-0A0C0440:53385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.393975] [ip-0A0C0418:70886:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.395026] [ip-0A0C0419:78417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.396253] [ip-0A0C0446:62070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.395895] [ip-0A0C044F:52025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.396439] [ip-0A0C0423:67753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.397272] [ip-0A0C0481:42307:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.397183] [ip-0A0C045B:62131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.400006] [ip-0A0C0465:46244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.401693] [ip-0A0C0446:62065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.400969] [ip-0A0C0476:34331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.401943] [ip-0A0C0433:68185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.401969] [ip-0A0C0433:68181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.402179] [ip-0A0C0416:61161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.402271] [ip-0A0C0416:61163:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.402527] [ip-0A0C0458:62223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.402778] [ip-0A0C0439:51356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.403230] [ip-0A0C0412:53550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.403505] [ip-0A0C0412:53555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.404285] [ip-0A0C0419:78418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.404461] [ip-0A0C041D:70688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.405475] [ip-0A0C0476:34330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.406410] [ip-0A0C0447:61038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.408547] [ip-0A0C045B:62124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.409581] [ip-0A0C0450:29811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.410176] [ip-0A0C0459:54284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.410612] [ip-0A0C043C:60340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.411025] [ip-0A0C047F:47509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.411222] [ip-0A0C0419:78426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.411928] [ip-0A0C0416:61162:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.413238] [ip-0A0C047A:49174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.413001] [ip-0A0C0476:34328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.413732] [ip-0A0C0407:78787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.414563] [ip-0A0C0448:66390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.414402] [ip-0A0C0413:8610 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.416374] [ip-0A0C0425:77159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.417137] [ip-0A0C0450:29809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.417018] [ip-0A0C0459:54287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.417156] [ip-0A0C0421:76074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.417931] [ip-0A0C047A:49171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.418089] [ip-0A0C045B:62130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.418063] [ip-0A0C047A:49173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.418018] [ip-0A0C0465:46245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.418663] [ip-0A0C0470:39097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.418291] [ip-0A0C0420:75218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.419288] [ip-0A0C044F:52021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.419881] [ip-0A0C0423:67754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.419622] [ip-0A0C0452:57909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.420303] [ip-0A0C0450:29813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.419735] [ip-0A0C0422:5126 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.420150] [ip-0A0C0420:75217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.420020] [ip-0A0C0462:85690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.420243] [ip-0A0C0420:75214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.421311] [ip-0A0C0451:56815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.421669] [ip-0A0C0445:64646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.422471] [ip-0A0C0423:67752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.421966] [ip-0A0C0425:77152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.421989] [ip-0A0C0418:70892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.422235] [ip-0A0C0422:5123 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.423026] [ip-0A0C0445:64647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.424171] [ip-0A0C0446:62069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.423342] [ip-0A0C0458:62227:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.423708] [ip-0A0C044B:57540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.424500] [ip-0A0C0434:62837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.426270] [ip-0A0C043E:47968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.427369] [ip-0A0C0423:67757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.426868] [ip-0A0C043E:47942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.428471] [ip-0A0C0446:62067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.428484] [ip-0A0C0471:43685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.428838] [ip-0A0C0447:61037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.430295] [ip-0A0C047F:47503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.430124] [ip-0A0C041C:80027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.430132] [ip-0A0C0427:71020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.430442] [ip-0A0C0448:66391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.430819] [ip-0A0C0447:61035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.431184] [ip-0A0C0428:62896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.431309] [ip-0A0C0454:58732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.431071] [ip-0A0C0418:70887:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.431431] [ip-0A0C0451:56813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.431556] [ip-0A0C0418:70890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.432182] [ip-0A0C0447:61040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.432310] [ip-0A0C044B:57535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.432229] [ip-0A0C047D:44679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.432992] [ip-0A0C043C:60342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.433031] [ip-0A0C045D:45614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.433368] [ip-0A0C0454:58736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.433724] [ip-0A0C044B:57536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.434056] [ip-0A0C047D:44673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.435493] [ip-0A0C0481:42306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.435275] [ip-0A0C043A:59520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.436507] [ip-0A0C047F:47510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.437269] [ip-0A0C0446:62066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.436729] [ip-0A0C0450:29808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.437374] [ip-0A0C047F:47504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.437639] [ip-0A0C0450:29814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.437087] [ip-0A0C0422:5130 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.437920] [ip-0A0C0434:62833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.439679] [ip-0A0C0414:90657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.439827] [ip-0A0C0414:90660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.438668] [ip-0A0C0451:56817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.439827] [ip-0A0C0414:90661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.438478] [ip-0A0C0422:5125 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.439144] [ip-0A0C0452:57907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.439011] [ip-0A0C0445:64644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.440183] [ip-0A0C0434:62830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.440220] [ip-0A0C0459:54283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.441237] [ip-0A0C0425:77156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.441503] [ip-0A0C0445:64651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.441498] [ip-0A0C0434:62835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.441733] [ip-0A0C0451:56814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.442878] [ip-0A0C0475:40576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.443337] [ip-0A0C0428:62894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.443081] [ip-0A0C0458:62228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.443281] [ip-0A0C0475:40569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.443467] [ip-0A0C043C:60341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.443905] [ip-0A0C0458:62230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.444765] [ip-0A0C0462:85686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.445959] [ip-0A0C0475:40573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.446490] [ip-0A0C0474:40121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.447380] [ip-0A0C0421:76071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.448973] [ip-0A0C0518:72833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.448494] [ip-0A0C041A:97645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.450480] [ip-0A0C0407:78784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.450843] [ip-0A0C047D:44678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.453705] [ip-0A0C047C:54264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.453714] [ip-0A0C047C:54240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.454692] [ip-0A0C0407:78785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.454692] [ip-0A0C0407:78786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.455705] [ip-0A0C0518:72827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.455025] [ip-0A0C047D:44674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.455319] [ip-0A0C0462:85684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.456151] [ip-0A0C0481:42301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.458226] [ip-0A0C0452:57918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.458254] [ip-0A0C0448:66388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.460536] [ip-0A0C0479:44927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.460140] [ip-0A0C0425:77155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.461477] [ip-0A0C0518:72831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.461090] [ip-0A0C041C:80020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.461159] [ip-0A0C041C:80025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.460931] [ip-0A0C043A:59518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.461865] [ip-0A0C045D:45625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.462259] [ip-0A0C043C:60353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.463231] [ip-0A0C0470:39098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.464036] [ip-0A0C0471:43681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.465682] [ip-0A0C0518:72830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.464845] [ip-0A0C0413:8614 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.468899] [ip-0A0C0454:58731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.469620] [ip-0A0C041A:97647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.471474] [ip-0A0C0452:57911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.471790] [ip-0A0C0428:62895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.471928] [ip-0A0C0452:57904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.472415] [ip-0A0C0428:62892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.471844] [ip-0A0C0462:85689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.473336] [ip-0A0C0428:62900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.473242] [ip-0A0C041C:80021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.473544] [ip-0A0C047D:44676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.474646] [ip-0A0C0454:58738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.475566] [ip-0A0C0469:43738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.476207] [ip-0A0C0479:44931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.476264] [ip-0A0C0426:72680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.476270] [ip-0A0C0470:39100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.477031] [ip-0A0C0474:40124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.477373] [ip-0A0C0425:77158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.477692] [ip-0A0C0407:78781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.478728] [ip-0A0C0518:72832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.478141] [ip-0A0C0462:85683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.479719] [ip-0A0C0426:72674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.478824] [ip-0A0C046A:49316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.479497] [ip-0A0C041A:97650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.479569] [ip-0A0C0421:76069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.480002] [ip-0A0C045D:45613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.480559] [ip-0A0C0425:77153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.481174] [ip-0A0C0426:72681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.481713] [ip-0A0C0471:43686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.481823] [ip-0A0C041A:97648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.482805] [ip-0A0C0470:39102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.482478] [ip-0A0C0427:70997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.483208] [ip-0A0C0518:72834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.483371] [ip-0A0C0471:43682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.485091] [ip-0A0C047C:54237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.485391] [ip-0A0C0459:54282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.486252] [ip-0A0C045D:45615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.486466] [ip-0A0C041D:70691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.486423] [ip-0A0C0471:43684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.486826] [ip-0A0C0469:43739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.486929] [ip-0A0C0481:42300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.487100] [ip-0A0C0481:42303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.487484] [ip-0A0C0481:42302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.488493] [ip-0A0C0448:66419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.489396] [ip-0A0C045D:45616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.490853] [ip-0A0C0459:54286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.493123] [ip-0A0C0454:58734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.492915] [ip-0A0C0474:40119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.493350] [ip-0A0C043A:59517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.493943] [ip-0A0C043F:55045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.494966] [ip-0A0C043A:59516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.495492] [ip-0A0C0413:8613 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.496415] [ip-0A0C0479:44924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.496236] [ip-0A0C0448:66392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.496554] [ip-0A0C0479:44932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.496381] [ip-0A0C043E:47943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.496783] [ip-0A0C0427:71001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.497839] [ip-0A0C0454:58735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.497888] [ip-0A0C0479:44926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.498336] [ip-0A0C0459:54288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.499147] [ip-0A0C0439:51331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.499205] [ip-0A0C041D:70689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.499642] [ip-0A0C043A:59521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.502641] [ip-0A0C0470:39104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.502736] [ip-0A0C041D:70692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.504124] [ip-0A0C0426:72675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.503964] [ip-0A0C0477:49012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.505140] [ip-0A0C0426:72678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.506057] [ip-0A0C0413:8611 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.506954] [ip-0A0C043E:47945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.508050] [ip-0A0C041D:70694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.508991] [ip-0A0C041D:70695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.510038] [ip-0A0C0413:8615 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.510037] [ip-0A0C0413:8612 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.512611] [ip-0A0C0477:49008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.513118] [ip-0A0C043A:59522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.513370] [ip-0A0C046A:49343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.515172] [ip-0A0C0448:66395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.515637] [ip-0A0C0474:40126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.516554] [ip-0A0C0439:51330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.516681] [ip-0A0C047C:54236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.516449] [ip-0A0C043F:55041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.517172] [ip-0A0C0421:76073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.518361] [ip-0A0C0427:70999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.521282] [ip-0A0C0439:51333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.521645] [ip-0A0C0474:40120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.522479] [ip-0A0C0474:40123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.524211] [ip-0A0C0448:66393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.524754] [ip-0A0C046A:49319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.526227] [ip-0A0C0427:71000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.527005] [ip-0A0C043F:55040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.528862] [ip-0A0C043E:47946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.528959] [ip-0A0C0469:43737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.532779] [ip-0A0C047C:54239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.534342] [ip-0A0C0469:43742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.534713] [ip-0A0C0439:51334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.535556] [ip-0A0C0427:71005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.536479] [ip-0A0C0469:43741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.539062] [ip-0A0C0439:51336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.541144] [ip-0A0C0477:49014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.543172] [ip-0A0C046A:49320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.545123] [ip-0A0C047C:54235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.544595] [ip-0A0C046A:49322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.546172] [ip-0A0C0469:43744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.547955] [ip-0A0C047C:54241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.550584] [ip-0A0C043E:47944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.550910] [ip-0A0C043E:47947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.552803] [ip-0A0C0421:76070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.552923] [ip-0A0C0421:76067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.553339] [ip-0A0C043F:55064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.558815] [ip-0A0C0421:76068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.559245] [ip-0A0C046A:49334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.562924] [ip-0A0C043F:55046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.567315] [ip-0A0C0477:49013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.571366] [ip-0A0C0477:49010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624158.579090] [ip-0A0C0477:49007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634624159475, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634624159517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634624159518, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634624159518, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624159518, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634624159518, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634624159518, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:10] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:16:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:40812 - context.c:584] INFO job (ID: 867538203604189995) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40812 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40812 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40817 - context.c:584] INFO job (ID: 867538584975550971) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40817 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40817 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40808 - context.c:584] INFO job (ID: 867538759201155064) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40808 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40808 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40806 - context.c:584] INFO job (ID: 867538538759126592) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40806 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40806 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40810 - context.c:584] INFO job (ID: 867538612324146904) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40810 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40810 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40804 - context.c:584] INFO job (ID: 867538164760671740) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40804 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40804 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40805 - context.c:584] INFO job (ID: 867537956121160324) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40805 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40805 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:40819 - context.c:584] INFO job (ID: 867538099492632459) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:40819 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:40819 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251945, "event_type": "POINT_IN_TIME", "key": "seed", "value": 762122672, "metadata": {"file": "main.py", "lineno": 72}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251945, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251945, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251945, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251945, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624251946, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:17:33] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624275921, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634624275975, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624275980, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634624275981, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624278496, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634624278496, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634624278496, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624278497, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624280074, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2130.0946105038443, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624280074, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624280075, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2130.0946105038443, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634624280075, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624280075, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624280754, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4947.5058048543815, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624280754, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624280754, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4947.5058048543815, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624280754, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624280755, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624281423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5028.82743633616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624281423, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624281423, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5028.82743633616, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624281423, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624281423, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624282061, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5266.286370262473, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624282062, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624282062, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5266.286370262473, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634624282062, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624282062, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624282694, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5320.034352365425, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624282694, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624282694, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5320.034352365425, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634624282694, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624282695, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624283313, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5433.18707958215, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624283313, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624283314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5433.18707958215, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634624283314, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624283314, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624283934, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5421.532401024227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624283934, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624283934, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5421.532401024227, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624283934, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624283934, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624284554, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5419.703888126797, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624284554, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624284555, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5419.703888126797, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634624284555, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624284555, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624285172, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.570130484754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624285172, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624285172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.570130484754, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634624285173, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624285173, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624285806, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5306.4987608132315, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624285806, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624285806, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5306.4987608132315, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634624285806, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624285807, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624286429, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.933583890251, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624286429, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624286429, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.933583890251, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634624286429, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624286429, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624287047, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5441.068070581304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624287047, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624287047, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5441.068070581304, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634624287047, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624287047, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624287674, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.650124635908, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624287674, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624287675, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.650124635908, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634624287675, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624287675, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624288300, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5375.460698637488, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624288300, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624288300, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5375.460698637488, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634624288300, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624288300, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624288936, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5284.412176953242, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624288937, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624288937, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5284.412176953242, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634624288937, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624288937, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624289557, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5416.246223521413, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624289558, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624289558, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5416.246223521413, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634624289558, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624289558, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624290182, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5383.672653592614, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624290182, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624290183, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5383.672653592614, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634624290183, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624290183, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624290803, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5416.427328917542, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624290804, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624290804, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5416.427328917542, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634624290804, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624290804, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624291430, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.888195919859, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624291430, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624291431, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.888195919859, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634624291431, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624291431, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624292063, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.232815747093, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624292063, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624292063, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.232815747093, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634624292064, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624292064, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624292696, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5318.323826574068, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624292696, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624292696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5318.323826574068, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634624292696, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624292696, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624293322, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5368.65389955989, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624293322, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624293323, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5368.65389955989, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634624293323, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624293323, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624293966, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5225.246736181583, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624293966, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624293966, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5225.246736181583, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634624293967, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624293967, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624294591, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5384.343201478731, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624294591, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624294591, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5384.343201478731, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634624294591, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624294591, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624295218, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.259002972564, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624295218, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624295218, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.259002972564, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634624295218, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624295219, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624295845, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5368.430984147107, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624295845, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624295845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5368.430984147107, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634624295845, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624295845, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624296466, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5415.534409328968, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624296466, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624296466, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5415.534409328968, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634624296466, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624296466, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624297086, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.101847872632, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624297087, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624297087, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.101847872632, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634624297087, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624297087, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624297712, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.464403595622, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624297713, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624297713, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.464403595622, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634624297713, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624297713, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624298340, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5357.987022541178, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624298340, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624298340, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5357.987022541178, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634624298341, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624298341, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624298956, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5458.213423549209, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624298957, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624298957, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5458.213423549209, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634624298957, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624298957, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624299580, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5392.761865111132, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624299580, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624299580, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5392.761865111132, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634624299581, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624299581, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624300197, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5449.422857408009, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624300198, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624300198, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5449.422857408009, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634624300198, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624300198, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624300826, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5349.875520121355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624300826, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624300826, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5349.875520121355, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634624300826, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624300827, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624301441, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.46181565563, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624301441, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624301441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.46181565563, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634624301441, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624301441, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624302063, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5410.168529577517, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624302063, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624302063, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5410.168529577517, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634624302063, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624302063, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624302695, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.446906130839, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624302695, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624302696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.446906130839, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634624302696, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624302696, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624303317, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.587159909668, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624303317, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624303317, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.587159909668, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634624303317, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624303317, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624303946, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5343.394660571858, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624303947, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624303947, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5343.394660571858, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634624303947, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624303947, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624304572, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.0166741908, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624304572, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624304572, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.0166741908, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634624304572, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624304572, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624305200, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5354.833471514954, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624305200, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624305200, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5354.833471514954, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634624305200, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624305201, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624305822, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5409.526833520587, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624305822, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624305822, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5409.526833520587, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634624305822, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624305822, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624306444, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5402.672894516308, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624306445, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624306445, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5402.672894516308, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634624306445, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624306445, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624307064, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.407215791587, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624307064, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624307064, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.407215791587, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634624307064, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624307064, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624307682, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5441.339077561755, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624307682, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624307682, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5441.339077561755, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634624307683, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624307683, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624308300, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5443.438720445213, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624308300, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624308300, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5443.438720445213, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634624308301, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624308301, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624308920, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.1892050183405, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624308920, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624308920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.1892050183405, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634624308920, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624308920, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624309546, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.338518093619, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624309547, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624309547, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.338518093619, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634624309547, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624309547, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624310171, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5383.516353551483, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624310172, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624310172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5383.516353551483, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634624310172, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624310172, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624310788, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5461.55979775017, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624310788, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624310788, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5461.55979775017, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634624310860, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624310861, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624310877, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624311304, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.884188175201416, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624311304, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624311474, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5484.517047975305, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624311474, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624311474, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5484.517047975305, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624311626, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624311627, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624311642, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624312072, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8881796598434448, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624312072, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624312312, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4902.349115530021, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624312313, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624312313, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4902.349115530021, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624312381, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624312381, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624312396, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624312807, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8744173049926758, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624312807, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624313011, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.583658968595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624313012, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624313012, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.583658968595, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624313047, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624313048, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624313062, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624313460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910894393920898, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624313460, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624313663, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5461.627529074645, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624313663, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624313663, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5461.627529074645, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624313699, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624313700, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624313713, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624314112, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8859608173370361, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624314112, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624314310, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.624445502335, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624314311, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624314311, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.624445502335, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624314347, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624314347, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624314363, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624314763, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8901519775390625, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624314763, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624314962, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5465.153993754935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624314963, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624314963, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5465.153993754935, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624314998, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624314999, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624315012, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624315411, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8799548149108887, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624315411, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624315602, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5566.872826931083, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624315603, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624315603, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5566.872826931083, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624315647, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624315647, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624315660, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624316059, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8753482103347778, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624316059, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624316252, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5553.0207539936, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624316253, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624316253, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5553.0207539936, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624316282, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624316282, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624316297, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624316716, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8934862017631531, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624316716, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624316913, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5324.068076811194, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624316914, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624316914, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5324.068076811194, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624316950, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624316950, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624316964, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624317363, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8713008761405945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624317363, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624317560, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5509.170942346055, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624317561, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624317561, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5509.170942346055, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624317604, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624317604, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624317619, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624318027, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8757948279380798, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624318027, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624318226, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5402.028833847169, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624318227, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624318227, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5402.028833847169, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624318263, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624318264, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624318276, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624318677, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8792909383773804, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624318677, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624318880, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5455.696828595828, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624318880, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624318880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5455.696828595828, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624318928, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624318928, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624318942, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624319342, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895512819290161, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624319342, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624319536, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5524.759909912559, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624319537, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624319537, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5524.759909912559, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624319575, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624319575, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624319589, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624319988, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8942793011665344, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624319988, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624320184, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5513.997168830214, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624320185, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624320185, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5513.997168830214, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624320220, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624320220, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624320234, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624320634, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8648215532302856, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624320634, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624320829, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5523.36329218107, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624320829, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624320829, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5523.36329218107, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624320864, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624320864, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624320879, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624321278, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8902527093887329, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624321278, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624321472, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5535.808283878631, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624321472, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624321472, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5535.808283878631, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624321510, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624321510, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624321525, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624321923, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8847770690917969, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624321923, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624322117, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5536.421565738827, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624322117, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624322117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5536.421565738827, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624322154, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624322154, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624322168, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624322567, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8882700800895691, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624322567, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624322761, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5537.53756529058, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624322761, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624322761, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5537.53756529058, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624322797, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624322797, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624322812, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624323212, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8840467929840088, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624323212, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624323409, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5493.2906744500415, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624323409, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624323409, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5493.2906744500415, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624323445, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624323445, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624323459, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624323858, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8738299608230591, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624323858, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624324055, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5511.290940596905, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624324055, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624324056, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5511.290940596905, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624324092, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624324092, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624324107, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624324506, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8913512825965881, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624324506, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624324711, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5429.339850814004, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624324711, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624324712, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5429.339850814004, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624324746, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624324747, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624324761, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624325161, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8829972743988037, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624325161, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624325354, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5533.202264814998, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624325354, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624325354, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5533.202264814998, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624325390, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624325390, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624325404, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624325804, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8683350086212158, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624325804, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624326000, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5506.759920021663, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624326000, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624326001, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5506.759920021663, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624326037, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624326037, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624326052, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624326451, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8952264785766602, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624326451, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624326640, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5572.741516838204, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624326641, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624326641, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5572.741516838204, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624326708, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624326708, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624326723, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624327122, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.854966402053833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624327122, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624327325, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.316811932059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624327326, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624327326, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.316811932059, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624327362, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624327362, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624327377, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624327776, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8717581629753113, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624327776, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624327969, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5539.309295227688, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624327969, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624327969, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5539.309295227688, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624328006, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624328007, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624328022, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624328421, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8943770527839661, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624328421, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624328616, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5518.924791819395, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624328616, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624328616, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5518.924791819395, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624328651, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624328651, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624328667, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624329065, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8951560258865356, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624329065, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624329308, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5119.269077574417, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624329308, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624329308, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5119.269077574417, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624329344, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624329344, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624329359, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624329758, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8906692266464233, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624329758, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624329970, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.207938590931, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624329971, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624329971, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.207938590931, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624330006, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624330007, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624330021, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624330421, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8943609595298767, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624330421, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624330630, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5389.021110425352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624330631, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624330631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5389.021110425352, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624330666, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624330666, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624330681, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624331081, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8931604623794556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624331081, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624331275, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5516.988969828315, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624331276, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624331276, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5516.988969828315, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624331311, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624331311, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624331327, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624331725, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8992213010787964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624331726, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624331920, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5524.766407459155, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624331920, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624331920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5524.766407459155, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624331957, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624331957, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624331972, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624332371, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9035252332687378, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624332371, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624332571, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5470.291919973729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624332571, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624332572, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5470.291919973729, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624332608, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624332608, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624332623, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624333023, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8900227546691895, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624333023, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624333218, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5513.0329725096735, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624333218, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624333219, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5513.0329725096735, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624333255, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624333255, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624333270, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624333669, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9031543135643005, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624333669, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624333858, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5571.360183497337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624333859, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624333859, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5571.360183497337, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624333894, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624333894, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624333910, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624334309, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8873993158340454, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624334309, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624334509, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5470.160275369947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624334509, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624334509, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5470.160275369947, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624334545, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624334545, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624334560, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624334959, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8905544281005859, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624334959, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624335148, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5572.6930374065705, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624335148, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624335148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5572.6930374065705, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624335184, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624335185, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624335200, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624335599, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8979411721229553, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624335599, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624335796, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.2077558330375, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624335797, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624335797, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.2077558330375, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624335832, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624335832, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624335847, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624336247, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8945643901824951, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624336247, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624336436, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5565.0416701054455, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624336436, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624336436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5565.0416701054455, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624336473, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624336473, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624336487, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624336887, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8974566459655762, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624336887, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624337080, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5537.276472453419, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624337080, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624337080, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5537.276472453419, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624337115, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624337115, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624337130, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624337549, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8988438844680786, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624337550, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624337748, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5314.920103817894, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624337748, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624337748, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5314.920103817894, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624337785, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624337785, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624337800, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624338207, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9071172475814819, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624338207, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624338404, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.347515848593, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624338404, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624338404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.347515848593, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624338440, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624338440, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624338457, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624338854, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9016180038452148, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624338855, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624339042, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5581.508103227033, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624339042, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624339042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5581.508103227033, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624339078, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624339078, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624339094, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624339493, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9026120901107788, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624339493, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624339689, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5500.930534114263, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624339690, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624339690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5500.930534114263, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624339727, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624339727, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624339742, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624340141, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8905200958251953, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624340142, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624340332, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5556.030968682017, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624340332, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624340332, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5556.030968682017, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624340368, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624340368, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624340384, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624340782, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8931410312652588, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624340782, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624340970, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5588.907830298262, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624340970, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624340970, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5588.907830298262, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624341028, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624341029, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624341043, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634624341452, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9027707576751709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634624341452, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634624341653, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5380.499074543533, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624341654, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624341654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5380.499074543533, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634624341692, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624341693, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624341707, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634624342107, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011527299880981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634624342107, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634624342306, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5479.218166626945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624342307, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624342307, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5479.218166626945, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634624342343, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624342344, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624342359, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634624342759, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9084054231643677, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634624342759, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634624342759, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634624342949, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5550.094671139972, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624342950, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624342950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5550.094671139972, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:09 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
ENDING TIMING RUN AT 2021-10-19 06:19:12 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:13 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:14 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:15 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:16 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:17 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:18 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:19 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:20 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:21 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
ENDING TIMING RUN AT 2021-10-19 06:19:22 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:15:53 AM
