+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019052807859608138
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019052807859608138
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019052807859608138
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019052807859608138
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07382/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019052807859608138_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04BD
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C044D
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C04BE
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04DA
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C04CC
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C04C4
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:28:17 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634621302.314394] [ip-0A0C0436:31479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.335627] [ip-0A0C0466:33827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.345469] [ip-0A0C0466:33809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.356353] [ip-0A0C047E:28230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.360063] [ip-0A0C0490:17367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.369662] [ip-0A0C044D:28712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.371984] [ip-0A0C046B:16959:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.372011] [ip-0A0C046B:16983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.373032] [ip-0A0C046B:16960:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.375118] [ip-0A0C0436:31482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.378255] [ip-0A0C0461:92764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.386031] [ip-0A0C04AE:21665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.386019] [ip-0A0C04AE:21640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.388084] [ip-0A0C04C8:41709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.392188] [ip-0A0C0498:26045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.394164] [ip-0A0C048B:13992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.394912] [ip-0A0C0436:31480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.398457] [ip-0A0C049D:10882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.399156] [ip-0A0C040F:870  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.401103] [ip-0A0C048C:12499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.403435] [ip-0A0C04BE:49068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.405994] [ip-0A0C0436:31475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.406132] [ip-0A0C0498:26043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.410438] [ip-0A0C0489:31470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.411163] [ip-0A0C04A4:16565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.412644] [ip-0A0C04AF:8056 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.415389] [ip-0A0C04BE:49065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.415511] [ip-0A0C049C:2983 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.419605] [ip-0A0C04CC:41125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.419592] [ip-0A0C04C2:71049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.421425] [ip-0A0C04C2:71048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.421691] [ip-0A0C0497:63595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.422201] [ip-0A0C04C5:50663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.422614] [ip-0A0C0473:29747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.425878] [ip-0A0C04BB:88639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.427651] [ip-0A0C042D:2182 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.427714] [ip-0A0C044D:28714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.428894] [ip-0A0C0488:29315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.430084] [ip-0A0C048C:12500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.429863] [ip-0A0C044D:28710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.430460] [ip-0A0C04C5:50661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.431387] [ip-0A0C04CC:41121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.433785] [ip-0A0C0497:63601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.434524] [ip-0A0C04A4:16567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.435671] [ip-0A0C04C8:41704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.435796] [ip-0A0C0482:18920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.436179] [ip-0A0C04B0:2368 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.438497] [ip-0A0C04DA:34818:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.441067] [ip-0A0C049D:10880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.441449] [ip-0A0C0498:26042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.442718] [ip-0A0C0491:13547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.444978] [ip-0A0C0489:31467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.449712] [ip-0A0C0461:92769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.449896] [ip-0A0C0466:33807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.450320] [ip-0A0C0490:17365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.453046] [ip-0A0C047E:28224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.453267] [ip-0A0C047E:28225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.453175] [ip-0A0C04AD:90858:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.454037] [ip-0A0C04C0:47870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.456817] [ip-0A0C0461:92766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.457261] [ip-0A0C04C8:41710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.460884] [ip-0A0C046B:16974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.461765] [ip-0A0C04CF:71975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.462932] [ip-0A0C0491:13548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.462603] [ip-0A0C04DB:33509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.464737] [ip-0A0C042D:2183 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.465005] [ip-0A0C042D:2178 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.466356] [ip-0A0C0496:95455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.468829] [ip-0A0C047E:28226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.468515] [ip-0A0C048B:13994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.469175] [ip-0A0C048B:13996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.471406] [ip-0A0C04B2:89746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.471956] [ip-0A0C0486:31122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.472208] [ip-0A0C04A4:16570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.476253] [ip-0A0C04A2:7643 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.473793] [ip-0A0C04C7:42570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.473478] [ip-0A0C049C:2984 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.474541] [ip-0A0C04AF:8051 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.474575] [ip-0A0C048C:12503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.480421] [ip-0A0C046B:16957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.480353] [ip-0A0C04CD:37946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.480334] [ip-0A0C04CD:37949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.482044] [ip-0A0C04A0:5559 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.481732] [ip-0A0C04B7:84425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.482037] [ip-0A0C04BB:88643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.483020] [ip-0A0C048A:27952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.483307] [ip-0A0C044D:28713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.484952] [ip-0A0C04BB:88645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.485707] [ip-0A0C040F:869  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.486113] [ip-0A0C04AA:88685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.490285] [ip-0A0C04AD:90865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.490928] [ip-0A0C0490:17364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.493384] [ip-0A0C04C9:40588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.497571] [ip-0A0C04A2:7642 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.494023] [ip-0A0C049D:10883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.495263] [ip-0A0C04B3:23874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.495076] [ip-0A0C04CF:71970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.496377] [ip-0A0C0486:31128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.497845] [ip-0A0C0473:29748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.498688] [ip-0A0C04A6:15882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.501108] [ip-0A0C04BD:46642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.501580] [ip-0A0C0499:17837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.503772] [ip-0A0C0473:29746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.505753] [ip-0A0C0483:30111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.505749] [ip-0A0C0490:17366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.505739] [ip-0A0C0483:30112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.505862] [ip-0A0C0490:17371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.506501] [ip-0A0C046B:16956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.507166] [ip-0A0C040F:864  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.507221] [ip-0A0C04AD:90862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.507356] [ip-0A0C046E:33127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.508127] [ip-0A0C049F:4216 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.508794] [ip-0A0C0443:92427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.513409] [ip-0A0C04AE:21638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.513263] [ip-0A0C04AA:88680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.513376] [ip-0A0C0495:15727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.514279] [ip-0A0C04AF:8055 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.514873] [ip-0A0C04C9:40587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.515384] [ip-0A0C04B9:49141:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.516331] [ip-0A0C04D4:37768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.517487] [ip-0A0C0482:18921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.517771] [ip-0A0C0497:63600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.518376] [ip-0A0C046B:16958:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.518381] [ip-0A0C0436:31481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.518741] [ip-0A0C0436:31478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.519789] [ip-0A0C0461:92762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.520304] [ip-0A0C0489:31466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.520755] [ip-0A0C0466:33815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.520773] [ip-0A0C048C:12501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.521047] [ip-0A0C0429:21675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.521904] [ip-0A0C0496:95449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.522774] [ip-0A0C0466:33808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.522565] [ip-0A0C04C6:44236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.523584] [ip-0A0C04C3:47329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.525985] [ip-0A0C0495:15724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.527384] [ip-0A0C04AC:9628 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.527447] [ip-0A0C04AC:9630 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.527487] [ip-0A0C04A9:15257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.530070] [ip-0A0C048B:13997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.530369] [ip-0A0C0461:92768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.530950] [ip-0A0C0443:92404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.531182] [ip-0A0C04C8:41705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.531477] [ip-0A0C04AE:21639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.531399] [ip-0A0C04B7:84424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.532196] [ip-0A0C046B:16961:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.532168] [ip-0A0C0466:33814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.531937] [ip-0A0C048D:17196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.532173] [ip-0A0C0464:31171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.532727] [ip-0A0C048C:12497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.533263] [ip-0A0C048A:27955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.534317] [ip-0A0C0482:18914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.534061] [ip-0A0C0498:26041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.534855] [ip-0A0C049C:2985 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.534896] [ip-0A0C04DB:33506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.535083] [ip-0A0C04A8:21675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.536382] [ip-0A0C0436:31477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.536485] [ip-0A0C0438:32075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.537200] [ip-0A0C04B3:23877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.537416] [ip-0A0C04B3:23871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.537040] [ip-0A0C04B9:49143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.537557] [ip-0A0C04A9:15265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.538779] [ip-0A0C042D:2180 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.539481] [ip-0A0C0466:33811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.540085] [ip-0A0C0436:31476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.541355] [ip-0A0C0492:11020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.540821] [ip-0A0C0494:13183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.541855] [ip-0A0C04A4:16564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.541853] [ip-0A0C04B0:2369 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.542104] [ip-0A0C049F:4215 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.542853] [ip-0A0C0488:29311:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.543821] [ip-0A0C04DA:34814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.544134] [ip-0A0C04C0:47868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.544406] [ip-0A0C0466:33806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.544528] [ip-0A0C0437:27920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.545211] [ip-0A0C049D:10878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.546787] [ip-0A0C0490:17368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.546804] [ip-0A0C04D3:38992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.546862] [ip-0A0C04B5:16864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.547156] [ip-0A0C0461:92765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.546864] [ip-0A0C04B5:16863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.547488] [ip-0A0C04B1:19759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.548229] [ip-0A0C0496:95450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.549813] [ip-0A0C04C7:42564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.549651] [ip-0A0C040F:866  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.549539] [ip-0A0C0490:17370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.549484] [ip-0A0C044D:28708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.550282] [ip-0A0C04C2:71047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.550825] [ip-0A0C0486:31129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.550315] [ip-0A0C0498:26044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.550946] [ip-0A0C04DA:34816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.551397] [ip-0A0C04A6:15878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.551516] [ip-0A0C048B:13998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.551912] [ip-0A0C0499:17836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.552068] [ip-0A0C0490:17372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.553208] [ip-0A0C0488:29313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.555359] [ip-0A0C04AE:21642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.555412] [ip-0A0C0437:27917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.555244] [ip-0A0C0449:28712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.555228] [ip-0A0C0449:28717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.556433] [ip-0A0C047E:28228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.556625] [ip-0A0C04A1:34121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.556229] [ip-0A0C0498:26048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.558015] [ip-0A0C04A5:18270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.558088] [ip-0A0C04B6:83056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.558983] [ip-0A0C04B1:19760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.559231] [ip-0A0C044D:28709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.561199] [ip-0A0C0441:24541:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.561583] [ip-0A0C0473:29752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.562034] [ip-0A0C0493:12997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.562021] [ip-0A0C0484:33181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.562992] [ip-0A0C0472:33170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.563717] [ip-0A0C04AE:21641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.564254] [ip-0A0C047E:28227:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.563713] [ip-0A0C0461:92767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.564280] [ip-0A0C04B4:25631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.565798] [ip-0A0C04B0:2371 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566038] [ip-0A0C04AE:21636:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566294] [ip-0A0C047E:28231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.565991] [ip-0A0C0435:59803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.565732] [ip-0A0C04BE:49071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566435] [ip-0A0C047E:28229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.565819] [ip-0A0C044D:28707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566365] [ip-0A0C04CC:41122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566573] [ip-0A0C0487:31057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566559] [ip-0A0C0487:31058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566891] [ip-0A0C0461:92763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.566987] [ip-0A0C0489:31469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.567797] [ip-0A0C04BE:49069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.568460] [ip-0A0C0488:29316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.568123] [ip-0A0C0497:63597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.568608] [ip-0A0C0489:31468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.568583] [ip-0A0C04CF:71969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.569354] [ip-0A0C044D:28711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.569791] [ip-0A0C046E:33126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.571224] [ip-0A0C048B:13995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.572002] [ip-0A0C04A7:90511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.571402] [ip-0A0C04B2:89747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.571860] [ip-0A0C04AE:21637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.572248] [ip-0A0C04D4:37764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.572302] [ip-0A0C04AF:8050 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.572192] [ip-0A0C046C:26938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.573266] [ip-0A0C04A6:15883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.573256] [ip-0A0C04C0:47867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.573781] [ip-0A0C0443:92407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.574172] [ip-0A0C0496:95445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.575818] [ip-0A0C040F:868  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.579367] [ip-0A0C04C5:50657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.579554] [ip-0A0C04C5:50660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.580626] [ip-0A0C04CC:41120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.580401] [ip-0A0C0494:13180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.580762] [ip-0A0C04C8:41703:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.580965] [ip-0A0C04C8:41707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.581241] [ip-0A0C0485:93603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.581178] [ip-0A0C046E:33129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.582205] [ip-0A0C04C2:71057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.581927] [ip-0A0C0498:26049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.583411] [ip-0A0C0493:13003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.583461] [ip-0A0C04CD:37948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.583526] [ip-0A0C04CD:37947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.584439] [ip-0A0C04B0:2367 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.584170] [ip-0A0C04BC:44524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.584659] [ip-0A0C048B:13993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.584858] [ip-0A0C04DA:34817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.585331] [ip-0A0C0429:21673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.585816] [ip-0A0C048A:27956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.585489] [ip-0A0C0429:21671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587344] [ip-0A0C040F:863  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587887] [ip-0A0C0491:13549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587223] [ip-0A0C04BE:49070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587401] [ip-0A0C049D:10879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587497] [ip-0A0C04D9:69797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587488] [ip-0A0C04D9:69784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587499] [ip-0A0C04D9:69783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.587957] [ip-0A0C049C:2982 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.588703] [ip-0A0C04C8:41708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.588449] [ip-0A0C04AB:6790 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.588496] [ip-0A0C0460:30846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.588771] [ip-0A0C04B9:49145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.589329] [ip-0A0C048C:12496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.589591] [ip-0A0C0435:59802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.589910] [ip-0A0C049D:10881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.590439] [ip-0A0C0464:31170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.590312] [ip-0A0C049D:10884:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.590583] [ip-0A0C049C:2987 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.591002] [ip-0A0C04C8:41706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.592620] [ip-0A0C04B2:89752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.593267] [ip-0A0C0491:13550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.592767] [ip-0A0C04C3:47332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.592721] [ip-0A0C049D:10877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.592888] [ip-0A0C04C9:40585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.592931] [ip-0A0C0498:26046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.593431] [ip-0A0C04C2:71073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.593942] [ip-0A0C04A0:5561 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.593811] [ip-0A0C04BE:49072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.594643] [ip-0A0C048C:12498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.595189] [ip-0A0C04D8:41033:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.595375] [ip-0A0C048C:12502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.595890] [ip-0A0C04AF:8053 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.596166] [ip-0A0C04A4:16591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.596090] [ip-0A0C048B:13999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.596593] [ip-0A0C04C2:71055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.596871] [ip-0A0C0482:18913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.597011] [ip-0A0C04D3:38989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.597496] [ip-0A0C04A1:34123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.597537] [ip-0A0C04CC:41123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.597742] [ip-0A0C04A1:34117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.598223] [ip-0A0C0495:15721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.598486] [ip-0A0C04BD:46651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.599692] [ip-0A0C040F:867  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.599290] [ip-0A0C0497:63599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.599852] [ip-0A0C04DB:33505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.600577] [ip-0A0C04CD:37952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.601207] [ip-0A0C0473:29753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.601157] [ip-0A0C04C5:50659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.601847] [ip-0A0C0499:17833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.601931] [ip-0A0C04CF:71973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.603768] [ip-0A0C04C0:47865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.604006] [ip-0A0C04CC:41118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.604075] [ip-0A0C04DB:33508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.605605] [ip-0A0C04C4:48812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.605865] [ip-0A0C04C3:47333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.606234] [ip-0A0C040F:865  :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.606341] [ip-0A0C0486:31135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.606352] [ip-0A0C0473:29751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.606822] [ip-0A0C046C:26939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.607424] [ip-0A0C04A0:5565 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.607259] [ip-0A0C04BB:88641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.606826] [ip-0A0C0497:63598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.607282] [ip-0A0C04A8:21674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.608779] [ip-0A0C04AF:8054 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.608967] [ip-0A0C04AF:8052 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.608926] [ip-0A0C04BE:49064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.609178] [ip-0A0C0482:18919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.609395] [ip-0A0C0482:18915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.610827] [ip-0A0C04A4:16566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.610978] [ip-0A0C04A4:16576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.611050] [ip-0A0C04C2:71050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.611367] [ip-0A0C04C5:50658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.611598] [ip-0A0C04AA:88674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.611685] [ip-0A0C04BD:46644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.612110] [ip-0A0C04A4:16568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.611836] [ip-0A0C049C:2981 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.613113] [ip-0A0C042D:2176 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.613576] [ip-0A0C04BE:49066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.614558] [ip-0A0C04A7:90515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.614755] [ip-0A0C0473:29749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.614855] [ip-0A0C042D:2175 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.619027] [ip-0A0C04A2:7639 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.615379] [ip-0A0C0443:92403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616353] [ip-0A0C04A7:90514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616364] [ip-0A0C042D:2177 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616120] [ip-0A0C04CF:71972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616257] [ip-0A0C04CF:71968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616573] [ip-0A0C04C5:50662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.616936] [ip-0A0C04BB:88640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.617183] [ip-0A0C0472:33165:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.617430] [ip-0A0C04AF:8049 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.617690] [ip-0A0C04D8:41039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.618071] [ip-0A0C0492:11023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.617937] [ip-0A0C04DA:34820:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.618501] [ip-0A0C04DA:34815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.619251] [ip-0A0C04C7:42567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.619240] [ip-0A0C0486:31126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.619563] [ip-0A0C04D8:41038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.620536] [ip-0A0C04CC:41124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.620670] [ip-0A0C048D:17201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.620810] [ip-0A0C04BB:88638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.620570] [ip-0A0C0489:31463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.620916] [ip-0A0C04C6:44234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.621262] [ip-0A0C042D:2181 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.621310] [ip-0A0C0488:29317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.621471] [ip-0A0C04B7:84430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.621922] [ip-0A0C0483:30113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.622024] [ip-0A0C04BB:88642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.621967] [ip-0A0C0489:31465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.622049] [ip-0A0C0449:28715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.622457] [ip-0A0C04BB:88644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623181] [ip-0A0C0491:13544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623163] [ip-0A0C04CC:41119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623233] [ip-0A0C0488:29309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623828] [ip-0A0C0485:93599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623545] [ip-0A0C04C2:71058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.623588] [ip-0A0C0489:31471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.624081] [ip-0A0C045F:29238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.624698] [ip-0A0C0472:33168:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629185] [ip-0A0C04A2:7641 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.625007] [ip-0A0C0482:18916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.626207] [ip-0A0C04C5:50664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.626736] [ip-0A0C0488:29312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.626794] [ip-0A0C049C:2988 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.626814] [ip-0A0C0497:63594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.627990] [ip-0A0C04C7:42568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.628346] [ip-0A0C0496:95451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.628965] [ip-0A0C0441:24539:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629135] [ip-0A0C04C9:40586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629054] [ip-0A0C04D4:37767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629573] [ip-0A0C0496:95446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629288] [ip-0A0C049C:3015 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629252] [ip-0A0C04CF:71974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.629866] [ip-0A0C0488:29310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.630172] [ip-0A0C0497:63596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.630560] [ip-0A0C04CF:71971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.631064] [ip-0A0C0473:29750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.630942] [ip-0A0C0482:18918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.632161] [ip-0A0C04B0:2370 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.632399] [ip-0A0C04A0:5563 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.632407] [ip-0A0C04B2:89744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.632667] [ip-0A0C04A5:18268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.636362] [ip-0A0C0483:30115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.636764] [ip-0A0C04B6:83085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.637420] [ip-0A0C048D:17202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.637643] [ip-0A0C04C0:47869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.638229] [ip-0A0C04AD:90863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.638405] [ip-0A0C04AD:90859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.638567] [ip-0A0C04BD:46647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.640176] [ip-0A0C0464:31168:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.639634] [ip-0A0C04DB:33504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.640724] [ip-0A0C0464:31167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.640262] [ip-0A0C04DB:33503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.641337] [ip-0A0C04B3:23872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.642342] [ip-0A0C0486:31123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.642811] [ip-0A0C04B0:2375 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.643575] [ip-0A0C04A8:21673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.649751] [ip-0A0C04A2:7645 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.645906] [ip-0A0C04A5:18269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.645781] [ip-0A0C0499:17834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.646652] [ip-0A0C0491:13551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.646827] [ip-0A0C0491:13545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.646898] [ip-0A0C0491:13546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.646644] [ip-0A0C04B2:89748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.647288] [ip-0A0C04B4:25625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.647571] [ip-0A0C04B0:2366 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.647632] [ip-0A0C048A:27957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.647486] [ip-0A0C04A8:21671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.649742] [ip-0A0C04B7:84431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.650223] [ip-0A0C04DA:34813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.650761] [ip-0A0C04D9:69782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.651653] [ip-0A0C0429:21669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.652899] [ip-0A0C04B0:2372 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.653192] [ip-0A0C04AA:88673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.653561] [ip-0A0C04DA:34838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.653799] [ip-0A0C04C3:47328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.653768] [ip-0A0C04C6:44230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.653530] [ip-0A0C04BC:44523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.654469] [ip-0A0C04B4:25627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.654672] [ip-0A0C04C7:42565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.654494] [ip-0A0C048F:21002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.655084] [ip-0A0C04D3:38996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.656855] [ip-0A0C0496:95443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.656451] [ip-0A0C04AA:88675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.657273] [ip-0A0C04AD:90861:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.658214] [ip-0A0C04D4:37766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.658385] [ip-0A0C04B6:83061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.658742] [ip-0A0C04DB:33507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.664203] [ip-0A0C04A2:7644 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.660340] [ip-0A0C049B:4885 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.665230] [ip-0A0C04A2:7638 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.661093] [ip-0A0C04B2:89745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.662024] [ip-0A0C048A:27954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.662225] [ip-0A0C04AC:9626 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.662653] [ip-0A0C04B7:84426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.662761] [ip-0A0C04B5:16865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.663497] [ip-0A0C04C7:42563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.662904] [ip-0A0C04AD:90860:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.663023] [ip-0A0C04AD:90864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.663946] [ip-0A0C04C4:48814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.664046] [ip-0A0C04D3:38987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.664188] [ip-0A0C04DB:33510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.664843] [ip-0A0C049F:4218 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.665698] [ip-0A0C04AB:6791 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.666304] [ip-0A0C0483:30114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.666730] [ip-0A0C04A0:5562 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.666575] [ip-0A0C0438:32071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.667583] [ip-0A0C0496:95444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.667519] [ip-0A0C0486:31124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.667664] [ip-0A0C0486:31125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.667704] [ip-0A0C0429:21670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.668110] [ip-0A0C04BD:46646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.669684] [ip-0A0C046E:33124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.669565] [ip-0A0C04B9:49147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.669613] [ip-0A0C04B9:49146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.670494] [ip-0A0C04A0:5558 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.670375] [ip-0A0C0438:32074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.671691] [ip-0A0C0494:13182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.672089] [ip-0A0C04B2:89751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.672548] [ip-0A0C04C0:47872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.672724] [ip-0A0C0483:30109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.673073] [ip-0A0C048D:17197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.673255] [ip-0A0C045F:29234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.678436] [ip-0A0C04A2:7640 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.674451] [ip-0A0C04BA:81325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.674651] [ip-0A0C04B2:89765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.675410] [ip-0A0C0443:92402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.674884] [ip-0A0C0484:33184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.675827] [ip-0A0C04C0:47871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.675885] [ip-0A0C04C0:47866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.677480] [ip-0A0C0484:33185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.679512] [ip-0A0C04B3:23870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.679829] [ip-0A0C0437:27922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.680068] [ip-0A0C04D3:38993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.680569] [ip-0A0C04CD:37945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.680696] [ip-0A0C04C4:48818:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.680870] [ip-0A0C04CD:37951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.681641] [ip-0A0C04C7:42569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.681714] [ip-0A0C04C7:42566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.681598] [ip-0A0C04CD:37950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.681988] [ip-0A0C0438:32070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.682126] [ip-0A0C04C3:47331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.682229] [ip-0A0C04AA:88672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.683618] [ip-0A0C0437:27923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.684169] [ip-0A0C0441:24546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.684151] [ip-0A0C0499:17832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.684900] [ip-0A0C04AC:9632 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.684820] [ip-0A0C0460:30850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.685405] [ip-0A0C04B7:84428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.685442] [ip-0A0C049F:4221 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.686169] [ip-0A0C04AA:88677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.686437] [ip-0A0C04AA:88676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.686493] [ip-0A0C04D4:37765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.687257] [ip-0A0C0492:11017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.687375] [ip-0A0C0443:92401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.687783] [ip-0A0C0493:12995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.689228] [ip-0A0C04A6:15876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.689238] [ip-0A0C04BD:46645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.689950] [ip-0A0C04C9:40584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.690045] [ip-0A0C04B1:19762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.690282] [ip-0A0C04C9:40590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.690124] [ip-0A0C04B9:49140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.690029] [ip-0A0C04C6:44233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.690813] [ip-0A0C04C4:48811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.691131] [ip-0A0C04C9:40583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.692807] [ip-0A0C04BD:46643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.693048] [ip-0A0C04C9:40589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.692800] [ip-0A0C0499:17831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.693141] [ip-0A0C0460:30843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.693424] [ip-0A0C04A0:5560 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.693602] [ip-0A0C04A6:15880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.693429] [ip-0A0C0499:17835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.694424] [ip-0A0C0495:15726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.694665] [ip-0A0C0435:59804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.694478] [ip-0A0C04C3:47334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.694865] [ip-0A0C048A:27958:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.694946] [ip-0A0C04B3:23873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.695129] [ip-0A0C048A:27953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.695927] [ip-0A0C04A0:5564 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.695820] [ip-0A0C048F:21007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.696120] [ip-0A0C04D9:69781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.696911] [ip-0A0C0443:92406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.696733] [ip-0A0C0499:17839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.696878] [ip-0A0C049F:4219 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.698142] [ip-0A0C04B3:23875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.698983] [ip-0A0C04A6:15877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.699240] [ip-0A0C04BD:46649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.699684] [ip-0A0C0483:30110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.700680] [ip-0A0C0483:30128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.700966] [ip-0A0C0443:92405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.700484] [ip-0A0C04C6:44231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.701208] [ip-0A0C048A:27951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.701528] [ip-0A0C04B7:84432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.702398] [ip-0A0C0492:11016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.702744] [ip-0A0C0460:30847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.702754] [ip-0A0C0449:28716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.704423] [ip-0A0C04A6:15909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.704594] [ip-0A0C0435:59807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.704615] [ip-0A0C04B7:84427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.705505] [ip-0A0C04AB:6793 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.705886] [ip-0A0C0487:31053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.706928] [ip-0A0C0472:33172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.707473] [ip-0A0C04B5:16869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.707308] [ip-0A0C04BC:44530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.707691] [ip-0A0C04B9:49142:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.708413] [ip-0A0C0495:15722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.708435] [ip-0A0C04B5:16867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.709438] [ip-0A0C04B3:23876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.709499] [ip-0A0C04D4:37763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.709608] [ip-0A0C0487:31051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.709553] [ip-0A0C04B9:49144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.709672] [ip-0A0C04B6:83057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.710459] [ip-0A0C048D:17195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.710184] [ip-0A0C04A8:21681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.711186] [ip-0A0C04A6:15879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.710412] [ip-0A0C04A9:15259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.710479] [ip-0A0C04A9:15261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.710904] [ip-0A0C049F:4220 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.712073] [ip-0A0C046E:33123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.713021] [ip-0A0C049F:4217 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.713457] [ip-0A0C046E:33125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.713255] [ip-0A0C049F:4222 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.712935] [ip-0A0C0484:33179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.713627] [ip-0A0C04D4:37762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.715509] [ip-0A0C04B1:19763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.715518] [ip-0A0C04BA:81324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.716232] [ip-0A0C0438:32076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.716290] [ip-0A0C0437:27916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.715930] [ip-0A0C04A9:15258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.717709] [ip-0A0C04AC:9625 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.718028] [ip-0A0C04AC:9631 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.718157] [ip-0A0C0495:15720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.718445] [ip-0A0C04D4:37769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.718494] [ip-0A0C045F:29232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.718243] [ip-0A0C049B:4882 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.720349] [ip-0A0C0464:31172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.722545] [ip-0A0C04A8:21670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.722794] [ip-0A0C04B6:83059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.723211] [ip-0A0C04C4:48815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.723350] [ip-0A0C046E:33128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.723341] [ip-0A0C04D9:69785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.723397] [ip-0A0C04C6:44229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.723869] [ip-0A0C048D:17200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.724118] [ip-0A0C0495:15723:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.724993] [ip-0A0C0441:24544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.725514] [ip-0A0C04A9:15262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.725746] [ip-0A0C0494:13177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.726123] [ip-0A0C04A5:18264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.726967] [ip-0A0C04A1:34118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.727205] [ip-0A0C0495:15725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.727196] [ip-0A0C0487:31055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.727099] [ip-0A0C0449:28711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.727702] [ip-0A0C04C6:44235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.727865] [ip-0A0C04C6:44232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.729772] [ip-0A0C0429:21676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.729777] [ip-0A0C0429:21672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.730214] [ip-0A0C0429:21674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.730506] [ip-0A0C04B6:83060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731575] [ip-0A0C04A7:90513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731579] [ip-0A0C0485:93600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731551] [ip-0A0C046E:33121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731249] [ip-0A0C04A8:21669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.732179] [ip-0A0C0438:32072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.732103] [ip-0A0C04C3:47330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731844] [ip-0A0C04A8:21672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.731969] [ip-0A0C04A9:15260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.732512] [ip-0A0C04C3:47337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.732335] [ip-0A0C0494:13178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.732681] [ip-0A0C04B5:16870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.733422] [ip-0A0C04D9:69787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.733834] [ip-0A0C0492:11022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.733837] [ip-0A0C0435:59808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.733940] [ip-0A0C046C:26943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.734301] [ip-0A0C04A1:34119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.734015] [ip-0A0C04D3:38990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.735430] [ip-0A0C04A7:90518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.736156] [ip-0A0C04D9:69805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.736196] [ip-0A0C0484:33180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.737262] [ip-0A0C04B4:25626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.737336] [ip-0A0C04B4:25630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.738321] [ip-0A0C0438:32077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.739077] [ip-0A0C04AC:9627 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.739258] [ip-0A0C04D3:38988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.739652] [ip-0A0C04A5:18263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.739558] [ip-0A0C04D3:38995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.740067] [ip-0A0C0464:31165:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.740097] [ip-0A0C0464:31169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.740343] [ip-0A0C0449:28710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.740764] [ip-0A0C04B5:16866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.741044] [ip-0A0C04B5:16868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.741991] [ip-0A0C04B6:83062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.743038] [ip-0A0C049B:4880 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.743669] [ip-0A0C04B1:19757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.743800] [ip-0A0C04B1:19764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.743614] [ip-0A0C0449:28713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.744238] [ip-0A0C04A1:34124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.744365] [ip-0A0C0493:13001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.744561] [ip-0A0C0493:13002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.744682] [ip-0A0C04A9:15256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.745106] [ip-0A0C0460:30845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.745484] [ip-0A0C0437:27918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.745573] [ip-0A0C04D8:41034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.745750] [ip-0A0C04D8:41040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.745653] [ip-0A0C04BC:44527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.747121] [ip-0A0C0492:11018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.747643] [ip-0A0C04A7:90512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.746776] [ip-0A0C04B6:83058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.748027] [ip-0A0C04BC:44529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.748872] [ip-0A0C0438:32073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.749074] [ip-0A0C0437:27919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.749403] [ip-0A0C0437:27921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.749395] [ip-0A0C0441:24542:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.749517] [ip-0A0C04AC:9629 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.750134] [ip-0A0C0492:11021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.749720] [ip-0A0C046C:26941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.751100] [ip-0A0C0441:24548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.751082] [ip-0A0C048D:17199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.751203] [ip-0A0C048D:17198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.750878] [ip-0A0C0449:28714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.752470] [ip-0A0C04A5:18267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.753119] [ip-0A0C0484:33183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.753840] [ip-0A0C046C:26937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.753791] [ip-0A0C0494:13175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.754222] [ip-0A0C0464:31166:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.755623] [ip-0A0C0494:13179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.756331] [ip-0A0C0492:11019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.756290] [ip-0A0C04AB:6787 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.756595] [ip-0A0C04A1:34120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.756758] [ip-0A0C04A1:34116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.757661] [ip-0A0C0485:93602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.757197] [ip-0A0C0494:13176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.757722] [ip-0A0C0493:12996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.759553] [ip-0A0C0487:31056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.759651] [ip-0A0C0487:31052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.759905] [ip-0A0C0487:31054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.760555] [ip-0A0C04A5:18265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.761261] [ip-0A0C04A7:90516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.760940] [ip-0A0C0460:30844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.761291] [ip-0A0C0484:33182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.762093] [ip-0A0C0435:59805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.763668] [ip-0A0C04A7:90517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.762967] [ip-0A0C049B:4884 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.764063] [ip-0A0C0441:24543:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.764143] [ip-0A0C0441:24540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.765318] [ip-0A0C04A5:18266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.766105] [ip-0A0C046C:26944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.766657] [ip-0A0C0435:59801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.767183] [ip-0A0C04D8:41035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.768664] [ip-0A0C0485:93601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.768217] [ip-0A0C048F:21003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.769296] [ip-0A0C04B4:25628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.769675] [ip-0A0C04B1:19761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.769937] [ip-0A0C04B4:25624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.770374] [ip-0A0C04B4:25629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.769906] [ip-0A0C0484:33186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.771227] [ip-0A0C0472:33171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.771287] [ip-0A0C0472:33169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.772188] [ip-0A0C04B1:19758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.772412] [ip-0A0C0472:33167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.773808] [ip-0A0C0485:93598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.773378] [ip-0A0C0493:12999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.773515] [ip-0A0C0493:12998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.775668] [ip-0A0C0472:33166:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.776653] [ip-0A0C0435:59806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.778611] [ip-0A0C046C:26940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.780799] [ip-0A0C04C4:48819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.781030] [ip-0A0C046C:26936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.783151] [ip-0A0C0485:93597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.782476] [ip-0A0C04BC:44528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.782794] [ip-0A0C04BA:81326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.783804] [ip-0A0C04C4:48817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.783451] [ip-0A0C04BC:44525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.783953] [ip-0A0C04C4:48816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.784718] [ip-0A0C0485:93604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.786193] [ip-0A0C04AB:6789 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.788086] [ip-0A0C0460:30849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.788326] [ip-0A0C0460:30848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.792619] [ip-0A0C045F:29239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.793704] [ip-0A0C04BC:44526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.795188] [ip-0A0C04D8:41036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.795108] [ip-0A0C045F:29233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.795374] [ip-0A0C04D8:41037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.799677] [ip-0A0C04AB:6788 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.803343] [ip-0A0C048F:21008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.804476] [ip-0A0C048F:21006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.804871] [ip-0A0C04AB:6786 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.804979] [ip-0A0C04AB:6792 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.810980] [ip-0A0C045F:29240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.825285] [ip-0A0C045F:29231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.827410] [ip-0A0C048F:21004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.830535] [ip-0A0C045F:29236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.836032] [ip-0A0C04BA:81323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.837182] [ip-0A0C048F:21005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.837657] [ip-0A0C04BA:81322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.842047] [ip-0A0C048F:21009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.846674] [ip-0A0C04BA:81330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.859825] [ip-0A0C049B:4881 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.861879] [ip-0A0C049B:4883 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.866788] [ip-0A0C04BA:81327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.867140] [ip-0A0C04BA:81328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.873288] [ip-0A0C049B:4886 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634621302.878445] [ip-0A0C049B:4887 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634621303790, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634621303832, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634621303833, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634621303833, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634621303833, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634621303833, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634621303834, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:34] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:35] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:36] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:36] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:36] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:36] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:28:36] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:59807 - context.c:584] INFO job (ID: 867564753836940266) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59807 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59807 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59804 - context.c:584] INFO job (ID: 867565166841470445) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59804 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59804 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59808 - context.c:584] INFO job (ID: 867564900950189566) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59808 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59808 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59806 - context.c:584] INFO job (ID: 867564177967563857) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59806 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59806 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59802 - context.c:584] INFO job (ID: 867564625607600003) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59802 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59802 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59801 - context.c:584] INFO job (ID: 867564761226645932) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59801 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59801 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59805 - context.c:584] INFO job (ID: 867564698800735518) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59805 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59805 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:59803 - context.c:584] INFO job (ID: 867564865976231662) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:59803 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:59803 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398494, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1513748780, "metadata": {"file": "main.py", "lineno": 72}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398494, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398494, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621398495, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:29:58] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:30:00] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634621422483, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634621422501, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621422506, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634621422506, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634621424745, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634621424745, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634621424745, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634621424745, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634621426786, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 1646.4911903151299, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621426787, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621426787, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1646.4911903151299, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634621426787, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621426787, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621427473, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4903.418592813236, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621427473, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621427473, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4903.418592813236, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621427473, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621427473, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621428136, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5069.857896526108, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621428137, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621428137, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5069.857896526108, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634621428137, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621428137, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621428779, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5233.4624932738225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621428780, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621428780, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5233.4624932738225, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634621428780, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621428780, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621429418, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5265.4501325996325, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621429419, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621429419, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5265.4501325996325, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634621429419, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621429419, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621430053, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5306.131136109833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621430053, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621430053, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5306.131136109833, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634621430053, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621430053, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621430673, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.617164774074, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621430674, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621430674, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.617164774074, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634621430674, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621430674, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621431301, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5362.04384416628, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621431301, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621431301, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5362.04384416628, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634621431301, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621431302, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621431916, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5468.4855448264625, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621431917, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621431917, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5468.4855448264625, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634621431917, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621431917, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621432531, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5474.199552131261, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621432531, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621432531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5474.199552131261, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634621432532, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621432532, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621433143, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5496.12071382803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621433144, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621433144, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5496.12071382803, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634621433144, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621433144, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621433755, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5497.4070847930625, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621433756, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621433756, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5497.4070847930625, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634621433756, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621433756, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621434367, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5505.639083116317, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621434367, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621434367, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5505.639083116317, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634621434367, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621434368, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621434985, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5443.709963636139, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621434985, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621434985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5443.709963636139, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634621434985, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621434986, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621435599, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5479.1350867152705, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621435600, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621435600, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5479.1350867152705, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634621435600, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621435600, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621436213, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5484.566139816754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621436213, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621436213, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5484.566139816754, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634621436213, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621436213, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621436837, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5389.084993738229, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621436837, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621436838, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5389.084993738229, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634621436838, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621436838, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621437456, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.846483366563, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621437456, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621437457, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.846483366563, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634621437457, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621437457, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621438084, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.517537809285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621438084, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621438084, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.517537809285, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634621438084, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621438084, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621438705, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5411.6435537717525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621438706, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621438706, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5411.6435537717525, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634621438706, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621438706, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621439328, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.143833777899, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621439329, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621439329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.143833777899, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634621439329, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621439329, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621439953, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5386.2138018833775, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621439953, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621439953, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5386.2138018833775, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634621439953, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621439954, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621440568, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5470.962981251878, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621440568, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621440568, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5470.962981251878, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634621440568, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621440569, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621441187, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.34651159833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621441188, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621441188, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.34651159833, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634621441188, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621441188, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621441805, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.592009720354, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621441805, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621441805, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.592009720354, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634621441805, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621441805, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621442440, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5299.606291750839, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621442440, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621442440, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5299.606291750839, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634621442440, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621442440, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621443056, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5458.410031210034, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621443056, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621443057, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5458.410031210034, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634621443057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621443057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621443678, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.635072925182, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621443678, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621443678, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.635072925182, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634621443678, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621443678, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621444293, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5471.795665851047, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621444293, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621444293, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5471.795665851047, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634621444293, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621444293, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621444905, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5493.012326950676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621444906, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621444906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5493.012326950676, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634621444906, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621444906, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621445519, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5484.561870926088, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621445519, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621445519, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5484.561870926088, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634621445520, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621445520, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621446150, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5333.798645891406, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621446150, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621446150, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5333.798645891406, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634621446150, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621446151, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621446765, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.667725965129, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621446766, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621446766, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.667725965129, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634621446766, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621446766, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621447384, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5439.625255425204, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621447384, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621447384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5439.625255425204, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634621447384, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621447385, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621448000, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5461.121701322958, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621448000, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621448001, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5461.121701322958, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634621448001, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621448001, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621448617, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.545999138901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621448617, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621448617, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.545999138901, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634621448617, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621448617, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621449227, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5511.885867288691, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621449228, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621449228, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5511.885867288691, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634621449228, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621449228, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621449839, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5496.922481046489, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621449840, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621449840, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5496.922481046489, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634621449840, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621449840, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621450449, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5520.239159182497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621450449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621450450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5520.239159182497, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634621450450, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621450450, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621451064, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5468.394302539085, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621451065, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621451065, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5468.394302539085, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634621451065, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621451065, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621451678, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5485.1467708619975, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621451679, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621451679, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5485.1467708619975, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634621451679, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621451679, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621452311, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.06843694041, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621452312, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621452312, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.06843694041, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634621452312, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621452312, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621452925, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5487.2013129230645, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621452925, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621452926, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5487.2013129230645, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634621452926, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621452926, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621453532, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5547.724139234168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621453532, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621453532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5547.724139234168, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634621453533, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621453533, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621454146, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.784455552167, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621454146, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621454146, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.784455552167, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634621454146, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621454147, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621454750, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5570.657662664566, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621454750, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621454751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5570.657662664566, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634621454751, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621454751, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621455362, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5500.956300672979, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621455362, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621455362, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5500.956300672979, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634621455363, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621455363, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621455983, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5421.569943287043, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621455983, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621455983, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5421.569943287043, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634621455983, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621455984, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621456589, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5551.008467409513, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621456590, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621456590, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5551.008467409513, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634621456590, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621456590, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621457198, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5527.635483901334, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621457199, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621457199, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5527.635483901334, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634621457268, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621457268, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621457279, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634621457712, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8279305696487427, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634621457712, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634621457868, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5602.111213316404, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621457868, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621457869, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5602.111213316404, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634621457937, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634621457967, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621457967, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621458447, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8552795648574829, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634621458447, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634621458656, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4878.473628186244, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621458657, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621458657, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4878.473628186244, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634621458725, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621458725, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621458740, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634621459158, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8508251905441284, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634621459158, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634621459353, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5351.53324662749, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621459353, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621459354, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5351.53324662749, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634621459388, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621459389, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621459404, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634621459843, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8647788763046265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634621459843, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634621460026, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5270.771164076572, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621460027, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621460027, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5270.771164076572, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634621460061, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621460061, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621460077, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634621460510, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8851766586303711, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634621460510, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634621460693, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.547569394546, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621460694, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621460694, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.547569394546, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634621460729, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621460729, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621460745, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634621461177, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8869876861572266, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634621461178, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634621461358, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5345.09297207993, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621461358, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621461359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5345.09297207993, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634621461393, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621461394, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621461409, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634621461844, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8726073503494263, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634621461844, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634621462043, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5179.471933439842, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621462043, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621462043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5179.471933439842, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634621462080, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621462080, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621462097, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634621462531, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8611128330230713, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634621462531, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634621462718, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5267.55402258122, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621462718, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621462719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5267.55402258122, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634621462759, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621462760, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621462775, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634621463213, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8743561506271362, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634621463213, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634621463392, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5312.750175390714, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621463393, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621463393, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5312.750175390714, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634621463429, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621463430, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621463445, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634621463877, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.844712495803833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634621463877, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634621464061, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5319.014328983497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621464062, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621464062, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5319.014328983497, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634621464098, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621464098, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621464113, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634621464547, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8806928396224976, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634621464548, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634621464733, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5294.702292640281, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621464733, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621464733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5294.702292640281, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634621464769, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621464770, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621464785, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634621465221, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8774546384811401, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634621465221, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634621465410, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5251.384194717114, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621465410, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621465410, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5251.384194717114, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634621465445, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621465446, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621465462, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634621465898, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8907115459442139, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634621465898, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634621466088, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5233.911474608613, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621466088, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621466089, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5233.911474608613, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634621466125, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621466125, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621466141, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634621466587, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8701314926147461, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634621466587, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634621466778, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5150.545737354305, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621466778, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621466779, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5150.545737354305, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634621466814, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621466815, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621466830, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621467263, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8931540250778198, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621467263, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621467449, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5297.263663714233, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621467449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621467450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5297.263663714233, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634621467485, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621467485, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621467501, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621467933, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8669167757034302, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621467933, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621468122, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5279.101154236452, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621468122, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621468123, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5279.101154236452, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634621468161, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621468161, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621468177, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621468605, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8718059062957764, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621468605, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621468792, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5328.522922553224, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621468792, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621468792, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5328.522922553224, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634621468826, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621468826, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621468842, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621469277, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.895099401473999, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621469278, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621469469, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5229.066161156855, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621469470, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621469470, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5229.066161156855, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634621469507, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621469508, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621469523, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621469961, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8567328453063965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621469961, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621470156, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5184.652952628737, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621470156, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621470157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5184.652952628737, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634621470192, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621470192, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621470207, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621470647, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8829149007797241, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621470647, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621470831, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5259.093582036584, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621470832, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621470832, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5259.093582036584, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634621470874, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621470874, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621470889, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621471314, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8656194806098938, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621471314, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621471505, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5327.49964181824, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621471505, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621471505, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5327.49964181824, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634621471543, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621471543, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621471558, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621471983, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8838708400726318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621471983, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621472172, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5345.808693805015, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621472172, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621472172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5345.808693805015, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634621472212, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621472212, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621472227, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621472653, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8860062956809998, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621472653, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621472842, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5336.098948823987, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621472842, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621472842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5336.098948823987, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634621472877, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621472877, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621472893, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621473326, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8675158023834229, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621473326, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621473519, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5231.966444993646, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621473520, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621473520, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5231.966444993646, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634621473555, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621473555, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621473571, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621474037, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8852503299713135, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621474038, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621474241, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4898.891434590136, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621474241, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621474241, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4898.891434590136, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634621474281, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621474281, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621474297, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621474729, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9030424952507019, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621474730, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621474928, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5200.5980543601245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621474928, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621474928, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5200.5980543601245, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634621474967, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621474967, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621474983, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621475411, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8863086700439453, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621475411, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621475592, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5384.17246331464, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621475592, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621475592, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5384.17246331464, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634621475628, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621475628, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621475645, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621476078, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8906652927398682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621476079, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621476288, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5098.243883168648, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621476288, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621476288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5098.243883168648, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634621476324, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621476324, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621476340, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621476768, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8240146636962891, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621476768, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621476986, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5076.596899545937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621476986, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621476987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5076.596899545937, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634621477021, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621477021, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621477038, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621477471, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8799158334732056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621477472, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621477685, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5065.2823078908195, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621477685, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621477685, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5065.2823078908195, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634621477720, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621477720, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621477736, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621478169, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.87577885389328, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621478169, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621478361, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5247.141350285593, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621478361, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621478361, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5247.141350285593, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634621478396, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621478396, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621478412, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634621478847, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8835514783859253, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634621478847, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634621479044, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5186.478970687439, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621479045, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621479045, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5186.478970687439, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634621479079, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621479080, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621479095, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634621479516, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8864967226982117, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634621479516, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634621479721, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5239.8779865701945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621479721, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621479721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5239.8779865701945, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634621479756, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621479757, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621479773, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634621480200, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8972013592720032, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634621480200, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634621480383, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.102889779218, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621480384, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621480384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.102889779218, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634621480420, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621480420, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621480436, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634621480871, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8871997594833374, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634621480872, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634621481059, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5258.363611601871, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621481060, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621481060, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5258.363611601871, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634621481094, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621481095, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621481110, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634621481547, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.904657781124115, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634621481547, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634621481733, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5263.762725335931, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621481734, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621481734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5263.762725335931, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634621481769, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621481769, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621481786, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634621482221, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8960511684417725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634621482222, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634621482415, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5204.127833808959, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621482415, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621482415, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5204.127833808959, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634621482451, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621482452, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621482468, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634621482902, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8833752870559692, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634621482902, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634621483091, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5259.85123876257, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621483091, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621483091, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5259.85123876257, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634621483127, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621483127, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621483142, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634621483576, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8989862203598022, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634621483576, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634621483770, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5228.377476760889, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621483770, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621483770, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5228.377476760889, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634621483807, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621483807, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621483822, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634621484267, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8947474956512451, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634621484267, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634621484464, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5118.8804531886435, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621484464, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621484464, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5118.8804531886435, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634621484500, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621484500, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621484515, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634621484953, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8968075513839722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634621484953, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634621485141, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5245.981139892593, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621485141, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621485142, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5245.981139892593, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634621485177, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621485178, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621485192, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634621485627, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011484384536743, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634621485627, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634621485817, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5258.420510616196, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621485817, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621485818, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5258.420510616196, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634621485856, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621485856, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621485871, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634621486305, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9024927616119385, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634621486305, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634621486489, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5307.246155004896, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621486490, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621486490, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5307.246155004896, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634621486526, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621486527, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621486541, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634621486974, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8870500922203064, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634621486974, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634621487170, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5226.754451389371, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621487170, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621487170, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5226.754451389371, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634621487205, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621487206, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634621487221, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634621487652, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9082199335098267, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634621487653, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634621487653, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634621487839, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5308.011752843767, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634621487839, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634621487839, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5308.011752843767, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
ENDING TIMING RUN AT 2021-10-19 05:31:33 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:34 AM
RESULT,image_segmentation,,197,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:35 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:37 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:38 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:39 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:40 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:41 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:42 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:43 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:44 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:45 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:46 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
ENDING TIMING RUN AT 2021-10-19 05:31:47 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:28:17 AM
