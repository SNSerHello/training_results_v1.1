+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019061218308241354
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019061218308241354
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061218308241354
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061218308241354
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07369/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019061218308241354_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C0475
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C0463
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:12:21 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634623946.275641] [ip-0A0C040E:59876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.280204] [ip-0A0C040E:59873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.406526] [ip-0A0C040E:59872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.418802] [ip-0A0C040E:59875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.445840] [ip-0A0C040E:59892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.452245] [ip-0A0C040A:73379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.454377] [ip-0A0C040B:18028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.472583] [ip-0A0C040E:59870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.478800] [ip-0A0C0422:2056 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.480540] [ip-0A0C040E:59871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.483518] [ip-0A0C040E:59869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.483176] [ip-0A0C040B:18027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.495254] [ip-0A0C040B:18029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.499899] [ip-0A0C0410:88458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.501218] [ip-0A0C0424:60663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.508351] [ip-0A0C0422:2055 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.508796] [ip-0A0C0457:52155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.523731] [ip-0A0C0457:52160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.525405] [ip-0A0C040C:84415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.539462] [ip-0A0C0453:57654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.540014] [ip-0A0C047A:46386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.546626] [ip-0A0C0424:60656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.546955] [ip-0A0C047A:46390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.548721] [ip-0A0C0408:96228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.550849] [ip-0A0C040A:73380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.563366] [ip-0A0C0428:60687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.564092] [ip-0A0C044C:53974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.564091] [ip-0A0C044C:53979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.565056] [ip-0A0C042F:67044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.570872] [ip-0A0C046D:40687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.575818] [ip-0A0C042C:65187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.576006] [ip-0A0C040A:73381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.575791] [ip-0A0C040B:18026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.576735] [ip-0A0C0422:2061 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.577416] [ip-0A0C0409:38025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.577699] [ip-0A0C0480:40642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.580192] [ip-0A0C047B:47990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.581868] [ip-0A0C0410:88464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.583624] [ip-0A0C0414:88491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.585680] [ip-0A0C0458:59998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.593450] [ip-0A0C046D:40684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.593282] [ip-0A0C0453:57681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.593952] [ip-0A0C040A:73377:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.594303] [ip-0A0C043C:58126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.596902] [ip-0A0C045E:53350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.597680] [ip-0A0C0480:40639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.597599] [ip-0A0C0409:38026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.597421] [ip-0A0C0481:39523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.597969] [ip-0A0C041E:71802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.598752] [ip-0A0C0450:27564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.602758] [ip-0A0C0458:60000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.603229] [ip-0A0C0434:60608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.604112] [ip-0A0C0428:60688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.604995] [ip-0A0C043C:58122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.605902] [ip-0A0C0450:27569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.605482] [ip-0A0C0422:2058 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.608385] [ip-0A0C045A:56029:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.609795] [ip-0A0C047A:46389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.610119] [ip-0A0C0410:88461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.610943] [ip-0A0C045E:53347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.613851] [ip-0A0C045A:56028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.614143] [ip-0A0C0418:68620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.614840] [ip-0A0C0463:46507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.615230] [ip-0A0C0471:41457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.615837] [ip-0A0C040A:73376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.617833] [ip-0A0C042C:65190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.618009] [ip-0A0C0447:58840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.618983] [ip-0A0C0439:49052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.620354] [ip-0A0C0408:96234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.621525] [ip-0A0C041E:71803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.621617] [ip-0A0C0424:60662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.623790] [ip-0A0C041E:71798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.625336] [ip-0A0C0480:40646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.626255] [ip-0A0C0426:70453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.629090] [ip-0A0C0414:88490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.627455] [ip-0A0C0412:51344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.630777] [ip-0A0C0471:41459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.633654] [ip-0A0C0453:57661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.635378] [ip-0A0C0408:96230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.635460] [ip-0A0C046D:40690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.635355] [ip-0A0C041D:68501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.635883] [ip-0A0C0434:60606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.636800] [ip-0A0C040B:18031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.637911] [ip-0A0C043A:57286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.639354] [ip-0A0C040B:18032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.640388] [ip-0A0C047C:51474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.640568] [ip-0A0C0410:88463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.641447] [ip-0A0C042F:67043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.641732] [ip-0A0C040B:18053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.641849] [ip-0A0C042F:67046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.646219] [ip-0A0C047B:47986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.646017] [ip-0A0C040A:73374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.646308] [ip-0A0C041D:68497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.646339] [ip-0A0C043A:57290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.646877] [ip-0A0C046A:47100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.647460] [ip-0A0C040B:18025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.648260] [ip-0A0C040A:73375:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.649544] [ip-0A0C040A:73382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.650776] [ip-0A0C044A:50992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.651029] [ip-0A0C0418:68626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.651288] [ip-0A0C0431:62365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.650879] [ip-0A0C0422:2057 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.651193] [ip-0A0C0420:72984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.654258] [ip-0A0C0413:6379 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.657671] [ip-0A0C042C:65186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.658296] [ip-0A0C0439:49055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.658581] [ip-0A0C040C:84416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.658625] [ip-0A0C0419:76201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.658545] [ip-0A0C0445:62435:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.659874] [ip-0A0C047C:51450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.660407] [ip-0A0C0481:39540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.660632] [ip-0A0C0481:39520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.661566] [ip-0A0C0422:2054 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.662378] [ip-0A0C0422:2059 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.663178] [ip-0A0C0439:49049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.663225] [ip-0A0C0410:88459:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.665040] [ip-0A0C0518:70603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.665461] [ip-0A0C042A:62417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.666701] [ip-0A0C0424:60657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.666976] [ip-0A0C045B:59903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.666965] [ip-0A0C045B:59917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.666978] [ip-0A0C045B:59906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.666883] [ip-0A0C044E:49143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.667433] [ip-0A0C041B:89844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.668836] [ip-0A0C046A:47097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.670971] [ip-0A0C0427:68781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.672242] [ip-0A0C045B:59900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.673063] [ip-0A0C0416:58948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.673087] [ip-0A0C0455:50801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.673575] [ip-0A0C0424:60661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.673614] [ip-0A0C0424:60660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.673766] [ip-0A0C0408:96229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.676546] [ip-0A0C0479:42137:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.677522] [ip-0A0C0459:52088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.677634] [ip-0A0C0422:2060 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.678447] [ip-0A0C0467:42178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.678646] [ip-0A0C044E:49146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.681194] [ip-0A0C0455:50799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.681997] [ip-0A0C0453:57655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.682167] [ip-0A0C0425:74942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.681653] [ip-0A0C041F:75761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.682811] [ip-0A0C0456:59632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.681349] [ip-0A0C042E:70959:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.684862] [ip-0A0C047A:46391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.689437] [ip-0A0C042A:62415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.689351] [ip-0A0C0417:5043 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.690379] [ip-0A0C0409:38004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.690007] [ip-0A0C0410:88467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691183] [ip-0A0C045E:53345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691214] [ip-0A0C0469:41547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691140] [ip-0A0C0412:51342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691891] [ip-0A0C0409:38003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691711] [ip-0A0C0457:52156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.692435] [ip-0A0C040C:84414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.691856] [ip-0A0C0457:52159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.692448] [ip-0A0C043D:48196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.693824] [ip-0A0C0447:58846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.695854] [ip-0A0C044C:53973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.695920] [ip-0A0C0434:60601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.695954] [ip-0A0C045C:52789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.696399] [ip-0A0C0410:88460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.698126] [ip-0A0C0411:71671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.698830] [ip-0A0C0456:59631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.699493] [ip-0A0C044C:53975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.700330] [ip-0A0C0424:60658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.700645] [ip-0A0C0428:60694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.700407] [ip-0A0C0424:60659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.700300] [ip-0A0C043A:57289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.701708] [ip-0A0C040C:84418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.701552] [ip-0A0C041C:77805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.702437] [ip-0A0C0431:62366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.704186] [ip-0A0C041A:95353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.704744] [ip-0A0C0407:76581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.704857] [ip-0A0C0467:42171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.705460] [ip-0A0C0421:73845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.706413] [ip-0A0C045D:43413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.706632] [ip-0A0C0410:88462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.707155] [ip-0A0C042B:69862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.707156] [ip-0A0C042B:69865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.708756] [ip-0A0C040C:84419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.711482] [ip-0A0C043D:48194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.711800] [ip-0A0C0470:36917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.711523] [ip-0A0C0457:52163:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.711612] [ip-0A0C042F:67045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.712342] [ip-0A0C0432:95358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.713555] [ip-0A0C0467:42174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.714278] [ip-0A0C0479:42132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.716067] [ip-0A0C0414:88488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.715175] [ip-0A0C047B:47985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.715440] [ip-0A0C042C:65188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.715674] [ip-0A0C0477:46227:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.716702] [ip-0A0C0448:64175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.718827] [ip-0A0C0430:5639 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.718852] [ip-0A0C0430:5638 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.719731] [ip-0A0C0426:70451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.720470] [ip-0A0C0446:59852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.720490] [ip-0A0C0446:59859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.720630] [ip-0A0C0471:41462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.721122] [ip-0A0C0457:52158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.723000] [ip-0A0C047A:46392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.723757] [ip-0A0C047A:46388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.723884] [ip-0A0C047A:46387:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.723926] [ip-0A0C0454:56494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.723970] [ip-0A0C045D:43419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.724072] [ip-0A0C0418:68621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.725473] [ip-0A0C0426:70452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.725435] [ip-0A0C047B:47989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.725326] [ip-0A0C044F:49833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.725247] [ip-0A0C0481:39524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.726403] [ip-0A0C044A:50993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.727390] [ip-0A0C0411:71673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.728504] [ip-0A0C040C:84413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.728463] [ip-0A0C0452:55677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.728711] [ip-0A0C041B:89848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.729258] [ip-0A0C0477:46225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.730621] [ip-0A0C044C:53977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.730684] [ip-0A0C046D:40685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.731278] [ip-0A0C0458:59997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.731700] [ip-0A0C044C:53978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.731749] [ip-0A0C0419:76200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.733107] [ip-0A0C0408:96235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.733891] [ip-0A0C040C:84417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.733759] [ip-0A0C0419:76202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.733307] [ip-0A0C0420:72987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.733460] [ip-0A0C0420:73012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.736686] [ip-0A0C047A:46393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.736870] [ip-0A0C0450:27566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.736609] [ip-0A0C0471:41456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.736927] [ip-0A0C0450:27567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.736771] [ip-0A0C0457:52157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.737492] [ip-0A0C0433:65949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.738188] [ip-0A0C042C:65185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.738312] [ip-0A0C0425:74943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.738528] [ip-0A0C0418:68624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.739701] [ip-0A0C0434:60604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.740313] [ip-0A0C0428:60690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.740731] [ip-0A0C045B:59899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.741011] [ip-0A0C0463:46506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.741513] [ip-0A0C040C:84421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.741955] [ip-0A0C0427:68778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.742425] [ip-0A0C0427:68780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.742931] [ip-0A0C0463:46512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.743032] [ip-0A0C0476:32128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.743468] [ip-0A0C0475:38343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.743494] [ip-0A0C0475:38338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.744919] [ip-0A0C0432:95356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.745063] [ip-0A0C0518:70605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.745040] [ip-0A0C044C:53976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.745320] [ip-0A0C0518:70604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.746112] [ip-0A0C041E:71800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.746986] [ip-0A0C0479:42138:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747406] [ip-0A0C041E:71796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747304] [ip-0A0C045A:56031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747065] [ip-0A0C045C:52791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747557] [ip-0A0C046D:40688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747544] [ip-0A0C0453:57656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.747669] [ip-0A0C0457:52162:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.748618] [ip-0A0C0480:40640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749042] [ip-0A0C041A:95350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749448] [ip-0A0C0480:40645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749276] [ip-0A0C0408:96231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749306] [ip-0A0C0453:57660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.750644] [ip-0A0C0414:88485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749654] [ip-0A0C0411:71669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.750092] [ip-0A0C047C:51449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.750114] [ip-0A0C0416:58947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.749760] [ip-0A0C041F:75763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.750547] [ip-0A0C0407:76576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.750806] [ip-0A0C047B:47988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.751499] [ip-0A0C0412:51340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.751657] [ip-0A0C0417:5042 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.752455] [ip-0A0C044C:53980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.753035] [ip-0A0C045B:59905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.752795] [ip-0A0C0453:57659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.752124] [ip-0A0C046A:47110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.753298] [ip-0A0C0467:42173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.753675] [ip-0A0C044A:50994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.754028] [ip-0A0C0469:41548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.754176] [ip-0A0C0465:44008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.754612] [ip-0A0C0480:40641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.754551] [ip-0A0C0458:60002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.755294] [ip-0A0C045E:53346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.754971] [ip-0A0C0445:62437:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.755655] [ip-0A0C0445:62440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.756226] [ip-0A0C0416:58943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.756180] [ip-0A0C0409:38002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.756604] [ip-0A0C0409:37999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.756777] [ip-0A0C0409:38001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.757235] [ip-0A0C045A:56027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.757109] [ip-0A0C0453:57658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.758450] [ip-0A0C0414:88489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.757544] [ip-0A0C0418:68622:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.759087] [ip-0A0C0423:65564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.756522] [ip-0A0C042E:70955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.758388] [ip-0A0C044E:49144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.759124] [ip-0A0C0423:65561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.759147] [ip-0A0C0423:65566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.759993] [ip-0A0C042C:65182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.758061] [ip-0A0C042E:70954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.761071] [ip-0A0C0408:96232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.762478] [ip-0A0C0414:88487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.761611] [ip-0A0C0409:38000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.761965] [ip-0A0C0455:50802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.762881] [ip-0A0C043F:52847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.764315] [ip-0A0C0408:96240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.764589] [ip-0A0C0413:6376 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.765520] [ip-0A0C045D:43414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.764765] [ip-0A0C0413:6373 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.766221] [ip-0A0C0423:65567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.765792] [ip-0A0C0416:58945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.765827] [ip-0A0C0439:49050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.765977] [ip-0A0C046D:40689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.766541] [ip-0A0C0442:51572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.768169] [ip-0A0C043C:58127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.768675] [ip-0A0C042A:62416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.769098] [ip-0A0C0432:95355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.769220] [ip-0A0C0454:56495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.769000] [ip-0A0C041D:68499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.769774] [ip-0A0C042C:65184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.769867] [ip-0A0C042C:65183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770296] [ip-0A0C045B:59901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770553] [ip-0A0C045B:59904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770502] [ip-0A0C0518:70598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770246] [ip-0A0C043C:58129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770721] [ip-0A0C042B:69866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770610] [ip-0A0C0451:54587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770761] [ip-0A0C0440:51113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.770568] [ip-0A0C042F:67041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.771384] [ip-0A0C0470:36916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.772292] [ip-0A0C0450:27571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.772370] [ip-0A0C0450:27568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.773198] [ip-0A0C0452:55684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.773544] [ip-0A0C0458:59996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.773620] [ip-0A0C0458:60001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.773597] [ip-0A0C043C:58128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.774091] [ip-0A0C047B:47984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.774175] [ip-0A0C047B:47987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.774323] [ip-0A0C0445:62439:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.775930] [ip-0A0C0447:58847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.777420] [ip-0A0C046D:40686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.777220] [ip-0A0C041C:77806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.777595] [ip-0A0C046D:40683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.777797] [ip-0A0C0434:60607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.778448] [ip-0A0C041E:71799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.778953] [ip-0A0C0432:95357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.779511] [ip-0A0C0480:40644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.779336] [ip-0A0C043E:45724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.779805] [ip-0A0C0480:40643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.779956] [ip-0A0C0428:60692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.779534] [ip-0A0C0476:32126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.781146] [ip-0A0C0428:60691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.780587] [ip-0A0C0417:5047 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.781284] [ip-0A0C042F:67042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.782052] [ip-0A0C0481:39521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.782403] [ip-0A0C0427:68775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784370] [ip-0A0C0446:59854:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784144] [ip-0A0C0463:46509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.783667] [ip-0A0C043B:57364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.783648] [ip-0A0C043B:57369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784883] [ip-0A0C045E:53357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784957] [ip-0A0C041E:71819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784164] [ip-0A0C043A:57283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784439] [ip-0A0C042F:67048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.784532] [ip-0A0C042F:67065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.786539] [ip-0A0C045A:56032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.785721] [ip-0A0C0412:51341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.787057] [ip-0A0C045E:53343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.787318] [ip-0A0C047C:51451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.787614] [ip-0A0C047B:47983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.787784] [ip-0A0C0447:58843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.787910] [ip-0A0C0421:73849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.788517] [ip-0A0C0421:73850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.789095] [ip-0A0C047F:44719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.788795] [ip-0A0C0459:52087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.791022] [ip-0A0C0414:88484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.789317] [ip-0A0C0413:6380 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.789556] [ip-0A0C0459:52086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.790955] [ip-0A0C0431:62368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.790773] [ip-0A0C0448:64179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.790834] [ip-0A0C0439:49051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.791901] [ip-0A0C0450:27565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.792105] [ip-0A0C0450:27570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.792226] [ip-0A0C0428:60689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.792320] [ip-0A0C0428:60717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.793427] [ip-0A0C0414:88486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.792627] [ip-0A0C0412:51345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.794033] [ip-0A0C041A:95352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.794182] [ip-0A0C0474:37922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.794585] [ip-0A0C042A:62418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.795300] [ip-0A0C045A:56026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.795231] [ip-0A0C0425:74941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.794796] [ip-0A0C041F:75764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.795605] [ip-0A0C0458:60003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.796087] [ip-0A0C0463:46517:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.796157] [ip-0A0C041D:68496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.796291] [ip-0A0C041B:89851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.796661] [ip-0A0C0420:72988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.797281] [ip-0A0C041E:71797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.797257] [ip-0A0C0451:54586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.797441] [ip-0A0C0471:41460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.797609] [ip-0A0C0471:41458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.797605] [ip-0A0C045C:52786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.800272] [ip-0A0C0481:39545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.800326] [ip-0A0C043D:48188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.801807] [ip-0A0C045E:53348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.801684] [ip-0A0C0470:36942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.801436] [ip-0A0C041C:77804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.801645] [ip-0A0C0418:68627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.803158] [ip-0A0C0419:76203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804149] [ip-0A0C043C:58125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804492] [ip-0A0C0447:58844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804925] [ip-0A0C045E:53344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804916] [ip-0A0C0458:59999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804888] [ip-0A0C0469:41552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.804977] [ip-0A0C0481:39525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.805085] [ip-0A0C0481:39522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.806149] [ip-0A0C045A:56030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.805857] [ip-0A0C043A:57288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.807390] [ip-0A0C0434:60602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.808075] [ip-0A0C044A:50995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.808060] [ip-0A0C0439:49053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.808173] [ip-0A0C0439:49054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.807627] [ip-0A0C0413:6377 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.809784] [ip-0A0C0431:62370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.811556] [ip-0A0C0426:70457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.811721] [ip-0A0C0463:46508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.812881] [ip-0A0C0448:64178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.812973] [ip-0A0C0407:76575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.813154] [ip-0A0C041A:95355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.814095] [ip-0A0C045A:56025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.814056] [ip-0A0C0465:44010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.815322] [ip-0A0C043C:58123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.813872] [ip-0A0C042E:70956:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.816828] [ip-0A0C044A:50996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.816468] [ip-0A0C043C:58124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.816605] [ip-0A0C0474:37921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.817230] [ip-0A0C041D:68495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.817715] [ip-0A0C0479:42129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.818803] [ip-0A0C047C:51448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.819191] [ip-0A0C0439:49056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.818614] [ip-0A0C0417:5046 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.819362] [ip-0A0C047D:41836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.820124] [ip-0A0C0477:46226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.820550] [ip-0A0C0418:68623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.820707] [ip-0A0C0418:68625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.821364] [ip-0A0C0465:44005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.821722] [ip-0A0C045D:43416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.822516] [ip-0A0C0471:41461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.822776] [ip-0A0C044F:49830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.822590] [ip-0A0C044B:55262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.822617] [ip-0A0C0434:60603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.822963] [ip-0A0C0465:44011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.823488] [ip-0A0C0434:60605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.823753] [ip-0A0C0467:42177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.824000] [ip-0A0C0416:58944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.824514] [ip-0A0C0431:62363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.824636] [ip-0A0C0471:41455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.824725] [ip-0A0C0455:50803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.824150] [ip-0A0C0412:51339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.825314] [ip-0A0C0425:74945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.825249] [ip-0A0C043A:57284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.826621] [ip-0A0C0463:46505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.826574] [ip-0A0C0476:32129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.826895] [ip-0A0C042A:62411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.827129] [ip-0A0C044E:49142:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829170] [ip-0A0C0407:76579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829425] [ip-0A0C0463:46511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829269] [ip-0A0C0455:50797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829331] [ip-0A0C044F:49828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829136] [ip-0A0C041B:89849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829579] [ip-0A0C0474:37923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829965] [ip-0A0C0447:58842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.830010] [ip-0A0C0447:58845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.829650] [ip-0A0C041F:75766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.830277] [ip-0A0C0442:51571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832288] [ip-0A0C044A:50991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832550] [ip-0A0C0426:70454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832754] [ip-0A0C0454:56520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832843] [ip-0A0C0433:65952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832705] [ip-0A0C0440:51117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832497] [ip-0A0C0459:52083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.832523] [ip-0A0C0412:51346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.834051] [ip-0A0C0456:59634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.833930] [ip-0A0C044E:49145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.834167] [ip-0A0C043A:57287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835055] [ip-0A0C0456:59635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.834865] [ip-0A0C043E:45727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835176] [ip-0A0C0445:62441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835352] [ip-0A0C0447:58841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835562] [ip-0A0C047C:51446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835275] [ip-0A0C0427:68779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835744] [ip-0A0C0518:70601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.834791] [ip-0A0C046A:47098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835745] [ip-0A0C0475:38336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.835548] [ip-0A0C043A:57285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.837177] [ip-0A0C0412:51343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.837428] [ip-0A0C0413:6374 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.839188] [ip-0A0C047C:51444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.838637] [ip-0A0C0413:6375 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.839873] [ip-0A0C0469:41550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.841244] [ip-0A0C041D:68498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.842173] [ip-0A0C0419:76198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.841384] [ip-0A0C046A:47099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.842682] [ip-0A0C0518:70599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.843182] [ip-0A0C0448:64180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.842806] [ip-0A0C045C:52785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.843424] [ip-0A0C047D:41837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.844719] [ip-0A0C0467:42175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.845823] [ip-0A0C0451:54590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.846194] [ip-0A0C046A:47101:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.847912] [ip-0A0C041B:89850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.847933] [ip-0A0C0420:72986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.847684] [ip-0A0C041F:75762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.848816] [ip-0A0C0426:70455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.848587] [ip-0A0C0455:50796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.848977] [ip-0A0C0411:71670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.850309] [ip-0A0C043D:48191:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.851332] [ip-0A0C0456:59629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.851014] [ip-0A0C044E:49140:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.850724] [ip-0A0C046A:47102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.852036] [ip-0A0C0431:62364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.851981] [ip-0A0C047C:51445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.852013] [ip-0A0C0421:73848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.851952] [ip-0A0C046A:47104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.853082] [ip-0A0C0518:70600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.853158] [ip-0A0C0518:70602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.853669] [ip-0A0C0426:70458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.853883] [ip-0A0C0420:72989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.854640] [ip-0A0C0470:36915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.854615] [ip-0A0C044F:49832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.854797] [ip-0A0C041D:68500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.854987] [ip-0A0C043F:52843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.855134] [ip-0A0C0445:62438:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.855306] [ip-0A0C042B:69863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.855294] [ip-0A0C041D:68494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.856539] [ip-0A0C0430:5645 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.856153] [ip-0A0C044E:49147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.856235] [ip-0A0C0413:6378 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.856671] [ip-0A0C0445:62436:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857032] [ip-0A0C0419:76199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857482] [ip-0A0C044A:50989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857463] [ip-0A0C0479:42133:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857233] [ip-0A0C043F:52844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857721] [ip-0A0C042A:62412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.857334] [ip-0A0C0462:83464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.859070] [ip-0A0C045D:43415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.859193] [ip-0A0C041C:77802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.859724] [ip-0A0C044A:50990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.860301] [ip-0A0C0426:70456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.860226] [ip-0A0C0421:73847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.860746] [ip-0A0C042A:62413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.862084] [ip-0A0C044E:49141:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.862824] [ip-0A0C041C:77801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.861294] [ip-0A0C042E:70957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.863336] [ip-0A0C0420:72985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.863868] [ip-0A0C042A:62414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.863619] [ip-0A0C0420:72983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.864406] [ip-0A0C042B:69861:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.864835] [ip-0A0C0431:62389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.865161] [ip-0A0C0411:71668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.865556] [ip-0A0C0445:62434:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.865363] [ip-0A0C041F:75765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.865993] [ip-0A0C0459:52085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.866868] [ip-0A0C0433:65953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.866814] [ip-0A0C0407:76577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.866910] [ip-0A0C0419:76205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.866284] [ip-0A0C041F:75767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.866990] [ip-0A0C0419:76204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.867783] [ip-0A0C0431:62367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.868243] [ip-0A0C0467:42172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.868356] [ip-0A0C0467:42170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.868607] [ip-0A0C0430:5644 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.869762] [ip-0A0C0423:65565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.870107] [ip-0A0C0423:65568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.869485] [ip-0A0C0455:50800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.869845] [ip-0A0C0475:38335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.869231] [ip-0A0C0417:5044 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.870081] [ip-0A0C0454:56492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.870640] [ip-0A0C0479:42134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.870670] [ip-0A0C0455:50798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.872194] [ip-0A0C0479:42130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.872035] [ip-0A0C043D:48189:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.873035] [ip-0A0C0430:5642 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.871497] [ip-0A0C042E:70960:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.874345] [ip-0A0C0452:55682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.872366] [ip-0A0C042E:70953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.875607] [ip-0A0C0446:59858:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.875377] [ip-0A0C0479:42131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.874955] [ip-0A0C041B:89845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.874955] [ip-0A0C041B:89847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.875882] [ip-0A0C043F:52842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.873968] [ip-0A0C042E:70958:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.876106] [ip-0A0C0459:52081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.877277] [ip-0A0C0423:65563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.876875] [ip-0A0C0416:58942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.876949] [ip-0A0C0416:58950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.877098] [ip-0A0C041C:77803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.877761] [ip-0A0C0416:58949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.877562] [ip-0A0C0427:68777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.877595] [ip-0A0C041B:89846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.878242] [ip-0A0C043F:52845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.878875] [ip-0A0C0427:68776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.879469] [ip-0A0C0427:68782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.879137] [ip-0A0C041F:75768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.879845] [ip-0A0C0469:41549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.880048] [ip-0A0C0459:52082:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.880654] [ip-0A0C0421:73851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.881018] [ip-0A0C0477:46231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.881151] [ip-0A0C0417:5048 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.882346] [ip-0A0C0417:5045 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.883894] [ip-0A0C0440:51112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.884648] [ip-0A0C0459:52084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.885350] [ip-0A0C0452:55679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.884460] [ip-0A0C0442:51574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.885905] [ip-0A0C0432:95354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.886545] [ip-0A0C0423:65562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.886250] [ip-0A0C0456:59633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.887113] [ip-0A0C0469:41546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.888385] [ip-0A0C0456:59630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.888659] [ip-0A0C0421:73852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.889733] [ip-0A0C0432:95353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.889502] [ip-0A0C041A:95349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.889359] [ip-0A0C0462:83471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.890501] [ip-0A0C047F:44717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.890248] [ip-0A0C0417:5049 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.891077] [ip-0A0C0425:74939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.892766] [ip-0A0C0456:59628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.893277] [ip-0A0C0430:5641 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.893486] [ip-0A0C0432:95359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.892847] [ip-0A0C0462:83467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.894014] [ip-0A0C047F:44722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.894538] [ip-0A0C0411:71674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.894310] [ip-0A0C043D:48192:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.894517] [ip-0A0C043D:48190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.895004] [ip-0A0C043D:48193:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.896055] [ip-0A0C0432:95360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.896190] [ip-0A0C0470:36919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.896817] [ip-0A0C0407:76574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.896650] [ip-0A0C041A:95351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.897197] [ip-0A0C042B:69859:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.897431] [ip-0A0C0425:74938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.899024] [ip-0A0C0425:74944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.899349] [ip-0A0C041A:95348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.899820] [ip-0A0C0452:55678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.900628] [ip-0A0C041A:95354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.901883] [ip-0A0C041C:77807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.902041] [ip-0A0C041C:77800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.902537] [ip-0A0C045C:52794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.903711] [ip-0A0C0421:73844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.904470] [ip-0A0C0465:44009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.905995] [ip-0A0C0440:51115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.906381] [ip-0A0C0448:64182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.906635] [ip-0A0C0448:64177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.906500] [ip-0A0C0477:46229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.907687] [ip-0A0C045D:43420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.907680] [ip-0A0C0425:74940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.910281] [ip-0A0C0446:59857:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.910249] [ip-0A0C045D:43417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.909796] [ip-0A0C043B:57367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.910407] [ip-0A0C042B:69867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.910207] [ip-0A0C045C:52787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.911010] [ip-0A0C0448:64203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.911361] [ip-0A0C042B:69860:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.911783] [ip-0A0C045C:52790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.912169] [ip-0A0C0469:41545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.913856] [ip-0A0C044F:49831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.914171] [ip-0A0C045D:43418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.913868] [ip-0A0C045C:52793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.914194] [ip-0A0C0469:41551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.914655] [ip-0A0C0411:71667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.914596] [ip-0A0C0474:37925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.914726] [ip-0A0C0411:71672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.915426] [ip-0A0C0451:54585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.916528] [ip-0A0C0475:38339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.917190] [ip-0A0C0407:76580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.917347] [ip-0A0C0433:65951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.917399] [ip-0A0C0407:76578:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.917674] [ip-0A0C0440:51114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.921140] [ip-0A0C0454:56493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.921307] [ip-0A0C0454:56496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.923367] [ip-0A0C0446:59853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.923089] [ip-0A0C0465:44007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.923745] [ip-0A0C0430:5643 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.923997] [ip-0A0C0454:56497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.923943] [ip-0A0C0430:5640 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.925061] [ip-0A0C0448:64176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.927423] [ip-0A0C0446:59855:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.928126] [ip-0A0C0433:65950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.927437] [ip-0A0C0442:51576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.928392] [ip-0A0C0476:32127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.929360] [ip-0A0C0475:38341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.929628] [ip-0A0C0475:38337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.930618] [ip-0A0C044B:55268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.930786] [ip-0A0C044B:55267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.931125] [ip-0A0C0454:56491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.932400] [ip-0A0C0477:46230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.933259] [ip-0A0C0470:36918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.933957] [ip-0A0C0446:59856:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.934747] [ip-0A0C0452:55681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.935717] [ip-0A0C043E:45723:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.935991] [ip-0A0C043E:45725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.937723] [ip-0A0C0477:46224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.938159] [ip-0A0C0475:38340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.938214] [ip-0A0C044F:49841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.937983] [ip-0A0C0476:32125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.941516] [ip-0A0C0470:36920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.941590] [ip-0A0C047D:41833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.942197] [ip-0A0C0477:46223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.942811] [ip-0A0C0470:36922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.944109] [ip-0A0C044F:49829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.944919] [ip-0A0C044F:49827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.945315] [ip-0A0C043B:57366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.947091] [ip-0A0C0433:65954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.946163] [ip-0A0C0442:51573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.948597] [ip-0A0C044B:55263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.949417] [ip-0A0C0433:65978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.949659] [ip-0A0C0452:55680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.950221] [ip-0A0C0433:65955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.950200] [ip-0A0C043B:57370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.951745] [ip-0A0C0452:55683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.953734] [ip-0A0C0476:32132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.956985] [ip-0A0C044B:55265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.957032] [ip-0A0C0451:54588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.957522] [ip-0A0C043E:45721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.960032] [ip-0A0C0476:32130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.963316] [ip-0A0C0465:44012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.963399] [ip-0A0C0465:44006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.965117] [ip-0A0C047F:44721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.965345] [ip-0A0C0476:32131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.966402] [ip-0A0C043F:52840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.966392] [ip-0A0C043F:52846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.966058] [ip-0A0C0442:51569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.967882] [ip-0A0C0451:54583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.969143] [ip-0A0C043F:52841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.969247] [ip-0A0C0474:37920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.970253] [ip-0A0C047F:44720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.972078] [ip-0A0C0442:51575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.973091] [ip-0A0C0474:37924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.978501] [ip-0A0C043E:45722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.978663] [ip-0A0C0474:37926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.977988] [ip-0A0C0442:51570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.978834] [ip-0A0C0440:51118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.978986] [ip-0A0C0440:51116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.979576] [ip-0A0C043B:57365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.980000] [ip-0A0C0474:37927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.979575] [ip-0A0C043B:57368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.980518] [ip-0A0C043B:57363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.980934] [ip-0A0C0440:51119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.984667] [ip-0A0C0451:54589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.986502] [ip-0A0C047F:44716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.986727] [ip-0A0C047F:44715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.991155] [ip-0A0C047F:44718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.991570] [ip-0A0C0451:54584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623946.999206] [ip-0A0C043E:45726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.004622] [ip-0A0C043E:45720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.004574] [ip-0A0C0462:83465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.005208] [ip-0A0C047D:41831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.005235] [ip-0A0C047D:41829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.013113] [ip-0A0C047D:41832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.013293] [ip-0A0C047D:41830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.018275] [ip-0A0C047D:41838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.018221] [ip-0A0C0462:83468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.029632] [ip-0A0C044B:55266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.029716] [ip-0A0C044B:55269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.034260] [ip-0A0C044B:55264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.043730] [ip-0A0C0462:83466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.044086] [ip-0A0C0462:83469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634623947.051216] [ip-0A0C0462:83470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634623947955, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634623947997, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634623947997, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634623947998, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623947998, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634623947998, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634623947998, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:38] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:39] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:12:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:38026 - context.c:584] INFO job (ID: 867538482601204004) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38026 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38026 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38002 - context.c:584] INFO job (ID: 867538747892628713) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38002 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38002 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38000 - context.c:584] INFO job (ID: 867538574880160287) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38000 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38000 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38001 - context.c:584] INFO job (ID: 867537993521521860) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38001 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38001 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38003 - context.c:584] INFO job (ID: 867538548058175388) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38003 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38003 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:37999 - context.c:584] INFO job (ID: 867538262756698325) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:37999 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:37999 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38004 - context.c:584] INFO job (ID: 867538411200807486) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38004 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38004 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:38025 - context.c:584] INFO job (ID: 867538782380475767) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:38025 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:38025 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042039, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2508472952, "metadata": {"file": "main.py", "lineno": 72}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042040, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624042041, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:02] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:14:03] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624066073, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634624066103, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624066107, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634624066107, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624068757, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624068758, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634624068758, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624068758, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624070110, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2487.0381630784586, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624070110, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624070110, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2487.0381630784586, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634624070110, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624070110, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624070783, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4992.61761267421, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624070784, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624070784, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4992.61761267421, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624070784, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624070784, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624071436, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5156.30283634203, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624071436, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624071436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5156.30283634203, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624071436, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624071436, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624072068, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5319.040427067324, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624072069, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624072069, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5319.040427067324, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634624072069, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624072069, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624072708, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5261.19632261422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624072708, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624072708, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5261.19632261422, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634624072708, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624072709, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624073335, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5362.109129676957, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624073336, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624073336, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5362.109129676957, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634624073336, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624073336, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624073956, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5422.151914854488, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624073956, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624073956, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5422.151914854488, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624073956, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624073956, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624074583, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.489826104337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624074583, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624074583, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.489826104337, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634624074583, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624074583, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624075198, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5468.218192315706, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624075198, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624075199, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5468.218192315706, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634624075199, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624075199, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624075820, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.620515799069, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624075820, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624075820, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.620515799069, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634624075820, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624075820, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624076442, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.5795319786, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624076443, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624076443, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.5795319786, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634624076443, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624076443, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624077066, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5392.933148477599, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624077067, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624077067, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5392.933148477599, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634624077067, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624077067, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624077689, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.786574245494, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624077689, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624077690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.786574245494, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634624077690, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624077690, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624078309, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.095051679462, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624078309, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624078309, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.095051679462, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634624078309, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624078309, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624078936, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5359.566210479798, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624078937, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624078937, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5359.566210479798, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634624078937, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624078937, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624079565, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5353.316042100657, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624079565, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624079565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5353.316042100657, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634624079565, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624079565, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624080187, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5406.644195348545, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624080187, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624080187, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5406.644195348545, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634624080188, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624080188, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624080816, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5351.492603794649, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624080816, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624080816, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5351.492603794649, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634624080816, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624080816, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624081438, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.281771253038, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624081438, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624081438, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.281771253038, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634624081438, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624081439, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624082074, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5290.41103665747, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624082074, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624082074, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5290.41103665747, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634624082074, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624082074, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624082700, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5370.64664471335, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624082701, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624082701, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5370.64664471335, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634624082701, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624082701, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624083332, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5321.0487398207, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624083333, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624083333, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5321.0487398207, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634624083333, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624083333, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624083958, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.113943165698, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624083959, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624083959, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.113943165698, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634624083959, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624083959, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624084586, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5360.24707661505, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624084586, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624084586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5360.24707661505, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634624084587, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624084587, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624085214, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5360.346978713207, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624085214, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624085214, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5360.346978713207, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634624085214, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624085214, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624085837, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5399.013525824571, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624085837, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624085837, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5399.013525824571, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634624085837, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624085837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624086463, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5372.700268771087, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624086463, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624086463, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5372.700268771087, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634624086463, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624086463, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624087090, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.045743694658, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624087090, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624087091, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.045743694658, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634624087091, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624087091, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624087711, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5421.799379524302, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624087711, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624087711, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5421.799379524302, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634624087711, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624087711, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624088328, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.651033332495, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624088328, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624088328, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.651033332495, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634624088328, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624088328, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624088945, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5444.456547648252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624088946, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624088946, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5444.456547648252, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634624088946, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624088946, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624089566, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.855726978961, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624089567, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624089567, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.855726978961, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634624089567, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624089567, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624090188, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5406.31233931091, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624090189, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624090189, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5406.31233931091, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634624090189, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624090189, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624090808, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5433.283434895956, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624090808, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624090808, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5433.283434895956, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634624090808, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624090808, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624091423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5466.8457700480785, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624091423, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624091423, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5466.8457700480785, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634624091424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624091424, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624092048, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5384.297944525101, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624092048, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624092048, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5384.297944525101, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634624092048, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624092048, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624092674, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.034019994669, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624092674, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624092674, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.034019994669, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634624092674, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624092674, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624093291, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.933666263174, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624093291, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624093291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.933666263174, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634624093291, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624093291, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624093907, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5460.62231480276, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624093907, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624093907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5460.62231480276, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634624093907, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624093907, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624094527, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5421.61582898102, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624094528, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624094528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5421.61582898102, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634624094528, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624094528, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624095141, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5478.144712603511, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624095142, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624095142, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5478.144712603511, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634624095142, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624095142, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624095756, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5472.1122374531, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624095757, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624095757, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5472.1122374531, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634624095757, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624095757, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624096375, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5438.500096862055, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624096375, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624096375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5438.500096862055, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634624096375, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624096376, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624096990, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5470.005282587374, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624096990, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624096990, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5470.005282587374, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634624096990, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624096991, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624097602, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5496.155009240948, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624097602, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624097602, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5496.155009240948, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634624097603, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624097603, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624098223, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5417.1268841739, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624098223, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624098223, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5417.1268841739, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634624098224, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624098224, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624098836, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5485.347458705744, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624098837, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624098837, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5485.347458705744, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634624098837, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624098837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624099451, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5470.064730835469, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624099452, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624099452, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5470.064730835469, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634624099452, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624099452, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624100076, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5388.372059412232, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624100076, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624100076, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5388.372059412232, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634624100076, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624100076, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624100693, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.467589888401, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624100693, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624100693, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.467589888401, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634624100778, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624100779, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624100794, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624101222, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7964926958084106, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624101223, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624101385, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5539.814467654878, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624101386, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624101386, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5539.814467654878, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624101551, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624101552, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624101566, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624101996, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8369332551956177, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624101997, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624102226, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4984.531791425022, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624102226, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624102227, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4984.531791425022, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624102283, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624102283, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624102297, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624102696, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8768398761749268, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624102697, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624102882, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5606.187210225136, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624102883, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624102883, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5606.187210225136, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624102942, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624102942, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624102956, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624103370, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8453364372253418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624103370, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624103566, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5386.520547945205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624103566, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624103567, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5386.520547945205, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624103624, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624103625, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624103639, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624104122, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8786075115203857, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624104122, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624104349, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4636.692791159895, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624104350, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624104350, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4636.692791159895, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624104405, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624104405, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624104419, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624104818, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8859742879867554, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624104818, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624105007, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5579.791020062977, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624105008, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624105008, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5579.791020062977, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624105066, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624105066, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624105080, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624105491, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.879223108291626, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624105491, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624105684, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.525886652406, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624105685, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624105685, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.525886652406, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624105742, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624105743, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624105757, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624106198, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8752338886260986, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624106198, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624106399, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5120.846494095144, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624106399, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624106399, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5120.846494095144, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624106457, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624106457, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624106471, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624106870, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8359401822090149, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624106870, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624107070, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5481.108382565136, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624107070, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624107070, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5481.108382565136, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624107129, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624107129, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624107143, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624107556, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.898713231086731, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624107556, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624107744, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5462.741103528741, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624107744, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624107745, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5462.741103528741, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624107802, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624107802, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624107816, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624108337, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8951148986816406, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624108337, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624108572, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4364.324306580306, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624108572, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624108572, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4364.324306580306, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624108625, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624108626, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624108640, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624109039, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8807812929153442, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624109039, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624109232, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5538.664898636287, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624109233, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624109233, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5538.664898636287, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624109291, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624109291, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624109305, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624109723, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8935120105743408, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624109723, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624109926, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5293.833144011107, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624109927, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624109927, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5293.833144011107, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624109984, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624109984, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624109998, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624110443, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8909028768539429, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624110443, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624110647, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5075.2257332644285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624110647, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624110647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5075.2257332644285, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624110712, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624110712, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624110726, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624111136, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8880590796470642, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624111136, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624111334, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5406.909709513385, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624111334, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624111334, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5406.909709513385, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624111383, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624111383, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624111397, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624111797, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8582563996315002, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624111797, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624111990, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5535.527785338975, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624111990, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624111991, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5535.527785338975, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624112045, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624112045, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624112060, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624112459, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.891626238822937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624112459, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624112657, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5491.528996516763, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624112658, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624112658, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5491.528996516763, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624112720, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624112720, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624112734, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624113134, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8859210014343262, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624113134, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624113328, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5525.04148428388, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624113329, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624113329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5525.04148428388, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624113364, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624113364, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624113381, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624113780, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.871201753616333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624113780, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624113978, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5474.731200214438, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624113978, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624113979, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5474.731200214438, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624114029, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624114029, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624114043, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624114443, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8979461193084717, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624114443, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624114631, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5581.892768619101, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624114632, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624114632, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5581.892768619101, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624114667, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624114668, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624114684, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624115082, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8907949328422546, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624115083, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624115276, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5522.075565368844, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624115277, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624115277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5522.075565368844, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624115312, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624115312, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624115328, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624115727, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8671321272850037, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624115728, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624115919, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5534.923396739653, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624115919, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624115920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5534.923396739653, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624115974, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624115975, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624115989, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624116389, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8910722732543945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624116389, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624116586, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5502.790640829757, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624116586, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624116586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5502.790640829757, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624116621, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624116621, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624116637, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624117038, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8994098901748657, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624117038, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624117226, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5553.9048713501525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624117227, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624117227, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5553.9048713501525, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624117263, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624117263, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624117279, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624117710, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8976627588272095, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624117710, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624117899, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5278.430860228264, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624117900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624117900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5278.430860228264, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624117958, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624117958, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624117972, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624118388, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9002969264984131, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624118388, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624118576, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5437.3082725216655, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624118576, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624118577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5437.3082725216655, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624118631, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624118631, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624118646, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624119045, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9006398916244507, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624119045, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624119234, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5577.743835252099, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624119234, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624119234, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5577.743835252099, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624119291, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624119291, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624119305, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624119705, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8871906399726868, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624119705, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624119933, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5232.04025588355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624119934, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624119934, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5232.04025588355, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624119970, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624119970, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624119987, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624120386, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8618486523628235, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624120386, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624120602, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5318.177318597916, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624120603, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624120603, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5318.177318597916, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624120660, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624120661, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624120675, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624121076, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8936247229576111, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624121076, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624121288, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5359.293097232596, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624121288, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624121288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5359.293097232596, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624121326, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624121326, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624121343, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624121744, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001591205596924, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624121745, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624121932, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5548.873102682526, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624121932, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624121933, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5548.873102682526, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624121988, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624121989, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624122003, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624122403, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9073653817176819, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624122403, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624122592, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5567.598588511189, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624122593, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624122593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5567.598588511189, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624122628, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624122628, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624122644, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624123048, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001802802085876, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624123048, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624123241, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5476.8673394319285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624123242, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624123242, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5476.8673394319285, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624123286, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624123287, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624123301, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624123707, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9038455486297607, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624123707, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624123896, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5514.133089388682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624123896, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624123897, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5514.133089388682, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624123945, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624123945, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624123959, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624124359, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8903516530990601, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624124359, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624124544, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5610.479090976407, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624124545, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624124545, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5610.479090976407, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624124603, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624124604, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624124617, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624125025, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9021391868591309, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624125025, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624125214, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5501.920572926833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624125215, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624125215, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5501.920572926833, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624125250, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624125250, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624125266, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624125668, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9043892621994019, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624125668, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624125862, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5493.190037867641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624125862, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624125862, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5493.190037867641, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624125899, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624125899, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624125917, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624126314, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011528491973877, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624126314, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624126505, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5545.209439658021, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624126505, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624126505, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5545.209439658021, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624126543, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624126543, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624126559, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624126958, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9023746252059937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624126958, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624127148, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5554.920640676703, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624127148, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624127148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5554.920640676703, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624127181, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624127181, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624127195, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624127603, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9025612473487854, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624127603, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624127790, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5517.900538209086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624127791, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624127791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5517.900538209086, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624127828, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624127828, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624127845, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624128246, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9023731350898743, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624128246, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624128432, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5566.712305229316, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624128432, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624128432, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5566.712305229316, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624128469, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624128469, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624128485, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624128885, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9040194749832153, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624128885, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624129069, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5605.65202293515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624129069, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624129069, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5605.65202293515, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624129131, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624129131, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624129145, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624129546, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9084665775299072, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624129546, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624129546, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634624129733, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5583.080259994319, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624129733, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624129733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5583.080259994319, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:36 AM
RESULT,image_segmentation,,195,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:37 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:39 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:40 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:41 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:42 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:43 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:44 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:45 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:46 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:47 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:48 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
ENDING TIMING RUN AT 2021-10-19 06:15:49 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:12:21 AM
